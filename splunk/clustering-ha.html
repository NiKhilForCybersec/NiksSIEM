<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Clustering & High Availability | Splunk</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    <link rel="stylesheet" href="shared-styles.css">
    <link rel="stylesheet" href="../assets/css/main.css">


</head>
<body>
    <div class="app-container">
        
        <!-- Sidebar -->
        <!-- Sidebar -->
        <!-- Sidebar -->
            <aside class="sidebar" id="sidebar">
            <div class="sidebar-header">
                <a href="index.html" class="sidebar-logo"><i class="fas fa-terminal"></i><span>Splunk Reference</span></a>
            </div>
            <div class="sidebar-content">
                <div class="sidebar-section">
                    <div class="sidebar-section-title">Certifications</div>
                    <ul class="sidebar-nav">
                        <li class="sidebar-nav-item"><a href="power-user-certification.html" class="sidebar-nav-link"><i class="fas fa-user-graduate" style="color:#7ce38b"></i>Power User</a></li>
                        <li class="sidebar-nav-item"><a href="admin-certification.html" class="sidebar-nav-link"><i class="fas fa-user-cog" style="color:#f0883e"></i>Admin</a></li>
                        <li class="sidebar-nav-item"><a href="es-admin-certification.html" class="sidebar-nav-link"><i class="fas fa-shield-alt" style="color:#8957e5"></i>ES Admin</a></li>
                        <li class="sidebar-nav-item"><a href="cloud-admin-certification.html" class="sidebar-nav-link"><i class="fas fa-cloud" style="color:#39c5cf"></i>Cloud Admin</a></li>
                    </ul>
                </div>
                <div class="sidebar-section">
                    <div class="sidebar-section-title">SPL Learning</div>
                    <ul class="sidebar-nav">
                        <li class="sidebar-nav-item"><a href="when-to-use-what.html" class="sidebar-nav-link"><i class="fas fa-map-signs" style="color:#f0883e"></i>When to Use What</a></li>
                        <li class="sidebar-nav-item"><a href="spl-practice-questions.html" class="sidebar-nav-link"><i class="fas fa-tasks" style="color:#58a6ff"></i>Practice MCQs</a></li>
                        <li class="sidebar-nav-item"><a href="spl-advanced-mcqs.html" class="sidebar-nav-link"><i class="fas fa-brain" style="color:#a371f7"></i>Advanced MCQs</a></li>
                    </ul>
                </div>
                <div class="sidebar-section">
                    <div class="sidebar-section-title">SPL Reference</div>
                    <ul class="sidebar-nav">
                        <li class="sidebar-nav-item"><a href="spl-fundamentals.html" class="sidebar-nav-link"><i class="fas fa-code"></i>SPL Fundamentals</a></li>
                        <li class="sidebar-nav-item"><a href="spl-intermediate.html" class="sidebar-nav-link"><i class="fas fa-layer-group"></i>SPL Intermediate</a></li>
                        <li class="sidebar-nav-item"><a href="spl-advanced.html" class="sidebar-nav-link"><i class="fas fa-rocket"></i>SPL Advanced</a></li>
                        <li class="sidebar-nav-item"><a href="knowledge-objects.html" class="sidebar-nav-link"><i class="fas fa-lightbulb"></i>Knowledge Objects</a></li>
                        <li class="sidebar-nav-item"><a href="data-models.html" class="sidebar-nav-link"><i class="fas fa-cubes"></i>Data Models</a></li>
                    </ul>
                </div>
                <div class="sidebar-section">
                    <div class="sidebar-section-title">Administration</div>
                    <ul class="sidebar-nav">
                        <li class="sidebar-nav-item"><a href="architecture.html" class="sidebar-nav-link"><i class="fas fa-sitemap"></i>Architecture</a></li>
                        <li class="sidebar-nav-item"><a href="data-ingestion.html" class="sidebar-nav-link"><i class="fas fa-database"></i>Data Ingestion</a></li>
                        <li class="sidebar-nav-item"><a href="forwarders.html" class="sidebar-nav-link"><i class="fas fa-share-alt"></i>Forwarders</a></li>
                        <li class="sidebar-nav-item"><a href="clustering-ha.html" class="sidebar-nav-link"><i class="fas fa-network-wired"></i>Clustering & HA</a></li>
                    </ul>
                </div>
                <div class="sidebar-section">
                    <div class="sidebar-section-title">Enterprise Security</div>
                    <ul class="sidebar-nav">
                        <li class="sidebar-nav-item"><a href="es-overview.html" class="sidebar-nav-link"><i class="fas fa-shield-alt"></i>ES Overview</a></li>
                        <li class="sidebar-nav-item"><a href="correlation-searches.html" class="sidebar-nav-link"><i class="fas fa-search-plus"></i>Correlation Searches</a></li>
                        <li class="sidebar-nav-item"><a href="risk-based-alerting.html" class="sidebar-nav-link"><i class="fas fa-chart-line"></i>Risk-Based Alerting</a></li>
                    </ul>
                </div>
                <div class="sidebar-section">
                    <div class="sidebar-section-title">Resources</div>
                    <ul class="sidebar-nav">
                        <li class="sidebar-nav-item"><a href="lab-exercises.html" class="sidebar-nav-link"><i class="fas fa-flask"></i>Lab Exercises</a></li>
                        <li class="sidebar-nav-item"><a href="troubleshooting.html" class="sidebar-nav-link"><i class="fas fa-wrench"></i>Troubleshooting</a></li>
                    </ul>
                </div>
            </div>
        </aside>
    
    <div class="sidebar-overlay" id="sidebarOverlay" onclick="toggleSidebar()"></div>

        
        <!-- Main Content Wrapper -->
        <div class="main-wrapper" id="mainWrapper">
            <button class="mobile-toggle" onclick="toggleSidebar()" title="Open menu">
                <i class="fas fa-bars"></i>
            </button>
            
            <main class="main-content">
                <div class="breadcrumb">
                    <a href="index.html">Home</a>
                    <span class="separator">/</span>
                    <a href="index.html">Splunk</a>
                    <span class="separator">/</span>
                    <span class="current">Clustering & High Availability</span>
                </div>

                <div class="hero-section">
                    <span class="version-badge">Splunk 9.x / Enterprise</span>
                    <h1 class="hero-title"><i class="fas fa-server" style="color: #3fb950;"></i> Clustering & High Availability</h1>
                    <p class="hero-subtitle">Master Splunk clustering for enterprise-grade resilience. Covers indexer clusters, search head clusters, multisite deployments, and disaster recovery strategies.</p>
                </div>

                <!-- Indexer Clustering -->
                <h2><i class="fas fa-database"></i> Indexer Clustering</h2>
                <div class="config-section">
                    <p>Indexer clustering provides data replication and search availability across multiple indexers.</p>
                    
                    <h3>Core Concepts</h3>
                    <div class="arch-diagram">
INDEXER CLUSTER ARCHITECTURE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚   Cluster Manager   â”‚
                         â”‚   (formerly Master) â”‚
                         â”‚   Port: 8089        â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚ Heartbeat & Coordination
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â–¼               â–¼               â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚ Indexer 1 â”‚   â”‚ Indexer 2 â”‚   â”‚ Indexer 3 â”‚
            â”‚ (Peer)    â”‚   â”‚ (Peer)    â”‚   â”‚ (Peer)    â”‚
            â”‚ Port:9997 â”‚   â”‚ Port:9997 â”‚   â”‚ Port:9997 â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚               â”‚               â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                         Data Replication (RF copies)

KEY TERMINOLOGY:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ Replication Factor (RF): Number of copies of raw data
â€¢ Search Factor (SF): Number of searchable copies
â€¢ Bucket: Container for indexed data (rawdata + indexes)
â€¢ Primary Bucket: Searchable copy that responds to searches
â€¢ Non-Primary: Backup copy, can be promoted if primary fails

COMMON CONFIGURATIONS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Small (3 indexers):    RF=2, SF=2  â†’ Can lose 1 indexer
Medium (5 indexers):   RF=3, SF=2  â†’ Can lose 2 indexers (data), 3 (search)
Large (9+ indexers):   RF=3, SF=3  â†’ Full redundancy
                    </div>

                    <h3>Cluster Manager Configuration</h3>
                    <div class="arch-diagram">
# /opt/splunk/etc/system/local/server.conf on Cluster Manager

[clustering]
mode = manager                    # This node is the cluster manager
replication_factor = 3            # 3 copies of data
search_factor = 2                 # 2 searchable copies
pass4SymmKey = your_secret_key    # Shared secret for cluster auth
cluster_label = production_idx    # Friendly name

[general]
site = site1                      # For multisite clustering

# Restart required after configuration
/opt/splunk/bin/splunk restart
                    </div>

                    <h3>Indexer Peer Configuration</h3>
                    <div class="arch-diagram">
# /opt/splunk/etc/system/local/server.conf on each Indexer

[replication_port://9887]
# Port for bucket replication between peers

[clustering]
mode = peer                                    # This node is a peer
manager_uri = https://cm.company.com:8089     # Cluster manager address
pass4SymmKey = your_secret_key                 # Must match CM
replication_port = 9887                        # Replication traffic port

[general]
site = site1                                   # For multisite

# /opt/splunk/etc/system/local/indexes.conf
[default]
repFactor = auto                               # Use cluster's RF setting
                    </div>
                </div>

                <!-- Search Head Clustering -->
                <h2><i class="fas fa-search"></i> Search Head Clustering</h2>
                <div class="config-section">
                    <p>Search Head Clustering provides high availability and horizontal scaling for search capacity.</p>
                    
                    <div class="arch-diagram">
SEARCH HEAD CLUSTER ARCHITECTURE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚      Deployer       â”‚
                         â”‚  (App Distribution) â”‚
                         â”‚     Port: 8089      â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚ Push Apps/Configs
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â–¼               â–¼               â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚   SH 1    â”‚â—„â”€â–ºâ”‚   SH 2    â”‚â—„â”€â–ºâ”‚   SH 3    â”‚
            â”‚ (Member)  â”‚   â”‚ (Captain) â”‚   â”‚ (Member)  â”‚
            â”‚  Web:443  â”‚   â”‚  Web:443  â”‚   â”‚  Web:443  â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚               â”‚               â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                         Artifact Replication
                         (Knowledge Objects, Jobs)

CAPTAIN ELECTION:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ Uses Raft consensus algorithm
â€¢ Captain coordinates scheduling and artifact replication
â€¢ Automatic failover if captain fails (election within seconds)
â€¢ Captain can be any member; role is dynamic

KEY CONCEPTS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ Deployer: Pushes apps to all SH cluster members
â€¢ Captain: Coordinates scheduling, manages job distribution
â€¢ Members: Handle user requests, run searches
â€¢ Artifacts: Saved searches, dashboards, reports replicated across members
â€¢ Replication Factor: Always equals member count (full replication)
                    </div>

                    <h3>SHC Member Configuration</h3>
                    <div class="arch-diagram">
# /opt/splunk/etc/system/local/server.conf on each Search Head

[shclustering]
disabled = false
mgmt_uri = https://sh1.company.com:8089       # This node's management URI
replication_factor = 3                         # Should equal member count
replication_port = 9900                        # Port for artifact replication
conf_deploy_fetch_url = https://deployer.company.com:8089
pass4SymmKey = shc_secret_key                  # Shared secret for SHC
shcluster_label = production_shc              # Friendly name

[clustering]
mode = searchhead                              # Connect to indexer cluster
manager_uri = https://cm.company.com:8089     # Indexer cluster manager
pass4SymmKey = idx_cluster_key                 # Indexer cluster secret

# Initialize cluster (run on ONE node first)
/opt/splunk/bin/splunk init shcluster-config -auth admin:password \
  -mgmt_uri https://sh1.company.com:8089 \
  -replication_port 9900 \
  -replication_factor 3 \
  -conf_deploy_fetch_url https://deployer.company.com:8089 \
  -secret shc_secret_key \
  -shcluster_label production_shc

# Bootstrap captain (run ONCE after all members configured)
/opt/splunk/bin/splunk bootstrap shcluster-captain -servers_list \
  "https://sh1.company.com:8089,https://sh2.company.com:8089,https://sh3.company.com:8089" \
  -auth admin:password
                    </div>
                </div>

                <!-- Multisite Clustering -->
                <h2><i class="fas fa-globe"></i> Multisite Clustering</h2>
                <div class="config-section">
                    <p>Multisite clustering enables geographic distribution for disaster recovery and data locality.</p>
                    
                    <div class="arch-diagram">
MULTISITE INDEXER CLUSTER
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

        SITE 1 (Primary - NYC)                    SITE 2 (DR - LAX)
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚              â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
    â”‚  â”‚ Cluster Manager â”‚    â”‚   Replicationâ”‚    â”‚   Indexer 4     â”‚  â”‚
    â”‚  â”‚   (site1)       â”‚â—„â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â–ºâ”‚   (site2)       â”‚  â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚              â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
    â”‚                         â”‚              â”‚                         â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚              â”‚â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
    â”‚  â”‚Indexer 1â”‚ â”‚Indexer 2â”‚â”‚              â”‚â”‚Indexer 5â”‚ â”‚Indexer 6â”‚  â”‚
    â”‚  â”‚ (site1) â”‚ â”‚ (site1) â”‚â”‚              â”‚â”‚ (site2) â”‚ â”‚ (site2) â”‚  â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚              â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚              â”‚            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
    â”‚  â”‚Indexer 3â”‚            â”‚              â”‚            â”‚Indexer 7â”‚  â”‚
    â”‚  â”‚ (site1) â”‚            â”‚              â”‚            â”‚ (site2) â”‚  â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚              â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

MULTISITE CONFIGURATION:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
site_replication_factor = origin:2, total:3
  â†’ 2 copies at originating site, 3 copies total across all sites

site_search_factor = origin:1, total:2  
  â†’ 1 searchable at origin, 2 searchable total

available_sites = site1, site2
  â†’ Define all sites in cluster
                    </div>

                    <h3>Multisite Cluster Manager Config</h3>
                    <div class="arch-diagram">
# /opt/splunk/etc/system/local/server.conf on Cluster Manager

[clustering]
mode = manager
multisite = true
available_sites = site1, site2
site_replication_factor = origin:2, total:3
site_search_factor = origin:1, total:2
pass4SymmKey = cluster_secret
cluster_label = multisite_prod

[general]
site = site1

# Each peer specifies its site in [general] section
# site = site1  OR  site = site2
                    </div>
                </div>

                <!-- Disaster Recovery -->
                <h2><i class="fas fa-life-ring"></i> Disaster Recovery</h2>
                <div class="config-section">
                    <h3>DR Strategies</h3>
                    <div class="arch-diagram">
DR STRATEGY COMPARISON
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

STRATEGY 1: MULTISITE CLUSTERING (Active-Active)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ Real-time replication across sites
â€¢ Automatic failover
â€¢ Both sites actively serving searches
â€¢ RPO: Near-zero | RTO: Minutes
â€¢ Cost: High (full infrastructure at both sites)

STRATEGY 2: INDEXER CLUSTER + STANDBY CM (Active-Passive)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ Single-site cluster with standby cluster manager
â€¢ Manual failover to standby CM if primary fails
â€¢ RPO: Near-zero (within cluster) | RTO: 15-30 minutes
â€¢ Cost: Medium (standby CM only)

# Standby CM configuration
[clustering]
mode = manager
replication_factor = 3
search_factor = 2

# On failover, update peers to point to new CM

STRATEGY 3: SUMMARY REPLICATION (Cold DR)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ Periodic backup to remote storage
â€¢ Restore to DR site when needed
â€¢ RPO: Hours to days | RTO: Hours to days
â€¢ Cost: Low

STRATEGY 4: SMARTSTORE + REMOTE STORAGE (Cloud DR)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ Data in S3/Azure Blob/GCS
â€¢ Spin up indexers anywhere pointing to same storage
â€¢ RPO: Near-zero | RTO: Minutes to hours
â€¢ Cost: Medium (cloud storage costs)
                    </div>

                    <h3>Failover Procedures</h3>
                    <div class="arch-diagram">
INDEXER CLUSTER FAILOVER SCENARIOS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

SCENARIO: Single Indexer Failure
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. CM detects peer heartbeat failure (default: 60 seconds)
2. CM marks peer as down
3. Primary buckets on failed peer promoted on other peers
4. Cluster continues operating (if RF allows)
5. When peer returns, bucket repair begins automatically

# Monitor cluster status
splunk show cluster-status --verbose

SCENARIO: Cluster Manager Failure
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. Indexers continue serving existing searches
2. NO new bucket creation or replication
3. NO search head can discover new data
4. Bring up standby CM or restore primary

# If CM down, peers operate in "maintenance mode"
# Manual steps to activate standby CM:
splunk edit cluster-config -mode manager -replication_factor 3 \
  -search_factor 2 -secret cluster_key -cluster_label prod_cluster

SCENARIO: Site Failure (Multisite)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. CM detects all peers at site are down
2. Remaining site(s) have search_factor copies
3. Searches continue on remaining site(s)
4. Data replication paused until site returns

# Force CM to continue without failed site
splunk edit cluster-config -site_search_factor "origin:1,site1:0,site2:1,total:1"
                    </div>
                </div>

                <!-- Decision Trees -->
                <h2><i class="fas fa-project-diagram"></i> Clustering Decision Trees</h2>
                
                <div class="config-section">
                    <h3>Decision Tree: Cluster Architecture Selection</h3>
                    <div class="arch-diagram">
Decision Tree: Choosing Cluster Architecture
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

START: What are your availability requirements?
â”‚
â”œâ”€â–º [No HA required - Dev/Test]
â”‚   â””â”€â–º Standalone deployment
â”‚       Single indexer, single search head
â”‚       No clustering needed
â”‚
â”œâ”€â–º [HA for data only]
â”‚   â”‚
â”‚   â””â”€â–º What's your data volume?
â”‚       â”‚
â”‚       â”œâ”€â–º <100 GB/day â†’ 3-node indexer cluster (RF=2, SF=2)
â”‚       â”‚                  Can lose 1 indexer
â”‚       â”‚
â”‚       â”œâ”€â–º 100-500 GB/day â†’ 5-node indexer cluster (RF=3, SF=2)
â”‚       â”‚                     Can lose 2 indexers
â”‚       â”‚
â”‚       â””â”€â–º >500 GB/day â†’ Scale horizontally
â”‚                          Add more indexers, maintain RF=3, SF=2
â”‚
â”œâ”€â–º [HA for search tier]
â”‚   â”‚
â”‚   â””â”€â–º How many concurrent users?
â”‚       â”‚
â”‚       â”œâ”€â–º <50 users â†’ Single SH + load balancer
â”‚       â”‚                Manual failover acceptable
â”‚       â”‚
â”‚       â”œâ”€â–º 50-200 users â†’ 3-node SH cluster
â”‚       â”‚                   Automatic failover
â”‚       â”‚
â”‚       â””â”€â–º >200 users â†’ 5+ node SH cluster
â”‚                         Horizontal scaling
â”‚
â””â”€â–º [Geographic DR required]
    â”‚
    â””â”€â–º Multisite clustering
        â”‚
        â”œâ”€â–º Active-Active â†’ site_search_factor includes both sites
        â”‚                    Users can search from either site
        â”‚
        â””â”€â–º Active-Passive â†’ site_search_factor = origin:1, total:1
                              DR site only activates on failure
                    </div>
                </div>

                <div class="config-section">
                    <h3>Decision Tree: RF and SF Selection</h3>
                    <div class="arch-diagram">
Decision Tree: Replication Factor & Search Factor
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

START: What's your tolerance for data loss?
â”‚
â”œâ”€â–º [Zero tolerance - Compliance/Financial]
â”‚   â”‚
â”‚   â””â”€â–º RF = 3 (minimum)
â”‚       Consider RF = 4 for critical data
â”‚       â”‚
â”‚       â””â”€â–º How fast must searches resume after failure?
â”‚           â”‚
â”‚           â”œâ”€â–º Immediately â†’ SF = RF (all copies searchable)
â”‚           â”‚                 Higher storage, faster recovery
â”‚           â”‚
â”‚           â””â”€â–º Minutes acceptable â†’ SF = 2
â”‚                                     Bucket promotion time needed
â”‚
â”œâ”€â–º [Some tolerance - Operational data]
â”‚   â”‚
â”‚   â””â”€â–º RF = 2
â”‚       Can survive single indexer failure
â”‚       â”‚
â”‚       â””â”€â–º SF = 2 (recommended)
â”‚           Both copies searchable
â”‚
â””â”€â–º [High tolerance - Non-critical logs]
    â”‚
    â””â”€â–º RF = 1 (no replication)
        OR separate non-clustered indexer
        Lower cost, no redundancy

FORMULA FOR INDEXER COUNT:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Minimum indexers = RF (for data safety)
Recommended = RF + 1 (survive 1 failure during rebuild)
Production = RF + 2 (survive failures + maintenance window)

Example: RF=3 â†’ Minimum 3, Recommended 4, Production 5 indexers
                    </div>
                </div>

                <div class="config-section">
                    <h3>Decision Tree: Troubleshooting Cluster Issues</h3>
                    <div class="arch-diagram">
Decision Tree: Cluster Health Troubleshooting
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

START: Cluster health check failed
â”‚
â”œâ”€â–º Check: splunk show cluster-status --verbose
â”‚
â”œâ”€â–º [Peers showing "Down"]
â”‚   â”‚
â”‚   â”œâ”€â–º Can you ping the peer? 
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â–º NO â†’ Network issue
â”‚   â”‚   â”‚       Check firewall, routing
â”‚   â”‚   â”‚       Ports: 8089, 9887 (replication)
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â–º YES â†’ Is Splunk running on peer?
â”‚   â”‚             â”‚
â”‚   â”‚             â”œâ”€â–º NO â†’ Check logs: /opt/splunk/var/log/splunk/splunkd.log
â”‚   â”‚             â”‚        Common: disk full, license issue, crash
â”‚   â”‚             â”‚
â”‚   â”‚             â””â”€â–º YES â†’ Check pass4SymmKey matches CM
â”‚   â”‚                       Check clustering mode = peer
â”‚   â”‚                       Restart peer if config correct
â”‚
â”œâ”€â–º [Bucket fixup in progress - never completes]
â”‚   â”‚
â”‚   â”œâ”€â–º Check: splunk list cluster-peers
â”‚   â”‚   Look for peers with many "primary" buckets
â”‚   â”‚
â”‚   â”œâ”€â–º Imbalanced cluster?
â”‚   â”‚   Some indexers have more data than others
â”‚   â”‚   Check disk space on all peers
â”‚   â”‚
â”‚   â””â”€â–º Replication bottleneck?
â”‚       Check network bandwidth between peers
â”‚       Check replication_port is not blocked
â”‚
â”œâ”€â–º [Search Head can't find data]
â”‚   â”‚
â”‚   â”œâ”€â–º Is SH connected to cluster?
â”‚   â”‚   Check: [clustering] mode = searchhead in server.conf
â”‚   â”‚
â”‚   â”œâ”€â–º Is manager_uri correct?
â”‚   â”‚   Must match CM's management address exactly
â”‚   â”‚
â”‚   â””â”€â–º Are indexes replicated?
â”‚       Check: repFactor = auto in indexes.conf on peers
â”‚
â””â”€â–º [Captain election failing (SHC)]
    â”‚
    â”œâ”€â–º Check majority of members reachable
    â”‚   Need >50% for quorum
    â”‚
    â”œâ”€â–º Check pass4SymmKey matches on all members
    â”‚
    â””â”€â–º Check mgmt_uri correctly configured on each member
        Must be unique per member
                    </div>
                </div>

                <!-- What Enterprises Usually Miss -->
                <h2><i class="fas fa-exclamation-circle"></i> What Enterprises Usually Miss</h2>
                
                <div class="enterprise-gap-grid">
                    <div class="gap-card gap-critical">
                        <h4>ğŸ”´ Critical Gaps</h4>
                        <ul>
                            <li><strong>No tested failover procedures:</strong> Cluster configured but never tested actual failure scenarios</li>
                            <li><strong>Cluster Manager single point of failure:</strong> CM failure stops bucket operations - no standby CM</li>
                            <li><strong>RF/SF mismatch with indexer count:</strong> RF=3 with only 3 indexers means zero tolerance for failure</li>
                            <li><strong>Network partition not planned:</strong> Split-brain scenarios not considered in architecture</li>
                            <li><strong>No maintenance mode usage:</strong> Taking indexers offline without enabling maintenance mode</li>
                        </ul>
                    </div>
                    
                    <div class="gap-card gap-warning">
                        <h4>ğŸŸ¡ Common Oversights</h4>
                        <ul>
                            <li><strong>SHC deployer vs cluster bundle:</strong> Confusion about when to use deployer vs cluster manager for configs</li>
                            <li><strong>Replication port blocked:</strong> Firewall rules don't include 9887 (replication) between peers</li>
                            <li><strong>Certificate expiration:</strong> Cluster communication fails when certs expire</li>
                            <li><strong>Unbalanced bucket distribution:</strong> Some indexers have 3x the buckets of others</li>
                            <li><strong>Captain election storms:</strong> Network instability causes repeated elections, impacting performance</li>
                        </ul>
                    </div>
                    
                    <div class="gap-card gap-info">
                        <h4>ğŸ”µ Often Overlooked</h4>
                        <ul>
                            <li><strong>Multisite search affinity:</strong> Not configuring searches to prefer local site</li>
                            <li><strong>Bucket fixup monitoring:</strong> No alerts when fixup takes longer than expected</li>
                            <li><strong>Rolling restart procedures:</strong> Restarting all peers simultaneously instead of rolling</li>
                            <li><strong>Summary replication delays:</strong> Summary data not replicating, breaking reports on failover</li>
                            <li><strong>License manager in cluster:</strong> License manager should be separate from CM</li>
                        </ul>
                    </div>
                    
                    <div class="gap-card gap-best-practice">
                        <h4>ğŸŸ¢ Best Practices Often Skipped</h4>
                        <ul>
                            <li><strong>Cluster health alerting:</strong> No proactive alerts on peer status, bucket state</li>
                            <li><strong>Documented runbooks:</strong> No step-by-step procedures for common failure scenarios</li>
                            <li><strong>Capacity headroom:</strong> Running at 90%+ capacity, no room for failover</li>
                            <li><strong>Regular DR drills:</strong> Annual DR testing not performed or documented</li>
                            <li><strong>Change management:</strong> Cluster changes made without change control process</li>
                        </ul>
                    </div>
                </div>

                <!-- Interview Q&A -->
                <h2><i class="fas fa-comments"></i> Interview Questions & Answers</h2>
                
                <div class="qa-section">
                    <div class="qa-item">
                        <button class="qa-question">Q: Explain the difference between Replication Factor and Search Factor. How do you choose values?</button>
                        <div class="qa-answer">
                            <p><strong>Definitions:</strong></p>
                            <div class="arch-diagram">
REPLICATION FACTOR (RF)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â€¢ Number of copies of RAW DATA maintained across cluster
â€¢ Includes both searchable and non-searchable copies
â€¢ Determines data durability - how many indexers can fail

SEARCH FACTOR (SF)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â€¢ Number of SEARCHABLE copies (with index files)
â€¢ Must be â‰¤ RF (can't search what doesn't exist)
â€¢ Determines search availability during failures

EXAMPLE: RF=3, SF=2
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ 3 copies of raw data exist
â€¢ 2 copies have index files (searchable)
â€¢ 1 copy is raw-only (can be promoted if needed)
â€¢ Can lose 1 indexer with no search impact
â€¢ Can lose 2 indexers and still have data (but reduced search)
                            </div>
                            <p><strong>Selection Criteria:</strong></p>
                            <ul>
                                <li><strong>Compliance requirements:</strong> Financial/healthcare often mandate RF=3+</li>
                                <li><strong>Recovery time objectives:</strong> Higher SF = faster recovery from failure</li>
                                <li><strong>Storage cost:</strong> RF=3 means 3x storage; balance cost vs resilience</li>
                                <li><strong>Indexer count:</strong> Need at least RF indexers; recommend RF+2 for maintenance</li>
                            </ul>
                            <p><strong>My recommendation:</strong> Start with RF=3, SF=2 for production. Provides good balance of durability and cost. Only increase SF to 3 if you need zero-impact failover.</p>
                        </div>
                    </div>

                    <div class="qa-item">
                        <button class="qa-question">Q: Your Cluster Manager fails. Walk me through the impact and recovery process.</button>
                        <div class="qa-answer">
                            <p><strong>Immediate Impact:</strong></p>
                            <div class="arch-diagram">
CLUSTER MANAGER FAILURE - IMPACT TIMELINE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

T+0 (CM fails):
â€¢ Existing searches continue normally
â€¢ Indexers continue accepting data
â€¢ Bucket replication STOPS
â€¢ No new primary bucket assignments

T+5 minutes:
â€¢ New data indexed but not replicated
â€¢ Single point of failure for new data
â€¢ Search heads see stale cluster state

T+60 minutes:
â€¢ Bucket fixup paused
â€¢ Imbalanced bucket distribution persists
â€¢ Risk of data loss increases

WHAT STILL WORKS:
â€¢ Indexing (local only)
â€¢ Searching existing data
â€¢ Forwarder connections

WHAT BREAKS:
â€¢ Bucket replication
â€¢ New bucket creation coordination
â€¢ Peer discovery for search heads
â€¢ Cluster configuration changes
                            </div>
                            <p><strong>Recovery Process:</strong></p>
                            <div class="arch-diagram">
OPTION 1: Restore Original CM
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. Identify failure cause (hardware, disk, config)
2. Restore from backup or fix issue
3. Start Splunk on CM
4. Verify: splunk show cluster-status
5. Monitor bucket fixup progress

OPTION 2: Promote Standby CM
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Prerequisites: Standby CM pre-configured, same server.conf

1. Verify standby has correct configuration
2. Update DNS/load balancer to point to standby
3. Start Splunk on standby CM
4. On EACH peer, update manager_uri:
   splunk edit cluster-config -manager_uri https://new-cm:8089
   splunk restart
5. On EACH search head, update manager_uri similarly

OPTION 3: Emergency CM Promotion
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
If no standby exists, convert any server to CM:

1. Install Splunk on new server
2. Configure as cluster manager:
   [clustering]
   mode = manager
   replication_factor = 3
   search_factor = 2
   pass4SymmKey = <same_as_original>
   cluster_label = <same_as_original>

3. Update all peers and search heads to new CM
4. Cluster will rebuild state from peers
                            </div>
                            <p><strong>Prevention:</strong> Always have a warm standby CM with replicated configuration. Test failover quarterly.</p>
                        </div>
                    </div>

                    <div class="qa-item">
                        <button class="qa-question">Q: How do you perform a rolling upgrade on an indexer cluster without downtime?</button>
                        <div class="qa-answer">
                            <p><strong>Rolling Upgrade Procedure:</strong></p>
                            <div class="arch-diagram">
PHASE 1: PREPARATION
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â–¡ Verify cluster health: splunk show cluster-status
â–¡ Ensure all peers are "Up" and bucket fixup complete
â–¡ Check RF/SF allows for one peer offline
â–¡ Backup cluster manager configuration
â–¡ Download new Splunk version to all nodes
â–¡ Schedule maintenance window (inform users)

PHASE 2: UPGRADE CLUSTER MANAGER FIRST
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
1. Enable maintenance mode on CM (prevents bucket fixup):
   splunk enable maintenance-mode

2. Stop Splunk on CM:
   splunk stop

3. Upgrade Splunk (rpm/deb/tgz):
   rpm -Uvh splunk-new-version.rpm

4. Start Splunk on CM:
   splunk start --accept-license

5. Verify CM is up:
   splunk show cluster-status

PHASE 3: UPGRADE PEERS ONE AT A TIME
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
For EACH peer (one at a time):

1. Offline the peer:
   splunk offline

2. Wait for peer to complete offline:
   - Watch CM status: peer shows "Decommissioned"
   - Primaries reassigned to other peers

3. Stop Splunk:
   splunk stop

4. Upgrade Splunk:
   rpm -Uvh splunk-new-version.rpm

5. Start Splunk:
   splunk start --accept-license

6. Wait for peer to rejoin cluster:
   - Status changes from "Up" to "Searchable"
   - Bucket fixup begins

7. Wait for bucket fixup to complete before next peer

PHASE 4: FINALIZE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
1. Disable maintenance mode:
   splunk disable maintenance-mode

2. Verify all peers healthy:
   splunk show cluster-status --verbose

3. Upgrade search heads (similar rolling process)
                            </div>
                            <p><strong>Critical Notes:</strong></p>
                            <ul>
                                <li>Never upgrade more than (RF-SF) peers simultaneously</li>
                                <li>Wait for bucket fixup between peer upgrades</li>
                                <li>Upgrade CM first - it must be â‰¥ peer versions</li>
                                <li>Search heads can be upgraded in parallel (SHC uses rolling restart)</li>
                            </ul>
                        </div>
                    </div>

                    <div class="qa-item">
                        <button class="qa-question">Q: Explain multisite clustering. When would you use site_replication_factor vs regular replication_factor?</button>
                        <div class="qa-answer">
                            <p><strong>Multisite Clustering Overview:</strong></p>
                            <div class="arch-diagram">
MULTISITE VS SINGLE-SITE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

SINGLE-SITE CLUSTERING:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
replication_factor = 3
search_factor = 2

â€¢ All copies distributed across all peers equally
â€¢ No awareness of physical location
â€¢ Good for: Single datacenter deployments

MULTISITE CLUSTERING:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
site_replication_factor = origin:2, total:3
site_search_factor = origin:1, total:2

â€¢ Data replication aware of site boundaries
â€¢ Ensures copies exist at multiple geographic locations
â€¢ Good for: DR, data locality requirements

EXAMPLE BREAKDOWN:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
site_replication_factor = origin:2, total:3

"origin:2" â†’ 2 copies at the site where data originated
"total:3"  â†’ 3 copies total across all sites

If data arrives at Site1:
â€¢ 2 copies stay at Site1
â€¢ 1 copy replicates to Site2

site_search_factor = origin:1, total:2

â€¢ 1 searchable copy at origin site
â€¢ 2 searchable copies total
â€¢ Site2 may have searchable or non-searchable copy
                            </div>
                            <p><strong>When to Use Multisite:</strong></p>
                            <ul>
                                <li><strong>Disaster Recovery:</strong> Ensure data survives complete site failure</li>
                                <li><strong>Data Sovereignty:</strong> Keep data in specific geographic regions</li>
                                <li><strong>Search Locality:</strong> Users at each site search local data faster</li>
                                <li><strong>Compliance:</strong> Some regulations require geographic redundancy</li>
                            </ul>
                            <p><strong>Configuration Example:</strong></p>
                            <div class="arch-diagram">
# Active-Active (both sites serve searches)
site_replication_factor = origin:2, site1:1, site2:1, total:3
site_search_factor = origin:1, site1:1, site2:1, total:2

# Active-Passive (DR site for failover only)
site_replication_factor = origin:2, total:3
site_search_factor = origin:2, total:2
# site2 only has non-searchable copies until promoted
                            </div>
                        </div>
                    </div>

                    <div class="qa-item">
                        <button class="qa-question">Q: How do you troubleshoot a Search Head Cluster that's having captain election issues?</button>
                        <div class="qa-answer">
                            <p><strong>Captain Election Troubleshooting:</strong></p>
                            <div class="arch-diagram">
STEP 1: CHECK CURRENT STATE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# On any SHC member
splunk show shcluster-status -auth admin:password

Look for:
â€¢ Captain: Which member is captain (or "None")
â€¢ Members: List of all members and their status
â€¢ Service Ready: Whether cluster is serving users

STEP 2: COMMON CAUSES
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

CAUSE: Network partition
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Symptoms: Members see different captains, or no captain
Check: Can members ping each other on management port (8089)?
Fix: Resolve network connectivity

CAUSE: Quorum not met
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Symptoms: "Waiting for captain election"
Check: Need >50% of members for quorum
â€¢ 3 members: need 2
â€¢ 5 members: need 3
Fix: Bring offline members back online

CAUSE: pass4SymmKey mismatch
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Symptoms: Members can't communicate, authentication errors
Check: Compare pass4SymmKey in server.conf on all members
Fix: Ensure identical key on all members, restart

CAUSE: mgmt_uri misconfiguration
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Symptoms: Members can't find each other
Check: Each member's mgmt_uri must resolve correctly
Fix: Ensure DNS/hostnames resolve, use FQDN

STEP 3: FORCE CAPTAIN ELECTION
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# If cluster is stuck, force election to specific member
splunk edit shcluster-config -election_timeout_ms 60000
splunk restart

# Or bootstrap to specific captain
splunk bootstrap shcluster-captain \
  -servers_list "https://sh1:8089,https://sh2:8089,https://sh3:8089" \
  -auth admin:password

STEP 4: EXAMINE LOGS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Key log file
/opt/splunk/var/log/splunk/splunkd.log

# Search for SHC-related entries
grep -i "shcluster\|captain\|raft" splunkd.log | tail -100
                            </div>
                            <p><strong>Prevention:</strong> Monitor captain stability, alert on election frequency, ensure network reliability between members.</p>
                        </div>
                    </div>

                    <div class="qa-item">
                        <button class="qa-question">Q: Design a Splunk cluster for a 1TB/day deployment with 99.9% availability SLA.</button>
                        <div class="qa-answer">
                            <p><strong>Architecture Design:</strong></p>
                            <div class="arch-diagram">
HIGH-AVAILABILITY ARCHITECTURE FOR 1TB/DAY
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚         LOAD BALANCER (HA pair)     â”‚
                    â”‚      F5 / HAProxy / AWS ALB         â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â–¼                         â–¼                         â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Search Head 1 â”‚         â”‚  Search Head 2 â”‚         â”‚  Search Head 3 â”‚
    â”‚  (SHC Member)  â”‚â—„â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚  (SHC Captain) â”‚â—„â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚  (SHC Member)  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                         â”‚                         â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚         CLUSTER MANAGER (HA)        â”‚
                    â”‚    Primary + Warm Standby           â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â–¼           â–¼           â–¼         â–¼         â–¼           â–¼           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”
â”‚ IDX 1 â”‚   â”‚ IDX 2 â”‚   â”‚ IDX 3 â”‚ â”‚ IDX 4 â”‚ â”‚ IDX 5 â”‚   â”‚ IDX 6 â”‚   â”‚ IDX 7 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚           â”‚           â”‚         â”‚         â”‚           â”‚           â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚         SHARED STORAGE / S3         â”‚
                    â”‚    SmartStore for warm/cold data    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

SIZING CALCULATIONS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Daily Volume: 1 TB/day
Retention: 90 days hot, 1 year warm
Compression: ~50%

Storage per indexer:
â€¢ Hot: 1TB Ã— 0.5 Ã— 14 days / 7 indexers = 1 TB each
â€¢ Warm: SmartStore (S3) = ~45 TB total
â€¢ RF=3 multiplier: 3 TB raw storage per indexer

Indexer Specs:
â€¢ CPU: 16 cores
â€¢ RAM: 64 GB
â€¢ Disk: 4 TB NVMe (hot) + SmartStore cache (1 TB SSD)

CONFIGURATION
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

[clustering]
mode = manager
replication_factor = 3        # Survive 2 indexer failures
search_factor = 2             # 2 searchable copies
cluster_label = prod_1tb

# 7 indexers with RF=3:
# Can lose 2 indexers and maintain full RF
# Can lose 4 indexers and still have data (degraded)

AVAILABILITY MATH
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
99.9% = 8.76 hours downtime/year

Components:
â€¢ Load balancer: 99.99% (HA pair)
â€¢ SHC: 99.99% (3 members, auto-failover)
â€¢ Indexer Cluster: 99.99% (RF=3, 7 peers)
â€¢ Cluster Manager: 99.95% (with warm standby)

Combined: ~99.9% achievable with this architecture
                            </div>
                            <p><strong>Additional Components:</strong></p>
                            <ul>
                                <li><strong>Deployer:</strong> For SHC app deployment</li>
                                <li><strong>License Manager:</strong> Separate from CM, can be HA</li>
                                <li><strong>Monitoring Console:</strong> Dedicated instance</li>
                                <li><strong>Deployment Server:</strong> For forwarder management</li>
                            </ul>
                        </div>
                    </div>

                    <div class="qa-item">
                        <button class="qa-question">Q: What's the difference between the Deployer and Cluster Manager? When do you use each?</button>
                        <div class="qa-answer">
                            <p><strong>Comparison:</strong></p>
                            <div class="arch-diagram">
DEPLOYER VS CLUSTER MANAGER
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

DEPLOYER (Search Head Cluster)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Purpose: Distribute apps/configs to SHC members
Scope: Search Head Cluster only
Location: Separate from SHC members

Manages:
â€¢ Apps (SA-*, TA-*, custom apps)
â€¢ UI configurations
â€¢ Saved searches, dashboards, reports
â€¢ Lookup files
â€¢ Authentication configs

Push Method:
â€¢ Place apps in: $SPLUNK_HOME/etc/shcluster/apps/
â€¢ Apply: splunk apply shcluster-bundle -target https://sh1:8089

CLUSTER MANAGER (Indexer Cluster)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Purpose: Coordinate indexer cluster operations
Scope: Indexer Cluster only
Location: Dedicated server

Manages:
â€¢ Bucket replication
â€¢ Primary bucket assignment
â€¢ Peer membership
â€¢ Index configurations
â€¢ Props/transforms (via cluster bundle)

Push Method:
â€¢ Place configs in: $SPLUNK_HOME/etc/manager-apps/
â€¢ Apply: splunk apply cluster-bundle

WHAT GOES WHERE?
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

                    â”‚ Deployer (SHC) â”‚ Cluster Manager â”‚
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
Apps for searching  â”‚      âœ“         â”‚                 â”‚
Dashboards         â”‚      âœ“         â”‚                 â”‚
Saved searches     â”‚      âœ“         â”‚                 â”‚
props.conf (parse) â”‚                â”‚       âœ“         â”‚
transforms.conf    â”‚                â”‚       âœ“         â”‚
indexes.conf       â”‚                â”‚       âœ“         â”‚
inputs.conf        â”‚                â”‚       âœ“ (peers) â”‚
TAs (field extract)â”‚      âœ“ (views) â”‚       âœ“ (parse) â”‚

NOTE ON TAs:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TAs often have BOTH search-time and index-time components:
â€¢ Deployer: dashboards, saved searches, eventtypes
â€¢ CM: props.conf, transforms.conf for parsing
â€¢ Some TAs need deployment to BOTH
                            </div>
                            <p><strong>Best Practice:</strong> For TAs, deploy parsing configs (props/transforms) via Cluster Manager, and UI components via Deployer. Some shops split TAs into two packages.</p>
                        </div>
                    </div>

                    <div class="qa-item">
                        <button class="qa-question">Q: How do you handle a "split-brain" scenario in a multisite cluster?</button>
                        <div class="qa-answer">
                            <p><strong>Split-Brain Scenario:</strong></p>
                            <div class="arch-diagram">
SPLIT-BRAIN: WAN LINK FAILURE BETWEEN SITES
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

       SITE 1 (NYC)                      SITE 2 (LAX)
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       âœ—        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ CM (Primary)  â”‚â—„â”€â”€â”€â”€â”€â”€Xâ”€â”€â”€â”€â”€â”€â”€â–ºâ”‚ Indexers      â”‚
    â”‚ Indexers      â”‚    WAN DOWN    â”‚ (No CM access)â”‚
    â”‚ Search Heads  â”‚                â”‚ Search Heads  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    
    Site 1: CM continues,            Site 2: Peers lose
    bucket ops normal                CM heartbeat, enter
                                     "detention" mode

WHAT HAPPENS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. Site 2 peers can't reach CM
2. After heartbeat timeout (default 60s), peers enter detention
3. Detained peers:
   â€¢ Continue indexing locally
   â€¢ Stop accepting search requests
   â€¢ Stop bucket replication
4. Search heads at Site 2 see reduced capacity
5. Data at Site 2 accumulates without replication

PREVENTION: SITE MAJORITY DESIGN
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ Place CM at site with majority of indexers
â€¢ OR use 3 sites (tiebreaker architecture)

Three-Site Tiebreaker:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Site 1  â”‚     â”‚  Site 2  â”‚     â”‚  Site 3  â”‚
â”‚  (IDX)   â”‚â—„â”€â”€â”€â–ºâ”‚  (IDX)   â”‚â—„â”€â”€â”€â–ºâ”‚  (CM only)â”‚
â”‚  3 peers â”‚     â”‚  3 peers â”‚     â”‚ Tiebreakerâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

If Site 1 loses connectivity:
â€¢ Site 2 + Site 3 form majority
â€¢ CM at Site 3 maintains cluster
                            </div>
                            <p><strong>Recovery Procedure:</strong></p>
                            <div class="arch-diagram">
RECOVERING FROM SPLIT-BRAIN
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1. Restore WAN connectivity

2. Detained peers automatically reconnect to CM

3. Bucket fixup begins:
   â€¢ Data indexed during split needs replication
   â€¢ May take hours depending on data volume

4. Monitor progress:
   splunk show cluster-status --verbose
   # Watch for "Searchable" status on all peers

5. Validate data integrity:
   # Compare event counts between sites
   | tstats count where index=* by splunk_server

MANUAL INTERVENTION (if automatic recovery fails):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# On detained peers, force restart
splunk offline
splunk restart

# If peer won't rejoin, remove and re-add
# On CM:
splunk remove cluster-peer -peer_uri https://peer:8089
# Re-configure peer from scratch
                            </div>
                        </div>
                    </div>
                </div>

                <h2><i class="fas fa-link"></i> Related Resources</h2>
                <ul>
                    <li><a href="architecture.html">Splunk Architecture</a></li>
                    <li><a href="administration.html">Administration</a></li>
                    <li><a href="troubleshooting.html">Troubleshooting</a></li>
                    <li><a href="enterprise-scenarios.html">Enterprise Scenarios</a></li>
                    <li><a href="monitoring-console.html">Monitoring Console</a></li>
                </ul>

            </main>
        </div>
    </div>
    
    <script>
        function toggleSidebar() {
            const sidebar = document.getElementById('sidebar');
            const mainWrapper = document.getElementById('mainWrapper');
            const overlay = document.getElementById('sidebarOverlay');
            
            if (window.innerWidth <= 768) {
                sidebar.classList.toggle('open');
                overlay.classList.toggle('active');
            } else {
                sidebar.classList.toggle('collapsed');
                mainWrapper.classList.toggle('expanded');
            }
        }
        
        // Q&A toggle
        document.querySelectorAll('.qa-question').forEach(btn => {
            btn.addEventListener('click', () => {
                const answer = btn.nextElementSibling;
                const isOpen = answer.style.display === 'block';
                document.querySelectorAll('.qa-answer').forEach(a => a.style.display = 'none');
                if (!isOpen) answer.style.display = 'block';
            });
        });
        
        window.addEventListener('resize', () => {
            const sidebar = document.getElementById('sidebar');
            const overlay = document.getElementById('sidebarOverlay');
            if (window.innerWidth > 768) {
                sidebar.classList.remove('open');
                overlay.classList.remove('active');
            }
        });
    </script>
</body>
</html>
