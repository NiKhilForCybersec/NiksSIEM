<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Alerting & Alert Actions | Splunk</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    <link rel="stylesheet" href="../assets/css/main.css">


</head>
<body>
    <div class="app-container">
            <!-- Sidebar -->
        <!-- Sidebar -->
        <!-- Sidebar -->
    <aside class="sidebar" id="sidebar">
        <div class="sidebar-header">
            <a href="../index.html" class="sidebar-brand">
                <i class="fas fa-shield-alt"></i>
                <span>Nik's SIEM</span>
            </a>
            <button class="sidebar-toggle" onclick="toggleSidebar()" title="Collapse sidebar">
                <i class="fas fa-chevron-left"></i>
            </button>
        </div>
        
        <div class="sidebar-content">
            <!-- Platforms Section -->
            <div class="sidebar-section">
                <div class="sidebar-section-title">Platforms</div>
                <ul class="sidebar-nav">
                    <li class="sidebar-nav-item"><a href="../xsiam/index.html" class="sidebar-nav-link"><span class="platform-dot"></span><span>XSIAM</span></a></li>
                    <li class="sidebar-nav-item"><a href="../splunk/index.html" class="sidebar-nav-link active"><span class="platform-dot"></span><span>Splunk</span></a></li>
                    <li class="sidebar-nav-item"><a href="../sentinel/index.html" class="sidebar-nav-link"><span class="platform-dot"></span><span>Sentinel</span></a></li>
                    <li class="sidebar-nav-item"><a href="../crowdstrike/index.html" class="sidebar-nav-link"><span class="platform-dot"></span><span>CrowdStrike</span></a></li>
                    <li class="sidebar-nav-item"><a href="../cortex/index.html" class="sidebar-nav-link"><span class="platform-dot"></span><span>Cortex XDR</span></a></li>
                    <li class="sidebar-nav-item"><a href="../mde/index.html" class="sidebar-nav-link"><span class="platform-dot"></span><span>Defender for Endpoint</span></a></li>
                    <li class="sidebar-nav-item"><a href="../operations/index.html" class="sidebar-nav-link"><span class="platform-dot"></span><span>SOC Operations</span></a></li>
                </ul>
            </div>

            <div class="sidebar-divider"></div>
            
            <!-- Getting Started -->
            <div class="sidebar-section">
                <div class="sidebar-section-title">Getting Started</div>
                <ul class="sidebar-nav">
                    <li class="sidebar-nav-item"><a href="index.html" class="sidebar-nav-link"><i class="fas fa-home"></i><span>Overview</span></a></li>
                    <li class="sidebar-nav-item"><a href="architecture.html" class="sidebar-nav-link"><i class="fas fa-sitemap"></i><span>Architecture</span></a></li>
                    <li class="sidebar-nav-item"><a href="apps-tas.html" class="sidebar-nav-link"><i class="fas fa-puzzle-piece"></i><span>Apps & TAs</span></a></li>
                </ul>
            </div>
            
            <!-- Data Collection -->
            <div class="sidebar-section">
                <div class="sidebar-section-title">Data Collection</div>
                <ul class="sidebar-nav">
                    <li class="sidebar-nav-item"><a href="data-ingestion.html" class="sidebar-nav-link"><i class="fas fa-database"></i><span>Data Ingestion</span></a></li>
                    <li class="sidebar-nav-item"><a href="forwarders.html" class="sidebar-nav-link"><i class="fas fa-paper-plane"></i><span>Forwarders</span></a></li>
                    <li class="sidebar-nav-item"><a href="custom-log-onboarding.html" class="sidebar-nav-link"><i class="fas fa-plus-circle"></i><span>Custom Log Onboarding</span></a></li>
                    <li class="sidebar-nav-item"><a href="parsing-flows.html" class="sidebar-nav-link"><i class="fas fa-stream"></i><span>Parsing & Props/Transforms</span></a></li>
                    <li class="sidebar-nav-item"><a href="retention-tiers.html" class="sidebar-nav-link"><i class="fas fa-archive"></i><span>Index & Retention</span></a></li>
                </ul>
            </div>
            
            <!-- Query & Analysis -->
            <div class="sidebar-section">
                <div class="sidebar-section-title">Query & Analysis</div>
                <ul class="sidebar-nav">
                    <li class="sidebar-nav-item"><a href="spl-fundamentals.html" class="sidebar-nav-link"><i class="fas fa-terminal"></i><span>SPL Fundamentals</span></a></li>
                    <li class="sidebar-nav-item"><a href="spl-intermediate.html" class="sidebar-nav-link"><i class="fas fa-code"></i><span>SPL Intermediate</span></a></li>
                    <li class="sidebar-nav-item"><a href="spl-advanced.html" class="sidebar-nav-link"><i class="fas fa-rocket"></i><span>SPL Advanced</span></a></li>
                    <li class="sidebar-nav-item"><a href="data-models.html" class="sidebar-nav-link"><i class="fas fa-cubes"></i><span>Data Models & CIM</span></a></li>
                    <li class="sidebar-nav-item"><a href="knowledge-objects.html" class="sidebar-nav-link"><i class="fas fa-brain"></i><span>Knowledge Objects</span></a></li>
                    <li class="sidebar-nav-item"><a href="dashboards.html" class="sidebar-nav-link"><i class="fas fa-chart-line"></i><span>Dashboards</span></a></li>
                </ul>
            </div>
            
            <!-- Detection & Response -->
            <div class="sidebar-section">
                <div class="sidebar-section-title">Detection & Response</div>
                <ul class="sidebar-nav">
                    <li class="sidebar-nav-item"><a href="es-overview.html" class="sidebar-nav-link"><i class="fas fa-shield-alt"></i><span>Enterprise Security</span></a></li>
                    <li class="sidebar-nav-item"><a href="correlation-searches.html" class="sidebar-nav-link"><i class="fas fa-project-diagram"></i><span>Correlation Searches</span></a></li>
                    <li class="sidebar-nav-item"><a href="notable-events.html" class="sidebar-nav-link"><i class="fas fa-bell"></i><span>Notable Events</span></a></li>
                    <li class="sidebar-nav-item"><a href="risk-based-alerting.html" class="sidebar-nav-link"><i class="fas fa-exclamation-triangle"></i><span>Risk-Based Alerting</span></a></li>
                    <li class="sidebar-nav-item"><a href="alerting-actions.html" class="sidebar-nav-link"><i class="fas fa-bolt"></i><span>Alerting & Actions</span></a></li>
                    <li class="sidebar-nav-item"><a href="threat-hunting.html" class="sidebar-nav-link"><i class="fas fa-search"></i><span>Threat Hunting</span></a></li>
                </ul>
            </div>
            
            <!-- Security Use Cases -->
            <div class="sidebar-section">
                <div class="sidebar-section-title">Security Use Cases</div>
                <ul class="sidebar-nav">
                    <li class="sidebar-nav-item"><a href="security-usecases.html" class="sidebar-nav-link"><i class="fas fa-crosshairs"></i><span>Detection Use Cases</span></a></li>
                    <li class="sidebar-nav-item"><a href="enterprise-scenarios.html" class="sidebar-nav-link"><i class="fas fa-building"></i><span>Enterprise Scenarios</span></a></li>
                </ul>
            </div>
            
            <!-- Automation & Intel -->
            <div class="sidebar-section">
                <div class="sidebar-section-title">Automation & Intel</div>
                <ul class="sidebar-nav">
                    <li class="sidebar-nav-item"><a href="soar.html" class="sidebar-nav-link"><i class="fas fa-robot"></i><span>SOAR</span></a></li>
                    <li class="sidebar-nav-item"><a href="threat-intel.html" class="sidebar-nav-link"><i class="fas fa-skull-crossbones"></i><span>Threat Intelligence</span></a></li>
                </ul>
            </div>
            
            <!-- Operations -->
            <div class="sidebar-section">
                <div class="sidebar-section-title">Operations</div>
                <ul class="sidebar-nav">
                    <li class="sidebar-nav-item"><a href="troubleshooting.html" class="sidebar-nav-link"><i class="fas fa-wrench"></i><span>Troubleshooting</span></a></li>
                    <li class="sidebar-nav-item"><a href="performance-tuning.html" class="sidebar-nav-link"><i class="fas fa-tachometer-alt"></i><span>Performance Tuning</span></a></li>
                    <li class="sidebar-nav-item"><a href="monitoring-console.html" class="sidebar-nav-link"><i class="fas fa-heartbeat"></i><span>Monitoring Console</span></a></li>
                </ul>
            </div>
            
            <!-- Administration -->
            <div class="sidebar-section">
                <div class="sidebar-section-title">Administration</div>
                <ul class="sidebar-nav">
                    <li class="sidebar-nav-item"><a href="administration.html" class="sidebar-nav-link"><i class="fas fa-cog"></i><span>Administration</span></a></li>
                    <li class="sidebar-nav-item"><a href="security-rbac.html" class="sidebar-nav-link"><i class="fas fa-users-cog"></i><span>Security & RBAC</span></a></li>
                    <li class="sidebar-nav-item"><a href="clustering-ha.html" class="sidebar-nav-link"><i class="fas fa-server"></i><span>Clustering & HA</span></a></li>
                </ul>
            </div>
            
            <!-- Deployment -->
            <div class="sidebar-section">
                <div class="sidebar-section-title">Deployment</div>
                <ul class="sidebar-nav">
                    <li class="sidebar-nav-item"><a href="splunk-cloud.html" class="sidebar-nav-link"><i class="fas fa-cloud"></i><span>Splunk Cloud</span></a></li>
                </ul>
            </div>
        </div>
    </aside>
    
    <div class="sidebar-overlay" id="sidebarOverlay" onclick="toggleSidebar()"></div>
        <div class="main-wrapper" id="mainWrapper">
            <button class="mobile-toggle" onclick="toggleSidebar()"><i class="fas fa-bars"></i></button>
            <main class="main-content">
                <div class="breadcrumb">
                    <a href="../index.html">Home</a><span class="separator">/</span>
                    <a href="index.html">Splunk</a><span class="separator">/</span>
                    <span class="current">Alerting & Alert Actions</span>
                </div>

                <div class="hero-section">
                    <span class="version-badge">Splunk 9.x / Enterprise Security</span>
                    <h1 class="hero-title"><i class="fas fa-bolt" style="color: #3fb950;"></i> Alerting & Alert Actions</h1>
                    <p class="hero-subtitle">Complete guide to Splunk alerting mechanisms, alert actions, throttling strategies, and integration with ticketing systems.</p>
                </div>

                <!-- Alert Types -->
                <h2><i class="fas fa-bell"></i> Alert Types</h2>
                <div class="config-section">
                    <div class="arch-diagram">
SPLUNK ALERT TYPES
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

SCHEDULED ALERTS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ Run on defined schedule (cron or interval)
â€¢ Best for: Periodic checks, compliance reports, batch processing
â€¢ Latency: Based on schedule (1 min to 1 day)
â€¢ Resource impact: Predictable, scheduled

Example: Check for privilege escalation every 5 minutes
Schedule: */5 * * * *
Search: index=security EventCode=4672 | stats count by user | where count > 10

REAL-TIME ALERTS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ Continuously running search
â€¢ Best for: Critical security events requiring immediate response
â€¢ Latency: Near-instant (seconds)
â€¢ Resource impact: HIGH - consumes search slots continuously

Example: Critical malware detection
Search: index=endpoint signature_severity=critical
Trigger: Per-Result (each match triggers immediately)

âš ï¸ WARNING: Real-time searches are resource-intensive
Use sparingly - limit to truly critical alerts

ROLLING WINDOW ALERTS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ Scheduled but looks at sliding time window
â€¢ Best for: Trend detection, threshold monitoring
â€¢ Latency: Based on schedule
â€¢ Resource impact: Medium

Example: Alert if error rate exceeds baseline over 1 hour
Schedule: Every 5 minutes
Time Range: Last 60 minutes (rolling)
Search: index=app error | timechart count | where count > 100

CORRELATION SEARCH ALERTS (ES)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ Enterprise Security specific
â€¢ Creates notable events, not traditional alerts
â€¢ Integrates with Risk-Based Alerting
â€¢ Best for: Complex multi-stage attack detection
                    </div>
                </div>

                <!-- Alert Configuration -->
                <h2><i class="fas fa-cog"></i> Alert Configuration</h2>
                <div class="config-section">
                    <h3>Creating Alerts via savedsearches.conf</h3>
                    <div class="arch-diagram">
# /opt/splunk/etc/apps/my_security_app/local/savedsearches.conf

[Brute Force Detection]
search = index=security EventCode=4625 | stats count by src_ip, user | where count > 10
cron_schedule = */5 * * * *
enableSched = 1
dispatch.earliest_time = -10m
dispatch.latest_time = now

# Trigger conditions
alert_type = number of events
alert_comparator = greater than
alert_threshold = 0

# Alert suppression (throttling)
alert.suppress = 1
alert.suppress.period = 1h
alert.suppress.fields = src_ip, user

# Alert actions
action.email = 1
action.email.to = soc@company.com
action.email.subject = [ALERT] Brute Force Detected - $result.src_ip$
action.email.include.results_link = 1
action.email.include.view_link = 1

# Severity
alert.severity = 4

[Critical: Ransomware Indicators]
search = index=endpoint (file_extension IN ("*.encrypted", "*.locky", "*.wcry")) OR (process_name="vssadmin.exe" command_line="*delete shadows*")
realtime_schedule = 1
dispatch.earliest_time = rt-1m
dispatch.latest_time = rt

alert_type = always
action.webhook = 1
action.webhook.param.url = https://pagerduty.com/integration/xxxx
alert.severity = 5
                    </div>

                    <h3>Trigger Conditions</h3>
                    <div class="arch-diagram">
TRIGGER CONDITION OPTIONS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

alert_type = number of events
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
alert_comparator = greater than | less than | equal to | 
                   not equal to | drops by | rises by
alert_threshold = <number>

Examples:
â€¢ "greater than 0" â†’ Alert if any results
â€¢ "greater than 100" â†’ Alert if more than 100 events
â€¢ "drops by 50%" â†’ Alert if count drops 50% from previous

alert_type = number of hosts
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Count unique hosts in results
Useful for: Outbreak detection, spread monitoring

alert_type = number of sources
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Count unique sources
Useful for: DDoS detection, distributed attacks

alert_type = custom
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Define custom condition in search
Use | where or | eval to define trigger logic in search itself

alert_type = always
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Trigger every time search runs (for real-time per-result alerts)
                    </div>
                </div>

                <!-- Alert Actions -->
                <h2><i class="fas fa-paper-plane"></i> Alert Actions</h2>
                <div class="config-section">
                    <h3>Built-in Alert Actions</h3>
                    <div class="arch-diagram">
BUILT-IN ALERT ACTIONS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1. EMAIL
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
action.email = 1
action.email.to = user@company.com, team@company.com
action.email.cc = manager@company.com
action.email.from = splunk-alerts@company.com
action.email.subject = [ALERT] $name$ triggered
action.email.message.alert = Alert Details:\n\nTime: $trigger_time$\nResults: $result.count$
action.email.format = html
action.email.inline = 1
action.email.sendresults = 1
action.email.include.results_link = 1
action.email.priority = 1

2. WEBHOOK
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
action.webhook = 1
action.webhook.param.url = https://api.service.com/webhook

# Payload is JSON with search results
# Supports: Slack, Teams, PagerDuty, custom APIs

3. SCRIPT
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
action.script = 1
action.script.filename = my_alert_script.py
# Script location: $SPLUNK_HOME/bin/scripts/
# Receives: stdin with JSON payload

4. LOG EVENT
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
action.logevent = 1
action.logevent.param.index = alerts
action.logevent.param.source = alert:$name$

5. NOTABLE EVENT (ES)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
action.notable = 1
action.notable.param.rule_title = $name$
action.notable.param.rule_description = $description$
action.notable.param.severity = critical
action.notable.param.drilldown_name = View Related Events
action.notable.param.drilldown_search = index=security $result.src_ip$

6. RISK MODIFIER (ES)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
action.risk = 1
action.risk.param.risk_object = src
action.risk.param.risk_object_type = system
action.risk.param.risk_score = 50
                    </div>

                    <h3>Custom Webhook Integrations</h3>
                    <div class="arch-diagram">
SLACK INTEGRATION
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# savedsearches.conf
[Critical Security Alert]
...
action.webhook = 1
action.webhook.param.url = https://hooks.slack.com/services/XXX/YYY/ZZZ

# Slack receives JSON payload, format with Slack app or middleware

# Alternative: Use Slack Alert Action app from Splunkbase
action.slack = 1
action.slack.param.channel = #security-alerts
action.slack.param.message = :rotating_light: *$name$*\nTime: $trigger_time$\nResults: $result.count$

PAGERDUTY INTEGRATION
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

action.webhook = 1
action.webhook.param.url = https://events.pagerduty.com/v2/enqueue

# Requires Events API v2 format:
# {
#   "routing_key": "integration_key",
#   "event_action": "trigger",
#   "payload": {
#     "summary": "$name$",
#     "severity": "critical",
#     "source": "Splunk"
#   }
# }

SERVICENOW INTEGRATION
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Use ServiceNow app from Splunkbase
action.snow_event = 1
action.snow_event.param.node = $result.host$
action.snow_event.param.type = Splunk Alert
action.snow_event.param.severity = 2
action.snow_event.param.description = $name$: $result.count$ events detected

MICROSOFT TEAMS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

action.webhook = 1
action.webhook.param.url = https://company.webhook.office.com/webhookb2/xxx

# Teams expects Adaptive Card format
# Use middleware or Teams Alert Action app
                    </div>
                </div>

                <!-- Throttling & Suppression -->
                <h2><i class="fas fa-tachometer-alt"></i> Throttling & Suppression</h2>
                <div class="config-section">
                    <div class="arch-diagram">
ALERT THROTTLING STRATEGIES
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

BASIC SUPPRESSION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
alert.suppress = 1
alert.suppress.period = 1h

â†’ Suppresses ALL alerts for 1 hour after first trigger
â†’ Use for: Low-priority alerts, status checks

FIELD-BASED SUPPRESSION (Recommended)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
alert.suppress = 1
alert.suppress.period = 4h
alert.suppress.fields = src_ip, user

â†’ Suppresses alerts with SAME src_ip AND user for 4 hours
â†’ Different src_ip or user combinations still trigger
â†’ Use for: Per-entity throttling

Example scenarios:
â€¢ src_ip=1.2.3.4, user=john â†’ Triggered, then suppressed 4h
â€¢ src_ip=1.2.3.4, user=jane â†’ Triggers (different user)
â€¢ src_ip=5.6.7.8, user=john â†’ Triggers (different IP)

GROUP-BASED SUPPRESSION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
alert.suppress.group_name = security_alerts

â†’ Groups multiple alerts under same suppression key
â†’ Useful for related alerts that shouldn't all fire

DIGEST MODE
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
alert.digest_mode = 1

â†’ Aggregates all results into single alert
â†’ Instead of: 100 events = 100 alerts
â†’ Result: 100 events = 1 alert with 100 results attached
â†’ Use for: High-volume alerts where you want summary

COMBINATION STRATEGY (Best Practice)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
alert.suppress = 1
alert.suppress.period = 1h
alert.suppress.fields = src_ip
alert.digest_mode = 1
cron_schedule = */15 * * * *

â†’ Check every 15 minutes
â†’ Digest all results per run
â†’ Suppress per-src_ip for 1 hour
â†’ Result: Manageable alert volume
                    </div>

                    <h3>Alert Fatigue Prevention</h3>
                    <div class="arch-diagram">
ALERT FATIGUE MITIGATION STRATEGIES
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1. THRESHOLD TUNING
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Problem: Alert triggers on normal activity
Solution: Analyze baseline, set appropriate threshold

# Before tuning - understand your baseline
index=security EventCode=4625 
| timechart span=1h count
| stats avg(count) as avg_failures, stdev(count) as stdev_failures

# Set threshold at baseline + 3 standard deviations
threshold = avg + (3 * stdev)

2. ALLOWLISTING
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Problem: Known-good activity triggers alerts

# In search, exclude known-good
index=security EventCode=4625 
NOT [| inputlookup allowed_service_accounts.csv | fields user]
| stats count by src_ip, user 
| where count > 10

3. TIME-BASED FILTERING
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Problem: Scheduled tasks trigger during maintenance windows

# Exclude maintenance windows
| eval hour = strftime(_time, "%H")
| where NOT (hour >= 2 AND hour <= 4)

4. PRIORITIZATION TIERS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Create alert tiers with different actions:

Critical (Severity 5):
â€¢ PagerDuty + Slack + Email
â€¢ No suppression
â€¢ 24/7 response required

High (Severity 4):
â€¢ Slack + Email
â€¢ 30-minute suppression
â€¢ Business hours response

Medium (Severity 3):
â€¢ Email digest
â€¢ 4-hour suppression
â€¢ Next business day

Low (Severity 2):
â€¢ Log only
â€¢ Daily digest email
â€¢ Weekly review
                    </div>
                </div>

                <!-- Decision Trees -->
                <h2><i class="fas fa-project-diagram"></i> Alerting Decision Trees</h2>
                
                <div class="config-section">
                    <h3>Decision Tree: Alert Type Selection</h3>
                    <div class="arch-diagram">
Decision Tree: Choosing Alert Type
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

START: What's the detection requirement?
â”‚
â”œâ”€â–º [Must detect within seconds]
â”‚   â”‚
â”‚   â””â”€â–º Is it truly critical (ransomware, active breach)?
â”‚       â”‚
â”‚       â”œâ”€â–º YES â†’ Real-time alert
â”‚       â”‚         âš ï¸ Limit to 3-5 real-time alerts total
â”‚       â”‚         Use per-result trigger
â”‚       â”‚
â”‚       â””â”€â–º NO â†’ Scheduled alert every 1-5 minutes
â”‚                 Less resource intensive
â”‚
â”œâ”€â–º [Detection within minutes acceptable]
â”‚   â”‚
â”‚   â””â”€â–º What's the expected event volume?
â”‚       â”‚
â”‚       â”œâ”€â–º Low (<100/hour) â†’ Scheduled every 5 minutes
â”‚       â”‚
â”‚       â”œâ”€â–º Medium (100-1000/hour) â†’ Scheduled every 15 minutes
â”‚       â”‚                            Use digest mode
â”‚       â”‚
â”‚       â””â”€â–º High (>1000/hour) â†’ Scheduled every hour
â”‚                                Heavy aggregation in search
â”‚
â”œâ”€â–º [Trend/baseline detection]
â”‚   â”‚
â”‚   â””â”€â–º Rolling window alert
â”‚       â€¢ Schedule: Every 15-60 minutes
â”‚       â€¢ Time range: 1-24 hours (sliding)
â”‚       â€¢ Use streamstats or eventstats for baseline
â”‚
â””â”€â–º [Compliance/reporting]
    â”‚
    â””â”€â–º Scheduled report
        â€¢ Schedule: Daily/Weekly
        â€¢ Action: Email report PDF
        â€¢ Not a real alert - informational
                    </div>
                </div>

                <div class="config-section">
                    <h3>Decision Tree: Alert Action Selection</h3>
                    <div class="arch-diagram">
Decision Tree: Choosing Alert Actions
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

START: What response is needed?
â”‚
â”œâ”€â–º [Immediate human response required]
â”‚   â”‚
â”‚   â””â”€â–º What's the on-call structure?
â”‚       â”‚
â”‚       â”œâ”€â–º PagerDuty/Opsgenie â†’ Webhook to PagerDuty
â”‚       â”‚                         action.webhook + PagerDuty API
â”‚       â”‚
â”‚       â”œâ”€â–º Slack/Teams 24/7 â†’ Direct integration
â”‚       â”‚                       action.slack or webhook
â”‚       â”‚
â”‚       â””â”€â–º Phone/SMS â†’ PagerDuty or custom script
â”‚
â”œâ”€â–º [Ticket creation needed]
â”‚   â”‚
â”‚   â””â”€â–º What ticketing system?
â”‚       â”‚
â”‚       â”œâ”€â–º ServiceNow â†’ ServiceNow app
â”‚       â”‚                action.snow_event
â”‚       â”‚
â”‚       â”œâ”€â–º Jira â†’ Jira app or webhook
â”‚       â”‚
â”‚       â””â”€â–º Custom â†’ Webhook to ticket API
â”‚
â”œâ”€â–º [SOC workflow (ES)]
â”‚   â”‚
â”‚   â””â”€â–º Create Notable Event
â”‚       action.notable = 1
â”‚       Integrates with Incident Review
â”‚       Enables investigation workflow
â”‚
â”œâ”€â–º [Automated response]
â”‚   â”‚
â”‚   â””â”€â–º Is SOAR available?
â”‚       â”‚
â”‚       â”œâ”€â–º YES â†’ Trigger SOAR playbook
â”‚       â”‚         action.phantom or action.demisto
â”‚       â”‚
â”‚       â””â”€â–º NO â†’ Custom script
â”‚                action.script
â”‚                âš ï¸ Ensure script is secure
â”‚
â””â”€â–º [Logging/Audit only]
    â”‚
    â””â”€â–º Log to index
        action.logevent
        Index for historical analysis
        No immediate notification
                    </div>
                </div>

                <!-- What Enterprises Usually Miss -->
                <h2><i class="fas fa-exclamation-circle"></i> What Enterprises Usually Miss</h2>
                
                <div class="enterprise-gap-grid">
                    <div class="gap-card gap-critical">
                        <h4>ğŸ”´ Critical Gaps</h4>
                        <ul>
                            <li><strong>Real-time alert overuse:</strong> 50+ real-time alerts consuming all search capacity</li>
                            <li><strong>No throttling:</strong> Same alert fires 1000 times per hour, overwhelming SOC</li>
                            <li><strong>Alert actions failing silently:</strong> Webhook returns 500, no one notices</li>
                            <li><strong>No alert testing:</strong> Alerts deployed without verifying they trigger correctly</li>
                            <li><strong>Credentials in clear text:</strong> API keys/passwords visible in savedsearches.conf</li>
                        </ul>
                    </div>
                    
                    <div class="gap-card gap-warning">
                        <h4>ğŸŸ¡ Common Oversights</h4>
                        <ul>
                            <li><strong>No severity classification:</strong> All alerts treated equally, no prioritization</li>
                            <li><strong>Missing context in alerts:</strong> Email says "alert triggered" with no details</li>
                            <li><strong>Orphaned alerts:</strong> Hundreds of alerts, no one knows what each does</li>
                            <li><strong>No runbook links:</strong> SOC receives alert but no guidance on response</li>
                            <li><strong>Schedule overlap:</strong> 100 alerts scheduled at exactly 00:00 causing spike</li>
                        </ul>
                    </div>
                    
                    <div class="gap-card gap-info">
                        <h4>ğŸ”µ Often Overlooked</h4>
                        <ul>
                            <li><strong>Alert lifecycle:</strong> No process to review and retire old alerts</li>
                            <li><strong>False positive tracking:</strong> No metrics on alert accuracy</li>
                            <li><strong>Alert documentation:</strong> No description of what alert detects and why</li>
                            <li><strong>Action redundancy:</strong> Same event triggers 5 different alerts</li>
                            <li><strong>Timezone issues:</strong> Alerts using server time, analysts in different zones confused</li>
                        </ul>
                    </div>
                    
                    <div class="gap-card gap-best-practice">
                        <h4>ğŸŸ¢ Best Practices Often Skipped</h4>
                        <ul>
                            <li><strong>Alert metrics dashboard:</strong> No visibility into alert volume, accuracy, response time</li>
                            <li><strong>Scheduled staggering:</strong> Not distributing alert schedules to spread load</li>
                            <li><strong>Alert ownership:</strong> No defined owner responsible for each alert's health</li>
                            <li><strong>Maintenance windows:</strong> No suppression during known maintenance</li>
                            <li><strong>Escalation paths:</strong> No automated escalation if primary responder doesn't acknowledge</li>
                        </ul>
                    </div>
                </div>

                <!-- Interview Q&A -->
                <h2><i class="fas fa-comments"></i> Interview Questions & Answers</h2>
                
                <div class="qa-section">
                    <div class="qa-item">
                        <button class="qa-question">Q: How do you reduce alert fatigue in a SOC that's receiving 500+ alerts per day?</button>
                        <div class="qa-answer">
                            <p><strong>Systematic Approach:</strong></p>
                            <div class="arch-diagram">
ALERT FATIGUE REDUCTION FRAMEWORK
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

PHASE 1: ASSESSMENT (Week 1)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Analyze current alert volume
index=_audit action=alert_fired
| stats count by savedsearch_name
| sort -count

# Identify false positive rate
index=_internal sourcetype=scheduler
| stats count(eval(status="success")) as triggered,
        count(eval(result_count=0)) as no_results
        by savedsearch_name
| eval fp_rate = no_results / triggered * 100

# Top offenders (alerts firing most frequently)
index=_audit action=alert_fired earliest=-7d
| stats count, dc(triggered_by) as unique_triggers by savedsearch_name
| sort -count
| head 20

PHASE 2: CATEGORIZATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Categorize all 500+ alerts:

A. CRITICAL (Immediate action required)
   Target: 10-20 alerts
   Examples: Active breach, ransomware, data exfil
   Action: Keep, refine thresholds

B. HIGH (Action within 1 hour)
   Target: 30-50 alerts
   Examples: Suspicious auth, malware detected
   Action: Add throttling, improve context

C. MEDIUM (Action within 8 hours)
   Target: 50-100 alerts
   Examples: Policy violations, anomalies
   Action: Digest mode, roll-up dashboards

D. LOW (Informational)
   Target: Unlimited
   Action: Convert to reports, remove alerts

E. DEPRECATED/REDUNDANT
   Action: Disable or delete

PHASE 3: IMPLEMENTATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
For each alert category:

# Add proper throttling
alert.suppress = 1
alert.suppress.period = 4h
alert.suppress.fields = src_ip, dest

# Tune thresholds using baseline
| stats count by src_ip
| where count > [baseline + 3*stdev]

# Add allowlists
NOT [| inputlookup known_good.csv | fields src_ip]

# Consolidate redundant alerts
Merge: "Brute Force SSH" + "Brute Force RDP" + "Brute Force FTP"
Into: "Brute Force Authentication" with service field

PHASE 4: MONITORING
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Create alert health dashboard
â€¢ Daily alert count by severity
â€¢ Top triggering alerts
â€¢ Alerts with 0 triggers (dead alerts)
â€¢ Mean time to acknowledge
â€¢ False positive rate by alert

TARGET METRICS:
â€¢ <100 actionable alerts per day
â€¢ <5% false positive rate
â€¢ >80% acknowledged within SLA
                            </div>
                            <p><strong>Key Success Factors:</strong> Executive buy-in, dedicated tuning time, continuous improvement cycle, SOC feedback loop.</p>
                        </div>
                    </div>

                    <div class="qa-item">
                        <button class="qa-question">Q: Explain how you'd set up a multi-tier alerting system for different severity levels.</button>
                        <div class="qa-answer">
                            <p><strong>Multi-Tier Alert Architecture:</strong></p>
                            <div class="arch-diagram">
TIERED ALERTING SYSTEM
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

TIER 1: CRITICAL (Severity 5)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Response SLA: 15 minutes
Triggers: Active attacks, data breach, ransomware

Alert Actions:
1. PagerDuty (immediate page)
2. Slack #critical-alerts
3. Notable event (ES)
4. SOAR playbook trigger

# savedsearches.conf
[T1-Critical: Active Ransomware]
alert.severity = 5
alert.suppress = 0
action.webhook = 1
action.webhook.param.url = https://events.pagerduty.com/v2/enqueue
action.slack = 1
action.slack.param.channel = #critical-alerts
action.notable = 1

TIER 2: HIGH (Severity 4)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Response SLA: 1 hour
Triggers: Suspicious activity, potential compromise

Alert Actions:
1. Slack #security-alerts
2. Email SOC team
3. Notable event (ES)

[T2-High: Lateral Movement Detected]
alert.severity = 4
alert.suppress = 1
alert.suppress.period = 30m
alert.suppress.fields = src_user
action.slack = 1
action.slack.param.channel = #security-alerts
action.email = 1
action.email.to = soc@company.com
action.notable = 1

TIER 3: MEDIUM (Severity 3)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Response SLA: 8 hours (business hours)
Triggers: Policy violations, anomalies

Alert Actions:
1. Email digest
2. Notable event (review queue)

[T3-Medium: Policy Violation]
alert.severity = 3
alert.digest_mode = 1
alert.suppress = 1
alert.suppress.period = 4h
action.email = 1
action.email.to = security-review@company.com
action.notable = 1

TIER 4: LOW (Severity 2)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Response SLA: Best effort
Triggers: Informational, hygiene issues

Alert Actions:
1. Log to index
2. Weekly digest report

[T4-Low: Certificate Expiring Soon]
alert.severity = 2
alert.suppress = 1
alert.suppress.period = 24h
action.logevent = 1
action.logevent.param.index = alert_log

TIER 5: INFORMATIONAL (Severity 1)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
No immediate response required
Triggers: Trends, metrics, compliance

Alert Actions:
1. Dashboard only
2. Monthly report

ROUTING LOGIC (SOAR)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

                    Alert Triggered
                          â”‚
                          â–¼
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚ Check Severityâ”‚
                   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼                 â–¼                 â–¼
   Severity 5        Severity 4        Severity 3-1
        â”‚                 â”‚                 â”‚
        â–¼                 â–¼                 â–¼
   PagerDuty +       Slack +           Email/Log
   Slack +           Email +           Queued
   Auto-Response     Notable           Review
                            </div>
                        </div>
                    </div>

                    <div class="qa-item">
                        <button class="qa-question">Q: How would you integrate Splunk alerts with ServiceNow for automated ticket creation?</button>
                        <div class="qa-answer">
                            <p><strong>ServiceNow Integration Architecture:</strong></p>
                            <div class="arch-diagram">
SERVICENOW INTEGRATION OPTIONS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

OPTION 1: SERVICENOW APP (Recommended)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Install: Splunk Add-on for ServiceNow from Splunkbase

Configuration:
# $SPLUNK_HOME/etc/apps/Splunk_TA_snow/local/snow_account.conf
[snow_account]
url = https://company.service-now.com
username = splunk_integration
password = $7$encrypted_password

# In alert action:
action.snow_event = 1
action.snow_event.param.node = $result.host$
action.snow_event.param.type = Splunk Security Alert
action.snow_event.param.resource = $result.src_ip$
action.snow_event.param.severity = 2
action.snow_event.param.description = $name$: $result.count$ events detected
action.snow_event.param.additional_info = Search: $search$

OPTION 2: WEBHOOK TO SERVICENOW API
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Direct REST API integration
action.webhook = 1
action.webhook.param.url = https://company.service-now.com/api/now/table/incident

# Requires middleware or custom alert action to format payload:
{
  "short_description": "$name$",
  "description": "$description$\n\nResults: $result.count$",
  "urgency": "2",
  "impact": "2",
  "category": "Security",
  "subcategory": "SIEM Alert",
  "assignment_group": "SOC",
  "caller_id": "splunk_service",
  "u_splunk_search": "$search$",
  "u_splunk_results_link": "$results_link$"
}

OPTION 3: CUSTOM ALERT ACTION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Create custom alert action for full control

# alert_actions.conf
[create_snow_ticket]
label = Create ServiceNow Ticket
icon_path = snow_icon.png
payload_format = json
is_custom = 1

# Corresponding Python script
# $SPLUNK_HOME/bin/scripts/create_snow_ticket.py

import sys
import json
import requests

def create_ticket(payload):
    snow_url = "https://company.service-now.com/api/now/table/incident"
    auth = ("splunk_user", "password")
    
    ticket_data = {
        "short_description": payload.get("search_name"),
        "description": payload.get("results_string"),
        "urgency": map_severity(payload.get("severity")),
        "assignment_group": "SOC"
    }
    
    response = requests.post(snow_url, auth=auth, json=ticket_data)
    return response.json()

FIELD MAPPING BEST PRACTICES
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Splunk Field        â†’  ServiceNow Field
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
$name$              â†’  short_description
$description$       â†’  description
$result.host$       â†’  configuration_item
$result.src_ip$     â†’  u_source_ip (custom field)
$result.user$       â†’  u_affected_user (custom field)
severity 5          â†’  urgency: 1, impact: 1 (Critical)
severity 4          â†’  urgency: 2, impact: 2 (High)
severity 3          â†’  urgency: 3, impact: 3 (Medium)
$results_link$      â†’  u_splunk_link (custom field)
                            </div>
                        </div>
                    </div>

                    <div class="qa-item">
                        <button class="qa-question">Q: What's the difference between real-time alerts and scheduled alerts? When would you use each?</button>
                        <div class="qa-answer">
                            <p><strong>Comparison:</strong></p>
                            <div class="arch-diagram">
REAL-TIME VS SCHEDULED ALERTS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

REAL-TIME ALERTS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
How it works:
â€¢ Continuous search running on indexers
â€¢ Checks every event as it arrives
â€¢ Triggers within seconds of event

Pros:
âœ“ Near-instant detection
âœ“ Per-event triggering possible
âœ“ Best for critical security events

Cons:
âœ— VERY resource intensive
âœ— Consumes persistent search slot
âœ— Can impact overall search performance
âœ— Limited to ~10 real-time searches recommended

Use when:
â€¢ Active attack detection (ransomware, exfil)
â€¢ Compliance requires sub-minute detection
â€¢ Low-volume, high-criticality events

Resource cost:
â€¢ 1 real-time search â‰ˆ 10-20 scheduled searches
â€¢ Consumes memory continuously
â€¢ May require dedicated search head capacity

SCHEDULED ALERTS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
How it works:
â€¢ Runs at defined intervals (cron)
â€¢ Searches historical data within time window
â€¢ Triggers based on aggregated results

Pros:
âœ“ Resource efficient
âœ“ Predictable performance impact
âœ“ Supports aggregations (stats, timechart)
âœ“ Scales to hundreds of alerts

Cons:
âœ— Detection latency (schedule + search time)
âœ— May miss events between runs
âœ— Requires careful time window management

Use when:
â€¢ Most security use cases
â€¢ Threshold-based detection
â€¢ Trend analysis
â€¢ Compliance reporting

Resource cost:
â€¢ Brief CPU/memory spike during execution
â€¢ Can schedule during off-peak hours

COMPARISON TABLE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Feature          â”‚ Real-Time     â”‚ Scheduled
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Detection latencyâ”‚ Seconds       â”‚ Minutes-Hours
Resource usage   â”‚ Continuous    â”‚ Periodic
Scalability      â”‚ ~10 max       â”‚ Hundreds
Aggregations     â”‚ Limited       â”‚ Full support
Best for         â”‚ Critical only â”‚ Everything else

RECOMMENDATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Default: Scheduled alerts for 95% of use cases
Real-time: Only for 3-5 truly critical detections
Hybrid: Scheduled every 1-5 minutes for "near real-time"
                            </div>
                        </div>
                    </div>

                    <div class="qa-item">
                        <button class="qa-question">Q: How do you ensure alert reliability - that alerts fire when they should and don't miss events?</button>
                        <div class="qa-answer">
                            <p><strong>Alert Reliability Framework:</strong></p>
                            <div class="arch-diagram">
ALERT RELIABILITY ASSURANCE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1. TESTING & VALIDATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Test alert before production deployment

A. Create test data
| makeresults 
| eval src_ip="10.0.0.1", user="testuser", count=15
| collect index=test_data

B. Verify search returns expected results
index=test_data src_ip=10.0.0.1

C. Trigger alert manually
Settings â†’ Searches â†’ Run (with time range)

D. Verify action executed
- Check email received
- Check webhook/Slack
- Check _audit for alert_fired

2. TIME WINDOW MANAGEMENT
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Problem: Gap between scheduled runs

Schedule: Every 5 minutes
Search: earliest=-5m latest=now

Issue: If search takes 30 seconds, there's overlap/gap

Solution: Overlapping windows
Schedule: */5 * * * *
dispatch.earliest_time = -6m
dispatch.latest_time = now

# Deduplicate in search or use digest mode

3. MONITORING ALERT HEALTH
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Dashboard: Alert Health Monitor

# Alerts that haven't fired in 30 days (potentially broken)
| rest /servicesNS/-/-/saved/searches
| search is_scheduled=1 
| join savedsearch_name 
    [search index=_audit action=alert_fired earliest=-30d
     | stats max(_time) as last_fired by savedsearch_name]
| where isnull(last_fired) OR last_fired < relative_time(now(), "-30d")
| table title, last_fired, search

# Alerts with high skip ratio (resource issues)
index=_internal sourcetype=scheduler status=skipped
| stats count by savedsearch_name
| where count > 10
| table savedsearch_name, count

# Alert action failures
index=_internal sourcetype=scheduler 
  (action_status=failure OR status=error)
| stats count by savedsearch_name, status, action
| table savedsearch_name, action, status, count

4. SYNTHETIC TESTING
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Periodic synthetic events to verify pipeline

# Scheduled search to inject test event
| makeresults 
| eval test_marker="SYNTHETIC_TEST", severity="test"
| collect index=security sourcetype=test:synthetic

# Alert should detect synthetic event
index=security test_marker="SYNTHETIC_TEST"
| stats count
| where count > 0

# If alert doesn't fire on synthetic, investigate

5. REDUNDANCY FOR CRITICAL ALERTS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
For critical detections, implement backup:

Primary: Correlation search in ES
Backup: Scheduled alert with different logic
Tertiary: SOAR detection playbook

# Different approaches catch different scenarios
# If one misses, another catches
                            </div>
                        </div>
                    </div>
                </div>

                <h2><i class="fas fa-link"></i> Related Resources</h2>
                <ul>
                    <li><a href="correlation-searches.html">Correlation Searches</a></li>
                    <li><a href="notable-events.html">Notable Events</a></li>
                    <li><a href="soar.html">SOAR Integration</a></li>
                    <li><a href="risk-based-alerting.html">Risk-Based Alerting</a></li>
                </ul>
            </main>
        </div>
    </div>
    <script>
        function toggleSidebar() {
            const sidebar = document.getElementById('sidebar');
            const mainWrapper = document.getElementById('mainWrapper');
            const overlay = document.getElementById('sidebarOverlay');
            if (window.innerWidth <= 768) { sidebar.classList.toggle('open'); overlay.classList.toggle('active'); } 
            else { sidebar.classList.toggle('collapsed'); mainWrapper.classList.toggle('expanded'); }
        }
        document.querySelectorAll('.qa-question').forEach(btn => {
            btn.addEventListener('click', () => {
                const answer = btn.nextElementSibling;
                const isOpen = answer.style.display === 'block';
                document.querySelectorAll('.qa-answer').forEach(a => a.style.display = 'none');
                if (!isOpen) answer.style.display = 'block';
            });
        });
    </script>
</body>
</html>
