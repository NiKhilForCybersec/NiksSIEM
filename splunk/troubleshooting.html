<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    <title>Splunk Troubleshooting - SOC Compendium</title>
    <link rel="stylesheet" href="../assets/css/main.css">
</head>
<body>
    <nav class="main-nav">
        <div class="nav-container">
            <div class="nav-logo">SOC Compendium</div>
            <ul class="nav-links">
                <li><a href="../index.html">Home</a></li>
                <li><a href="../cortex/index.html">Cortex XDR</a></li>
                <li><a href="../crowdstrike/index.html">CrowdStrike</a></li>
                <li><a href="../mde/index.html">Microsoft Defender</a></li>
                <li><a href="../sentinel/index.html">Sentinel</a></li>
                <li><a href="index.html" class="active">Splunk</a></li>
                <li><a href="../operations/index.html">Operations</a></li>
            </ul>
        </div>
    </nav>

    <main class="content">
        <div class="content-header">
            <h1>Splunk Troubleshooting</h1>
            <p class="subtitle">Diagnosing and resolving common Splunk ES issues</p>
        </div>

        <section class="doc-section">
            <h2>Troubleshooting Overview</h2>
            <p>Effective Splunk troubleshooting requires understanding the platform's architecture, data flow, and common failure modes. This guide covers systematic approaches to diagnosing issues across search performance, data ingestion, Enterprise Security, and distributed deployments.</p>

            <div class="info-box">
                <h4>Troubleshooting Philosophy</h4>
                <ul>
                    <li><strong>Isolate the Layer:</strong> Determine if issue is search, indexing, forwarding, or infrastructure</li>
                    <li><strong>Check the Basics First:</strong> Disk space, services running, network connectivity, time sync</li>
                    <li><strong>Use Built-in Diagnostics:</strong> Monitoring Console, internal indexes, REST API</li>
                    <li><strong>Document and Reproduce:</strong> Capture exact symptoms, steps to reproduce, and environment details</li>
                </ul>
            </div>
        </section>

        <section class="doc-section">
            <h2>Search Performance Troubleshooting</h2>
            <p>Search performance issues are among the most common problems in Splunk deployments, affecting analyst productivity and dashboard responsiveness.</p>

            <h3>Identifying Slow Searches</h3>
            <div class="code-block">
                <div class="code-header">Find Slow Searches (Last 24 Hours)</div>
                <pre>index=_audit action=search info=completed
| eval search_time_sec=total_run_time
| where search_time_sec > 60
| stats count as executions, avg(search_time_sec) as avg_runtime,
        max(search_time_sec) as max_runtime, 
        values(user) as users by search
| where executions > 3
| sort - avg_runtime
| head 20
| eval avg_runtime=round(avg_runtime,1), max_runtime=round(max_runtime,1)</pre>
            </div>

            <div class="code-block">
                <div class="code-header">Search Job Inspector Analysis</div>
                <pre>| rest /services/search/jobs
| where dispatchState="DONE"
| eval runDuration_sec=round(runDuration,2)
| where runDuration_sec > 30
| table sid, label, runDuration_sec, eventCount, resultCount, 
        scanCount, dropCount, user, earliestTime, latestTime
| sort - runDuration_sec</pre>
            </div>

            <h3>Common Search Performance Issues</h3>
            <div class="styled-table">
                <table>
                    <thead>
                        <tr>
                            <th>Symptom</th>
                            <th>Likely Cause</th>
                            <th>Diagnostic Query</th>
                            <th>Solution</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>Searches timing out</td>
                            <td>Unbounded time range</td>
                            <td>Check earliest/latest in Job Inspector</td>
                            <td>Add explicit time bounds, use tstats</td>
                        </tr>
                        <tr>
                            <td>High scanCount vs eventCount</td>
                            <td>Inefficient filtering</td>
                            <td>Job Inspector execution costs</td>
                            <td>Move filters left, use indexed fields</td>
                        </tr>
                        <tr>
                            <td>Memory errors</td>
                            <td>Large result sets</td>
                            <td>Check dropCount in Job Inspector</td>
                            <td>Add limits, use summary indexes</td>
                        </tr>
                        <tr>
                            <td>Slow dashboard load</td>
                            <td>Too many concurrent searches</td>
                            <td>Monitoring Console > Search Activity</td>
                            <td>Use base searches, post-process</td>
                        </tr>
                        <tr>
                            <td>Subsearch timeouts</td>
                            <td>Subsearch exceeds limits</td>
                            <td>Check subsearch.maxresultrows</td>
                            <td>Use join, append, or map instead</td>
                        </tr>
                    </tbody>
                </table>
            </div>

            <h3>Search Optimization Checklist</h3>
            <div class="code-block">
                <div class="code-header">Before vs After: Common Optimizations</div>
                <pre># BAD: Unbounded search with wildcards
index=* sourcetype=* error
| stats count by host

# GOOD: Specific index and time bounds
index=main sourcetype=syslog error earliest=-1h latest=now
| stats count by host

# BAD: Transforming before filtering
index=firewall
| stats sum(bytes) as total_bytes by src_ip
| where total_bytes > 1000000000

# GOOD: Filter before transforming
index=firewall bytes>1000000
| stats sum(bytes) as total_bytes by src_ip
| where total_bytes > 1000000000

# BAD: Using search command for filtering
index=proxy
| search status=403 OR status=404

# GOOD: Direct field filtering (indexed)
index=proxy (status=403 OR status=404)

# BAD: Regex when simpler options exist
index=windows
| regex EventCode="^(4624|4625|4634)$"

# GOOD: Direct field comparison
index=windows EventCode IN (4624, 4625, 4634)</pre>
            </div>

            <h3>Using tstats for Performance</h3>
            <div class="code-block">
                <div class="code-header">tstats vs Traditional Search Comparison</div>
                <pre># Traditional search (slow for large datasets)
index=network sourcetype=firewall
| stats count by src_ip, dest_ip, dest_port

# tstats with accelerated data model (10-100x faster)
| tstats count FROM datamodel=Network_Traffic 
    WHERE All_Traffic.action=allowed 
    BY All_Traffic.src, All_Traffic.dest, All_Traffic.dest_port
| rename All_Traffic.* as *

# Check data model acceleration status
| rest /services/admin/summarization by_tstats=t
| search datamodel_acceleration.disabled=0
| table title, datamodel_acceleration.status, 
        datamodel_acceleration.complete_percent,
        datamodel_acceleration.earliest_time,
        datamodel_acceleration.latest_time</pre>
            </div>
        </section>

        <section class="doc-section">
            <h2>Data Ingestion Troubleshooting</h2>
            <p>Data flow issues can manifest as missing events, delayed data, or parsing problems. Understanding the data pipeline is essential for diagnosis.</p>

            <h3>Data Flow Architecture</h3>
            <div class="info-box">
                <h4>Data Flow Path</h4>
                <p><strong>Source → Universal Forwarder → Heavy Forwarder (optional) → Indexer → Search Head</strong></p>
                <ul>
                    <li><strong>Input Phase:</strong> Data collection from sources (files, network, APIs)</li>
                    <li><strong>Parsing Phase:</strong> Line breaking, timestamp extraction, transforms</li>
                    <li><strong>Indexing Phase:</strong> Segmentation, index file creation</li>
                    <li><strong>Search Phase:</strong> Query execution, results delivery</li>
                </ul>
            </div>

            <h3>Missing Data Diagnostics</h3>
            <div class="code-block">
                <div class="code-header">Check Data Ingestion by Sourcetype</div>
                <pre># Data volume by sourcetype (last 24h)
| tstats count WHERE index=* BY index, sourcetype, _time span=1h
| timechart span=1h sum(count) by sourcetype

# Identify sourcetypes with data gaps
index=_internal source=*metrics.log group=per_sourcetype_thruput
| timechart span=1h sum(kb) as KB by series
| foreach * [eval <<FIELD>>=if(<<FIELD>>==0, null(), <<FIELD>>)]

# Check last event time per sourcetype
| metadata type=sourcetypes index=*
| eval lastTime=strftime(lastTime,"%Y-%m-%d %H:%M:%S")
| eval age_hours=round((now()-lastTime)/3600,1)
| where age_hours > 1
| sort - age_hours
| table sourcetype, lastTime, age_hours, totalCount

# Data gaps detection
| tstats count WHERE index=main BY _time span=5m
| streamstats current=f window=1 last(count) as prev_count
| eval gap_pct=if(prev_count>0, round((1-count/prev_count)*100,1), 0)
| where gap_pct > 50 OR count=0
| table _time, count, prev_count, gap_pct</pre>
            </div>

            <h3>Forwarder Diagnostics</h3>
            <div class="code-block">
                <div class="code-header">Universal Forwarder Health Checks</div>
                <pre># List all forwarders and their status
| rest /services/deployment/server/clients
| table hostname, ip, dns, utsname, 
       splunkVersion, lastPhoneHomeTime,
       averagePhoneHomeInterval, registered
| eval last_seen_hours=round((now()-strptime(lastPhoneHomeTime,
       "%a %b %d %H:%M:%S %Y"))/3600,1)
| sort - last_seen_hours

# Forwarders not sending data
index=_internal source=*metrics.log group=tcpin_connections
| stats latest(sourceIp) as sourceIp, latest(kb) as kb_received,
        latest(_time) as last_data by hostname
| eval hours_ago=round((now()-last_data)/3600,1)
| where hours_ago > 1
| sort - hours_ago

# Check forwarder queue status (on forwarder)
index=_internal source=*metrics.log group=queue
| eval queue_fill_pct=round(current_size_kb/max_size_kb*100,1)
| stats latest(queue_fill_pct) as fill_pct by name
| where fill_pct > 50</pre>
            </div>

            <div class="code-block">
                <div class="code-header">On-Forwarder Diagnostics (CLI)</div>
                <pre># Check forwarder status
$SPLUNK_HOME/bin/splunk status

# List forwarding targets
$SPLUNK_HOME/bin/splunk list forward-server

# Check input status
$SPLUNK_HOME/bin/splunk list monitor

# View forwarder internal log
tail -f $SPLUNK_HOME/var/log/splunk/splunkd.log

# Common issues to look for:
# - "TcpOutputProc - Connection refused" (indexer unreachable)
# - "FileInputTracker - Error reading" (permission issues)
# - "Queue - queue full" (network bottleneck)
# - "License - daily volume exceeded" (license limit)</pre>
            </div>

            <h3>Parsing and Timestamp Issues</h3>
            <div class="code-block">
                <div class="code-header">Diagnose Timestamp Extraction Problems</div>
                <pre># Find events with incorrect timestamps
index=main earliest=-24h latest=now
| eval index_time=_indextime
| eval event_time=_time
| eval time_diff_hours=round((index_time-event_time)/3600,1)
| where abs(time_diff_hours) > 24
| stats count by sourcetype, time_diff_hours
| sort - count

# Events indexed in future (common misconfiguration)
index=main earliest=+1d
| stats count by sourcetype, host
| sort - count

# Check timestamp recognition
| makeresults 
| eval _raw="2024-01-15 14:30:45 ERROR Something failed"
| extract
| table _time, _raw

# Test timestamp extraction with specific format
index=main sourcetype=custom_app earliest=-1h
| head 10
| eval extracted_time=strptime(_raw, "%Y-%m-%d %H:%M:%S")
| eval matches=if(_time==extracted_time, "YES", "NO")
| table _raw, _time, extracted_time, matches</pre>
            </div>

            <h3>Common Ingestion Fixes</h3>
            <div class="styled-table">
                <table>
                    <thead>
                        <tr>
                            <th>Issue</th>
                            <th>Configuration File</th>
                            <th>Fix</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>Events merged incorrectly</td>
                            <td>props.conf</td>
                            <td><code>LINE_BREAKER = ([\r\n]+)</code></td>
                        </tr>
                        <tr>
                            <td>Wrong timestamp extracted</td>
                            <td>props.conf</td>
                            <td><code>TIME_FORMAT = %Y-%m-%d %H:%M:%S</code><br><code>TIME_PREFIX = ^</code></td>
                        </tr>
                        <tr>
                            <td>Events truncated</td>
                            <td>props.conf</td>
                            <td><code>TRUNCATE = 50000</code> (increase limit)</td>
                        </tr>
                        <tr>
                            <td>Missing field extractions</td>
                            <td>props.conf / transforms.conf</td>
                            <td>Add EXTRACT or REPORT configurations</td>
                        </tr>
                        <tr>
                            <td>Data going to wrong index</td>
                            <td>inputs.conf</td>
                            <td>Verify <code>index = </code> setting</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </section>

        <section class="doc-section">
            <h2>Enterprise Security Troubleshooting</h2>
            <p>Splunk Enterprise Security introduces additional components that require specific troubleshooting approaches.</p>

            <h3>Notable Events Not Generating</h3>
            <div class="code-block">
                <div class="code-header">Diagnose Notable Event Generation</div>
                <pre># Check correlation search execution
| rest /services/saved/searches
| search disabled=0 is_scheduled=1
| search eai:acl.app="SplunkEnterpriseSecuritySuite" 
         OR eai:acl.app="SA-*" OR eai:acl.app="DA-*"
| table title, disabled, is_scheduled, next_scheduled_time,
        cron_schedule, dispatch.earliest_time
| sort title

# Recent correlation search runs and results
index=_internal sourcetype=scheduler status=success
    savedsearch_name="*Rule*" OR savedsearch_name="*Threat*"
| stats count, latest(run_time) as last_run,
        latest(result_count) as last_results by savedsearch_name
| where last_results=0
| sort - count

# Check if notable index is receiving events
index=notable earliest=-24h
| stats count by rule_name
| sort - count

# Verify modular actions are working
index=_internal sourcetype=modular_alerts action=notable
| stats count by action, status
| sort - count</pre>
            </div>

            <h3>Asset and Identity Lookup Issues</h3>
            <div class="code-block">
                <div class="code-header">Asset/Identity Lookup Diagnostics</div>
                <pre># Check asset lookup status
| inputlookup asset_lookup_by_str
| stats count
| eval status=if(count>0, "OK: ".count." assets", "EMPTY - Check lookup")

# Check identity lookup status
| inputlookup identity_lookup_expanded
| stats count
| eval status=if(count>0, "OK: ".count." identities", "EMPTY - Check lookup")

# Verify asset merge is working
| rest /services/data/transforms/lookups
| search filename="*asset*"
| table title, filename, fields_list

# Check KV Store status
| rest /services/kvstore/status
| table title, current, backupRestoreStatus, replicaSetConfig.status

# Diagnose asset correlation
index=main sourcetype=firewall earliest=-1h
| lookup asset_lookup_by_str ip as src OUTPUT category, owner
| stats count by src, category, owner
| where isnull(category)
| head 20</pre>
            </div>

            <h3>Risk Score Calculation Issues</h3>
            <div class="code-block">
                <div class="code-header">Risk Framework Troubleshooting</div>
                <pre># Check risk index is receiving events
index=risk earliest=-24h
| stats count by source, risk_object_type
| sort - count

# Verify risk modifiers are applied
| rest /services/apps/local/SplunkEnterpriseSecuritySuite/
       configs/conf-risk_modifiers
| table title, risk_modifier_value, risk_object_field

# Check risk aggregation
| from datamodel:Risk.All_Risk
| stats sum(calculated_risk_score) as total_risk,
        count as events by risk_object
| sort - total_risk
| head 20

# Investigate why specific entity has no risk
| makeresults 
| eval user="suspect_user@domain.com"
| lookup identity_lookup_expanded identity as user OUTPUT category as user_category
| join type=left user [
    | from datamodel:Risk.All_Risk
    | search risk_object="suspect_user@domain.com"
    | stats sum(calculated_risk_score) as risk
  ]
| table user, user_category, risk</pre>
            </div>

            <h3>ES Dashboard Performance</h3>
            <div class="code-block">
                <div class="code-header">Dashboard Acceleration Status</div>
                <pre># Check data model acceleration health
| rest /services/admin/summarization by_tstats=t splunk_server=local
| eval size_mb=round(summary.size/1024/1024,2)
| eval complete_pct=round(datamodel_acceleration.complete_pct,1)
| table title, datamodel_acceleration.status, complete_pct,
        size_mb, datamodel_acceleration.earliest_time
| sort - complete_pct

# Identify unaccelerated searches hitting dashboards
index=_internal sourcetype=splunkd component=SavedSearchIntrospection
| stats count by savedsearch_name, app
| lookup savedsearch_tracker savedsearch_name OUTPUT is_accelerated
| where is_accelerated=0
| sort - count

# Check search scheduler queue
| rest /services/search/jobs
| search provenance="scheduler"
| stats count by dispatchState
| sort - count</pre>
            </div>
        </section>

        <section class="doc-section">
            <h2>Distributed Environment Issues</h2>
            <p>Multi-instance Splunk deployments introduce complexity around cluster management, replication, and search distribution.</p>

            <h3>Search Head Cluster Troubleshooting</h3>
            <div class="code-block">
                <div class="code-header">Search Head Cluster Health</div>
                <pre># Check SHC status (run on any member)
| rest /services/shcluster/member/members
| table label, status, last_heartbeat, 
       artifact_count, pending_job_count, replication_count
| sort - status

# Check captain status
| rest /services/shcluster/captain/info
| table label, dynamic_captain, elected_captain, 
       rolling_restart_flag, service_ready_flag

# Identify replication issues
index=_internal sourcetype=splunkd component=SHCluster*
| search "ERROR" OR "WARN"
| stats count by message
| sort - count

# Knowledge object sync status
| rest /services/shcluster/member/info
| table label, status, restart_reason, 
       last_conf_replication, pending_artifacts</pre>
            </div>

            <h3>Indexer Cluster Troubleshooting</h3>
            <div class="code-block">
                <div class="code-header">Indexer Cluster Diagnostics</div>
                <pre># Cluster health overview
| rest /services/cluster/master/health
| table all_data_is_searchable, all_peers_are_up,
       cm_version_is_compatible, multisite_data_policy_met,
       replication_factor_met, search_factor_met

# Peer status
| rest /services/cluster/master/peers
| table label, status, bucket_count, 
       is_searchable, last_heartbeat, replication_port
| sort - bucket_count

# Bucket fixup status (indicates data inconsistency)
| rest /services/cluster/master/fixup
| stats sum(count) as fixup_tasks by reason
| sort - fixup_tasks

# Check for rolling restart status
| rest /services/cluster/master/info
| table rolling_restart_flag, indexing_ready_flag,
       maintenance_mode, cluster_label

# Data replication lag
index=_internal sourcetype=splunkd component=CMBucketMgr
| search "replication" 
| timechart span=5m count by log_level</pre>
            </div>

            <h3>Deployment Server Issues</h3>
            <div class="code-block">
                <div class="code-header">Deployment Server Diagnostics</div>
                <pre># Check deployment server status
| rest /services/deployment/server/status
| table serverclass_count, client_count, 
       reload_needed, restart_needed

# Client check-in status
| rest /services/deployment/server/clients
| stats count by state
| sort - count

# Server classes and app deployment
| rest /services/deployment/server/serverclasses
| mvexpand restartSplunkd
| stats count by name, restartSplunkd, stateOnClient

# Failed deployments
index=_internal sourcetype=splunkd component=DC*
| search "ERROR" OR "WARN" OR "failed"
| stats count by message
| sort - count

# App deployment history
| rest /services/deployment/server/applications
| table name, serverclass, state, 
       lastModifiedTime, deployStatus</pre>
            </div>
        </section>

        <section class="doc-section">
            <h2>Infrastructure and Resource Issues</h2>

            <h3>Disk Space Management</h3>
            <div class="code-block">
                <div class="code-header">Storage Diagnostics</div>
                <pre># Index size and growth
| rest /services/data/indexes
| eval size_gb=round(currentDBSizeMB/1024,2)
| eval max_gb=round(maxTotalDataSizeMB/1024,2)
| eval pct_used=round(currentDBSizeMB/maxTotalDataSizeMB*100,1)
| table title, size_gb, max_gb, pct_used, 
       frozenTimePeriodInSecs, homePath
| sort - size_gb

# Identify indexes approaching limits
| rest /services/data/indexes
| eval pct_full=round(currentDBSizeMB/maxTotalDataSizeMB*100,1)
| where pct_full > 80
| table title, pct_full, maxTotalDataSizeMB, currentDBSizeMB
| sort - pct_full

# Data retention analysis
| rest /services/data/indexes
| eval retention_days=frozenTimePeriodInSecs/86400
| table title, retention_days, currentDBSizeMB
| sort - retention_days

# Hot/Warm/Cold bucket distribution
| rest /services/data/indexes-extended
| eval hot_pct=round(total_raw_size_hot_buckets/
                    (total_raw_size_hot_buckets+
                     total_raw_size_warm_buckets+
                     total_raw_size_cold_buckets)*100,1)
| table title, hot_pct, bucket_count_hot, 
       bucket_count_warm, bucket_count_cold</pre>
            </div>

            <h3>Memory and CPU Issues</h3>
            <div class="code-block">
                <div class="code-header">Resource Usage Analysis</div>
                <pre># CPU usage by process
index=_introspection sourcetype=splunk_resource_usage 
    component=PerProcess
| timechart span=5m avg(pctCPU) by process_type

# Memory usage trends
index=_introspection sourcetype=splunk_resource_usage 
    component=PerProcess
| eval mem_mb=mem/1024/1024
| timechart span=5m avg(mem_mb) by process_type

# Concurrent search usage
index=_internal sourcetype=scheduler
| timechart span=1h dc(sid) as concurrent_searches
| eval threshold=8
| eval over_threshold=if(concurrent_searches>threshold, 1, 0)

# High-resource searches
index=_audit action=search info=completed
| eval mem_used_mb=mem_used/1024/1024
| where mem_used_mb > 1000 OR total_run_time > 300
| table user, search, total_run_time, mem_used_mb, 
       scan_count, event_count
| sort - mem_used_mb</pre>
            </div>

            <h3>License Usage Troubleshooting</h3>
            <div class="code-block">
                <div class="code-header">License Diagnostics</div>
                <pre># Daily license usage
| rest /services/licenser/pools
| eval used_gb=round(used_bytes/1024/1024/1024,2)
| eval quota_gb=round(effective_quota/1024/1024/1024,2)
| eval pct_used=round(used_bytes/effective_quota*100,1)
| table title, used_gb, quota_gb, pct_used

# License usage by sourcetype
index=_internal source=*license_usage.log type=Usage
| stats sum(b) as bytes by st
| eval gb=round(bytes/1024/1024/1024,2)
| sort - gb
| head 20

# Historical license usage
index=_internal source=*license_usage.log type=Usage
| timechart span=1d sum(b) as bytes
| eval gb=round(bytes/1024/1024/1024,2)
| table _time, gb

# Identify license spikes
index=_internal source=*license_usage.log type=Usage
| timechart span=1h sum(b) as bytes by st
| eval total_gb=round(bytes/1024/1024/1024,2)
| where total_gb > 10
| sort - _time</pre>
            </div>
        </section>

        <section class="doc-section">
            <h2>Network and Connectivity Issues</h2>

            <h3>Connection Troubleshooting</h3>
            <div class="code-block">
                <div class="code-header">Network Connectivity Checks</div>
                <pre># Forwarder connection status
index=_internal source=*metrics.log group=tcpin_connections
| stats latest(sourceIp) as ip, sum(kb) as kb_received,
        latest(_time) as last_seen by hostname
| eval hours_since=round((now()-last_seen)/3600,1)
| sort - hours_since

# Connection errors
index=_internal sourcetype=splunkd 
    ("connection refused" OR "connection reset" OR "timeout")
| stats count by host, message
| sort - count

# TCP input health
index=_internal source=*metrics.log group=tcpin_connections
| stats sum(tcp_eps) as events_per_sec,
        sum(tcp_Kprocessed) as kb_processed by ingest_pipe
| sort - events_per_sec

# SSL/TLS issues
index=_internal sourcetype=splunkd component=TcpInputConfig
| search "SSL" OR "TLS" OR "certificate"
| stats count by message
| sort - count</pre>
            </div>

            <div class="code-block">
                <div class="code-header">CLI Network Diagnostics</div>
                <pre># Test connectivity to indexers
$SPLUNK_HOME/bin/splunk btool outputs list --debug

# Check listening ports
netstat -tlnp | grep splunk

# Test specific port connectivity
nc -zv indexer.company.com 9997
nc -zv indexer.company.com 8089

# Check firewall rules (Linux)
iptables -L -n | grep -E "(9997|8089|8000)"

# Verify certificates
openssl s_client -connect indexer.company.com:9997 -showcerts

# Test REST API
curl -k https://splunk.company.com:8089/services/server/info \
     -u admin:password</pre>
            </div>
        </section>

        <section class="doc-section">
            <h2>Common Error Messages</h2>

            <h3>Error Reference and Solutions</h3>
            <div class="styled-table">
                <table>
                    <thead>
                        <tr>
                            <th>Error Message</th>
                            <th>Cause</th>
                            <th>Solution</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><code>Search is waiting for a subsearch to complete</code></td>
                            <td>Subsearch exceeds time/result limits</td>
                            <td>Optimize subsearch, increase <code>max_results_perchunk</code></td>
                        </tr>
                        <tr>
                            <td><code>Your search is not scheduled because it conflicts</code></td>
                            <td>Too many concurrent scheduled searches</td>
                            <td>Stagger search schedules, reduce frequency</td>
                        </tr>
                        <tr>
                            <td><code>The maximum disk usage limit has been reached</code></td>
                            <td>Search dispatch directory full</td>
                            <td>Clear old search artifacts, increase limits</td>
                        </tr>
                        <tr>
                            <td><code>Unable to distribute to peer</code></td>
                            <td>Indexer unreachable</td>
                            <td>Check network, peer status, bundle replication</td>
                        </tr>
                        <tr>
                            <td><code>Bucket fixup required</code></td>
                            <td>Cluster bucket inconsistency</td>
                            <td>Allow automatic fixup, check peer health</td>
                        </tr>
                        <tr>
                            <td><code>License violation warning</code></td>
                            <td>Exceeded daily license quota</td>
                            <td>Reduce ingestion, purchase more license</td>
                        </tr>
                        <tr>
                            <td><code>kvstore startup failed</code></td>
                            <td>KV Store corruption or config issue</td>
                            <td>Check MongoDB logs, repair or restore</td>
                        </tr>
                        <tr>
                            <td><code>Too many concurrent real-time searches</code></td>
                            <td>Real-time search limit exceeded</td>
                            <td>Convert to scheduled searches where possible</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </section>

        <section class="doc-section">
            <h2>Troubleshooting Toolkit</h2>

            <h3>Essential Diagnostic Queries Collection</h3>
            <div class="code-block">
                <div class="code-header">Quick Health Check Dashboard Searches</div>
                <pre># 1. Overall System Health
| rest /services/server/status
| table splunk_version, cpu_arch, os_name, 
       ServerRoles, numberOfCores, physicalMemoryMB

# 2. Service Status
| rest /services/server/health/splunkd
| table health, reasons{}.reason, features{}.name, 
       features{}.health

# 3. Index Health Summary
| rest /services/data/indexes
| stats count as total_indexes, 
        sum(currentDBSizeMB) as total_size_mb,
        sum(totalEventCount) as total_events

# 4. Forwarder Health Summary
| rest /services/deployment/server/clients
| stats count as total_forwarders,
        count(eval(state="Active")) as active,
        count(eval(state!="Active")) as inactive

# 5. License Status
| rest /services/licenser/licenses
| table label, type, expiration_time, quota, 
       max_violations, window_period

# 6. Data Model Acceleration Status
| rest /services/admin/summarization by_tstats=t
| stats count as total_dm,
        count(eval('datamodel_acceleration.status'="enabled")) as accelerated,
        avg('datamodel_acceleration.complete_pct') as avg_completion</pre>
            </div>

            <h3>Splunk CLI Troubleshooting Commands</h3>
            <div class="code-block">
                <div class="code-header">Essential CLI Commands</div>
                <pre># Check configuration files for errors
$SPLUNK_HOME/bin/splunk btool check

# List effective configuration (merged from all layers)
$SPLUNK_HOME/bin/splunk btool inputs list --debug
$SPLUNK_HOME/bin/splunk btool props list --debug
$SPLUNK_HOME/bin/splunk btool transforms list --debug

# Validate app configuration
$SPLUNK_HOME/bin/splunk validate app $APP_NAME

# Clear search artifacts
$SPLUNK_HOME/bin/splunk clean dispatch -index main

# Rebuild index bucket summaries (TSIDX)
$SPLUNK_HOME/bin/splunk rebuild minfreemb=1000

# Export diagnostic bundle
$SPLUNK_HOME/bin/splunk diag --collect=all

# Check cluster status (on master)
$SPLUNK_HOME/bin/splunk show cluster-status

# Force cluster bundle push
$SPLUNK_HOME/bin/splunk apply cluster-bundle --answer-yes</pre>
            </div>

            <h3>Monitoring Console Key Views</h3>
            <div class="info-box">
                <h4>Essential Monitoring Console Dashboards</h4>
                <ul>
                    <li><strong>Overview:</strong> High-level health status of all instances</li>
                    <li><strong>Search Activity:</strong> Running searches, scheduler status, search failures</li>
                    <li><strong>Indexing Performance:</strong> Throughput, queue sizes, parsing issues</li>
                    <li><strong>Forwarder Monitoring:</strong> Forwarder health, missing forwarders, data gaps</li>
                    <li><strong>License Usage:</strong> Daily usage, violations, trending</li>
                    <li><strong>Resource Usage:</strong> CPU, memory, disk by component</li>
                    <li><strong>Distributed Environment:</strong> Cluster health, replication status</li>
                </ul>
            </div>
        </section>

        <section class="doc-section">
            <h2>Interview Questions</h2>
            
            <div class="qa-box">
                <h4>Q: How do you troubleshoot slow Splunk searches?</h4>
                <p><strong>A:</strong> I use a systematic approach: First, I open the Job Inspector to analyze execution costs, looking at scanCount vs eventCount ratio, time spent in each phase, and memory usage. High scanCount with low eventCount indicates inefficient filtering. I check if the search uses unbounded time ranges or wildcards in index/sourcetype. I verify if data model acceleration is available and being used. For dashboards, I look for opportunities to use base searches and post-processing. Common optimizations include moving filters left (before transformations), using tstats with accelerated data models, adding explicit time bounds, and avoiding subsearch where possible. I also check the Monitoring Console's Search Activity dashboard for concurrent search load.</p>
            </div>

            <div class="qa-box">
                <h4>Q: A sourcetype has stopped receiving data. Walk me through your troubleshooting process.</h4>
                <p><strong>A:</strong> I work through the data path systematically. First, I check the metadata to confirm when data was last received: <code>| metadata type=sourcetypes | search sourcetype=X</code>. Then I verify forwarder status—are the relevant forwarders checking in? I look at the deployment server to confirm the input configuration is deployed. On the forwarder, I check splunkd.log for errors, verify file permissions if it's a file monitor, and confirm the input is configured in inputs.conf. I use the Monitoring Console's Forwarder Monitoring to identify if it's a single forwarder or widespread issue. If forwarders are sending but data isn't appearing, I check for parsing issues in props.conf/transforms.conf and verify the index exists and isn't frozen. I also check license usage to ensure we haven't hit limits.</p>
            </div>

            <div class="qa-box">
                <h4>Q: How do you diagnose Enterprise Security notable events not generating?</h4>
                <p><strong>A:</strong> Notable event generation involves multiple components. First, I verify the correlation search is enabled and scheduled using the Content Management page or REST API. I check the scheduler to see if searches are running and producing results. In the internal index, I look for scheduler logs for that specific search. If the search runs but doesn't create notables, I check the adaptive response action configuration. I verify the notable index is receiving data and the modular alert action is working. For ES-specific issues, I check if data models are accelerated (most ES searches depend on them), verify asset and identity lookups are populated, and confirm the user has appropriate roles. I also check if there are any throttling configurations that might be suppressing alerts.</p>
            </div>

            <div class="qa-box">
                <h4>Q: What steps would you take if the indexer cluster shows bucket fixup issues?</h4>
                <p><strong>A:</strong> Bucket fixup indicates data replication inconsistencies. I start by checking cluster health on the master: <code>| rest /services/cluster/master/health</code> to see which factors are failing. I review the fixup queue: <code>| rest /services/cluster/master/fixup</code> to understand the scope and reasons. Common causes include peer outages, network issues, or disk space problems. I verify all peers are online and have adequate disk space. If a peer was down temporarily, fixup should resolve automatically—I monitor progress. For persistent issues, I check the master's splunkd.log for specific error messages. I ensure the replication and search factors are achievable given available peers. In severe cases, I may need to manually remove orphaned buckets or consider rolling restart after the queue clears. Throughout, I avoid making changes during active fixup to prevent additional churn.</p>
            </div>

            <div class="qa-box">
                <h4>Q: How do you handle license violation warnings?</h4>
                <p><strong>A:</strong> License violations require both immediate response and long-term planning. I first assess the severity—Splunk allows a few violations in a 30-day window before enforcement. I identify what's causing the spike using <code>index=_internal source=*license_usage.log</code> to see volume by sourcetype and index. Common causes include new data sources, misconfigured forwarders (sending duplicate data), verbose logging enabled accidentally, or growth in existing sources. For immediate relief, I identify and reduce unnecessary verbose logging, implement sourcetype filtering at the forwarder level, or temporarily disable non-critical inputs. For long-term, I work on data retention policies, implement filtering for noisy sources, consider summary indexing for commonly searched data, and plan for license expansion if growth is legitimate. I also set up alerts for when daily usage approaches 80% of quota to get early warning.</p>
            </div>
        </section>

        <div class="nav-buttons">
            <a href="dashboards.html" class="btn btn-secondary">← Dashboards</a>
            <a href="index.html" class="btn btn-primary">Back to Splunk Index</a>
        </div>
    </main>

    <footer class="main-footer">
        <p>&copy; 2024 SOC Compendium. For educational purposes.</p>
    </footer>
</body>
</html>
