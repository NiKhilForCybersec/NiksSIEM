<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    <title>Notable Events - SOC Compendium</title>
    <link rel="stylesheet" href="../assets/css/main.css">
</head>
<body>
    <nav class="main-nav">
        <div class="nav-container">
            <div class="nav-logo">SOC Compendium</div>
            <ul class="nav-links">
                <li><a href="../index.html">Home</a></li>
                <li><a href="../cortex/index.html">Cortex XDR</a></li>
                <li><a href="../crowdstrike/index.html">CrowdStrike</a></li>
                <li><a href="../mde/index.html">Microsoft Defender</a></li>
                <li><a href="../sentinel/index.html">Sentinel</a></li>
                <li><a href="index.html" class="active">Splunk</a></li>
                <li><a href="../operations/index.html">Operations</a></li>
            </ul>
        </div>
    </nav>

    <main class="content">
        <div class="content-header">
            <h1>Notable Events</h1>
            <p class="subtitle">Managing, triaging, and investigating ES notable events</p>
        </div>

        <section class="doc-section">
            <h2>Notable Events Overview</h2>
            <p>Notable events are the primary alert mechanism in Splunk Enterprise Security. Generated by correlation searches, they represent security-relevant findings that require analyst attention. Effective notable event management is central to SOC operations.</p>

            <div class="info-box">
                <h4>Notable Event Lifecycle</h4>
                <ol>
                    <li><strong>Generation:</strong> Correlation search triggers and creates notable event</li>
                    <li><strong>Assignment:</strong> Event assigned to analyst (manual or automatic)</li>
                    <li><strong>Triage:</strong> Initial assessment and priority validation</li>
                    <li><strong>Investigation:</strong> Deep dive into contributing events and context</li>
                    <li><strong>Response:</strong> Containment, remediation, or escalation actions</li>
                    <li><strong>Closure:</strong> Final disposition with documentation</li>
                </ol>
            </div>
        </section>

        <section class="doc-section">
            <h2>Notable Event Anatomy</h2>

            <h3>Key Notable Event Fields</h3>
            <div class="styled-table">
                <table>
                    <thead>
                        <tr>
                            <th>Field</th>
                            <th>Description</th>
                            <th>Example Values</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><code>event_id</code></td>
                            <td>Unique identifier for the notable</td>
                            <td>4A23B7C8-1234-5678-90AB-CDEF12345678</td>
                        </tr>
                        <tr>
                            <td><code>rule_name</code></td>
                            <td>Correlation search that generated the event</td>
                            <td>Brute Force Access Behavior Detected</td>
                        </tr>
                        <tr>
                            <td><code>urgency</code></td>
                            <td>Calculated priority (asset/identity priority × rule severity)</td>
                            <td>critical, high, medium, low, informational</td>
                        </tr>
                        <tr>
                            <td><code>status</code></td>
                            <td>Current workflow state</td>
                            <td>new, in progress, pending, resolved, closed</td>
                        </tr>
                        <tr>
                            <td><code>owner</code></td>
                            <td>Assigned analyst</td>
                            <td>jsmith, unassigned</td>
                        </tr>
                        <tr>
                            <td><code>security_domain</code></td>
                            <td>Category of security concern</td>
                            <td>access, endpoint, network, threat, identity</td>
                        </tr>
                        <tr>
                            <td><code>src</code></td>
                            <td>Source of the activity</td>
                            <td>IP address, hostname</td>
                        </tr>
                        <tr>
                            <td><code>dest</code></td>
                            <td>Destination of the activity</td>
                            <td>IP address, hostname</td>
                        </tr>
                        <tr>
                            <td><code>user</code></td>
                            <td>Associated user account</td>
                            <td>jdoe@company.com</td>
                        </tr>
                        <tr>
                            <td><code>risk_score</code></td>
                            <td>Accumulated risk for entities</td>
                            <td>0-100+ (higher = more risk)</td>
                        </tr>
                    </tbody>
                </table>
            </div>

            <h3>Notable Event Status Workflow</h3>
            <div class="code-block">
                <div class="code-header">Status Definitions</div>
                <pre>Status Values and Meanings:
├── New (0)           → Unreviewed, unassigned notable
├── In Progress (1)   → Analyst actively investigating
├── Pending (2)       → Waiting on external input/action
├── Resolved (3)      → Investigation complete, awaiting confirmation
└── Closed (4/5)      → Final state (True Positive/False Positive)

Common Disposition Labels:
├── Resolved - True Positive: Confirmed security incident
├── Resolved - False Positive: Benign activity, detection tuning needed
├── Resolved - Benign: Expected behavior, no action needed
├── Resolved - Policy Violation: Non-security policy issue
└── Resolved - Duplicate: Already handled in another notable</pre>
            </div>

            <h3>Urgency Calculation</h3>
            <div class="info-box">
                <h4>Urgency Matrix</h4>
                <p>Urgency = Asset/Identity Priority × Rule Severity</p>
                <table style="width: 100%; margin-top: 10px;">
                    <tr>
                        <th></th>
                        <th>Info Severity</th>
                        <th>Low Severity</th>
                        <th>Medium Severity</th>
                        <th>High Severity</th>
                        <th>Critical Severity</th>
                    </tr>
                    <tr>
                        <td><strong>Critical Asset</strong></td>
                        <td>low</td>
                        <td>medium</td>
                        <td>high</td>
                        <td>critical</td>
                        <td>critical</td>
                    </tr>
                    <tr>
                        <td><strong>High Asset</strong></td>
                        <td>info</td>
                        <td>low</td>
                        <td>medium</td>
                        <td>high</td>
                        <td>critical</td>
                    </tr>
                    <tr>
                        <td><strong>Medium Asset</strong></td>
                        <td>info</td>
                        <td>low</td>
                        <td>medium</td>
                        <td>medium</td>
                        <td>high</td>
                    </tr>
                    <tr>
                        <td><strong>Low Asset</strong></td>
                        <td>info</td>
                        <td>info</td>
                        <td>low</td>
                        <td>low</td>
                        <td>medium</td>
                    </tr>
                    <tr>
                        <td><strong>Unknown Asset</strong></td>
                        <td>info</td>
                        <td>low</td>
                        <td>medium</td>
                        <td>high</td>
                        <td>critical</td>
                    </tr>
                </table>
            </div>
        </section>

        <section class="doc-section">
            <h2>Working with Notable Events</h2>

            <h3>Incident Review Dashboard</h3>
            <p>The Incident Review dashboard is the primary interface for notable event triage.</p>

            <div class="info-box">
                <h4>Incident Review Best Practices</h4>
                <ul>
                    <li><strong>Filter First:</strong> Use urgency and status filters to focus on actionable items</li>
                    <li><strong>Sort by Priority:</strong> Critical/High urgency should be addressed first</li>
                    <li><strong>Bulk Operations:</strong> Use multi-select for similar events requiring same action</li>
                    <li><strong>Drilldown:</strong> Click event_id to access full details and contributing events</li>
                    <li><strong>Notes:</strong> Document investigation steps in comments field</li>
                </ul>
            </div>

            <h3>Notable Event Queries</h3>
            <div class="code-block">
                <div class="code-header">Query Notable Events Index</div>
                <pre># All notable events from last 24 hours
index=notable earliest=-24h
| table _time, rule_name, urgency, status_label, owner, src, dest, user

# Open critical/high notables
index=notable status IN (0, 1, 2) urgency IN (critical, high)
| stats count by rule_name, urgency
| sort - urgency, - count

# My assigned notables
index=notable owner="analyst_username" status IN (0, 1, 2)
| table _time, rule_name, urgency, status_label, src, dest, user
| sort - urgency, - _time

# Notable event aging (time since creation)
index=notable status IN (0, 1, 2)
| eval age_hours=round((now()-_time)/3600, 1)
| stats count, avg(age_hours) as avg_age, max(age_hours) as max_age 
    by urgency, status_label
| sort - urgency

# False positive analysis
index=notable status_label="Resolved - False Positive" earliest=-30d
| stats count by rule_name
| sort - count
| head 20</pre>
            </div>

            <h3>Notable Event Operations via SPL</h3>
            <div class="code-block">
                <div class="code-header">Bulk Notable Event Updates</div>
                <pre># Assign notables to analyst
index=notable rule_name="Low Priority Rule" status=0
| eval new_owner="analyst1"
| sendalert notable_update param.owner="$new_owner$"

# Close multiple false positives
index=notable rule_name="Noisy Detection" status=0
    src_ip IN (192.168.1.100, 192.168.1.101, 192.168.1.102)
| sendalert notable_update 
    param.status="5" 
    param.status_label="Resolved - False Positive"
    param.comment="Known scanner IPs - tuning ticket TUN-2024-001"

# Escalate to in progress with owner
| makeresults
| eval event_id="YOUR-EVENT-ID-HERE"
| sendalert notable_update 
    param.event_id="$event_id$"
    param.status="1"
    param.owner="senior_analyst"
    param.comment="Escalated for review - potential lateral movement"</pre>
            </div>

            <h3>REST API for Notable Operations</h3>
            <div class="code-block">
                <div class="code-header">Notable Event API Operations</div>
                <pre># Get notable event details
curl -k -u admin:password \
  "https://splunk:8089/services/notable/notable_id"

# Update notable event status
curl -k -u admin:password \
  -X POST "https://splunk:8089/services/notable/notable_id" \
  -d "status=1&owner=analyst1&comment=Starting%20investigation"

# Bulk update notables matching criteria
curl -k -u admin:password \
  -X POST "https://splunk:8089/services/notable/notable_update" \
  -d "search=index=notable rule_name=\"Test Rule\"" \
  -d "status=5" \
  -d "status_label=Resolved - False Positive" \
  -d "comment=Bulk close - known false positives"

# Get notable event comments/history
curl -k -u admin:password \
  "https://splunk:8089/services/notable/notable_id/comments"</pre>
            </div>
        </section>

        <section class="doc-section">
            <h2>Triage Methodology</h2>

            <h3>Initial Triage Steps</h3>
            <div class="code-block">
                <div class="code-header">Standard Triage Process</div>
                <pre>1. VALIDATE URGENCY
   ├── Check asset/identity priority accuracy
   ├── Verify rule severity is appropriate
   └── Adjust if urgency doesn't match actual risk

2. REVIEW DETECTION CONTEXT
   ├── Read rule description and intent
   ├── Check contributing events count
   ├── Note time span of activity
   └── Identify key entities (src, dest, user)

3. ENTITY INVESTIGATION
   ├── Asset: Is it known? What's its role? Criticality?
   ├── User: Normal behavior? Role-appropriate access?
   ├── IP: Internal/External? Reputation? Geolocation?
   └── Check recent risk scores for entities

4. CONTRIBUTING EVENTS ANALYSIS
   ├── View raw events that triggered detection
   ├── Look for patterns or anomalies
   ├── Check for related activity before/after
   └── Pivot to other data sources as needed

5. CONTEXTUAL ENRICHMENT
   ├── Threat intelligence lookups
   ├── Historical baseline comparison
   ├── Related notable events
   └── External intelligence (OSINT)

6. DISPOSITION DECISION
   ├── True Positive → Escalate/Respond
   ├── False Positive → Document and tune
   ├── Benign → Close with context
   └── Needs More Info → Mark pending</pre>
            </div>

            <h3>Triage Queries by Detection Type</h3>
            <div class="code-block">
                <div class="code-header">Authentication Anomaly Triage</div>
                <pre># Context: Brute Force / Failed Authentication alert

# Step 1: View the contributing events
index=notable event_id="EVENT_ID_HERE"
| fields orig_raw, contributing_events
| mvexpand contributing_events

# Step 2: Analyze authentication patterns for source
index=authentication src_ip=ALERT_SRC_IP earliest=-24h
| stats count, dc(user) as unique_users, 
    values(user) as users,
    dc(dest) as unique_dests,
    values(action) as actions by src_ip
| eval user_spray=if(unique_users>10, "YES", "NO")

# Step 3: Check user authentication baseline
index=authentication user=ALERT_USER earliest=-7d
| stats count by src_ip, dest, action
| sort - count

# Step 4: Verify if account is locked
index=windows EventCode=4740 TargetUserName=ALERT_USER
| table _time, TargetUserName, TargetDomainName, SubjectUserName

# Step 5: Check for successful auth following failures
index=authentication user=ALERT_USER action=success 
    earliest=-1h latest=now
| table _time, src_ip, dest, app, action

# DECISION MATRIX:
# - Many users from one IP = Likely spray attack (TP)
# - One user, many fails, then success = Possible compromise (TP)
# - One user, many fails, no success, known bad password = FP
# - VPN/Service account with scheduled reconnects = FP</pre>
            </div>

            <div class="code-block">
                <div class="code-header">Malware/Endpoint Alert Triage</div>
                <pre># Context: Malware/Suspicious Process Detection

# Step 1: Get full process context
index=endpoint sourcetype=sysmon EventCode=1 
    ComputerName=ALERT_HOST earliest=-1h
| search ProcessId=ALERT_PID OR ParentProcessId=ALERT_PID
| table _time, User, ParentImage, ParentCommandLine, 
    Image, CommandLine, ProcessId, ParentProcessId

# Step 2: Check file hash reputation
| makeresults 
| eval file_hash="SHA256_FROM_ALERT"
| lookup file_intel file_hash OUTPUT threat_key, description
| table file_hash, threat_key, description

# Step 3: Network connections from process
index=endpoint sourcetype=sysmon EventCode=3 
    ComputerName=ALERT_HOST ProcessId=ALERT_PID
| table _time, Image, DestinationIp, DestinationPort, DestinationHostname
| lookup ip_intel ip as DestinationIp OUTPUT threat_key

# Step 4: Files created by process
index=endpoint sourcetype=sysmon EventCode=11 
    ComputerName=ALERT_HOST ProcessId=ALERT_PID
| table _time, Image, TargetFilename

# Step 5: Registry modifications
index=endpoint sourcetype=sysmon EventCode IN (12,13,14)
    ComputerName=ALERT_HOST ProcessId=ALERT_PID
| table _time, Image, EventType, TargetObject, Details

# Step 6: Check if software is approved
| inputlookup approved_software.csv
| search process_name="ALERT_PROCESS_NAME"

# DECISION MATRIX:
# - Hash matches known malware = TP, initiate IR
# - Connections to threat intel IPs = TP, investigate
# - Creates persistence mechanisms = TP, investigate
# - Approved software, normal behavior = FP, tune detection
# - Pentest tools during authorized test = Benign</pre>
            </div>

            <div class="code-block">
                <div class="code-header">Network Threat Activity Triage</div>
                <pre># Context: Connection to Known Malicious IP/Domain

# Step 1: Understand the traffic pattern
index=firewall dest_ip=ALERT_DEST_IP earliest=-24h
| stats count, sum(bytes_out) as bytes_out, 
    sum(bytes_in) as bytes_in,
    values(dest_port) as ports,
    dc(src_ip) as unique_sources by dest_ip
| eval mb_out=round(bytes_out/1024/1024,2), 
    mb_in=round(bytes_in/1024/1024,2)

# Step 2: Check threat intelligence details
| makeresults 
| eval ip="ALERT_DEST_IP"
| lookup ip_intel ip OUTPUT threat_key, description, threat_category, weight
| table ip, threat_key, threat_category, description, weight

# Step 3: DNS resolution history
index=dns query_answer=ALERT_DEST_IP OR query=ALERT_DOMAIN
| stats count, values(query) as domains, values(src_ip) as resolvers 
    by query_answer

# Step 4: Identify all internal hosts communicating
index=firewall dest_ip=ALERT_DEST_IP OR dest=ALERT_DOMAIN
    earliest=-7d
| stats count, first(_time) as first_seen, last(_time) as last_seen,
    sum(bytes_out) as total_bytes by src_ip
| sort - count

# Step 5: Check proxy logs for additional context
index=proxy dest_ip=ALERT_DEST_IP earliest=-24h
| table _time, src_ip, user, url, http_method, status, bytes_out
| sort - _time

# Step 6: Determine if traffic was blocked
index=firewall dest_ip=ALERT_DEST_IP action=* 
| stats count by action

# DECISION MATRIX:
# - High-confidence intel + allowed traffic = TP, block & investigate
# - Low-confidence intel + blocked = Benign, intel working
# - Single connection, no data transfer = Likely redirect/ad
# - Multiple hosts, ongoing = TP, potential campaign
# - Known CDN/cloud IP in old blocklist = FP, update intel</pre>
            </div>
        </section>

        <section class="doc-section">
            <h2>Notable Event Creation</h2>

            <h3>Correlation Search to Notable</h3>
            <div class="code-block">
                <div class="code-header">Creating Notable Events from Correlation Searches</div>
                <pre># Basic structure of a correlation search that creates notables

# The search that detects the condition
index=authentication action=failure
| stats count as failure_count by src_ip, user
| where failure_count > 10

# Add notable action in Adaptive Response Actions:
# Action: Create Notable Event
# Title: Brute Force Attempt Detected
# Description: $failure_count$ failed authentication attempts from $src_ip$ for user $user$
# Security Domain: access
# Severity: high

# SPL-based notable creation (alternative)
index=authentication action=failure
| stats count as failure_count by src_ip, user
| where failure_count > 10
| sendalert notable 
    param.title="Brute Force Attempt Detected"
    param.description="$failure_count$ failed attempts from $src_ip$"
    param.security_domain="access"
    param.severity="high"
    param.drilldown_name="View Authentication Events"
    param.drilldown_search="index=authentication src_ip=$src_ip$ user=$user$"</pre>
            </div>

            <h3>Notable Event Throttling</h3>
            <div class="code-block">
                <div class="code-header">Throttle Configuration</div>
                <pre># Throttle settings prevent alert fatigue from repetitive events

# In savedsearches.conf:
[My Correlation Search]
alert.suppress = 1
alert.suppress.period = 3600
alert.suppress.fields = src_ip, user
# This prevents duplicate notables for same src_ip+user 
# combination within 1 hour

# Throttle in search (alternative approach)
index=authentication action=failure
| stats count as failure_count, latest(_time) as last_seen by src_ip, user
| where failure_count > 10

# Check for existing recent notable
| join type=left src_ip, user [
    | search index=notable rule_name="Brute Force*" status!=5
        earliest=-1h
    | stats count as existing_notable by src_ip, user
]
| where isnull(existing_notable) OR existing_notable=0

# Only create notable if no recent one exists</pre>
            </div>

            <h3>Custom Notable Fields</h3>
            <div class="code-block">
                <div class="code-header">Adding Custom Fields to Notables</div>
                <pre># Add contextual fields that will appear in the notable event

index=endpoint EventCode=1
| stats count by ComputerName, Image, CommandLine, User
| where count > 100

# Add custom fields for better context
| eval technique_id="T1059.001",
       technique_name="PowerShell",
       tactic="Execution",
       mitre_link="https://attack.mitre.org/techniques/T1059/001",
       analyst_guidance="Check for encoded commands, unusual parent processes"

# These fields can be referenced in notable title/description:
# param.title="MITRE $technique_id$: $technique_name$ on $ComputerName$"
# param.description="$analyst_guidance$"

# Risk-based notable creation (adds to entity risk scores)
| sendalert risk 
    param.risk_object_field="ComputerName"
    param.risk_object_type="system"
    param.risk_score="40"
    param.risk_message="High volume PowerShell execution"</pre>
            </div>
        </section>

        <section class="doc-section">
            <h2>Notable Event Automation</h2>

            <h3>Auto-Assignment Rules</h3>
            <div class="code-block">
                <div class="code-header">Automatic Notable Assignment</div>
                <pre># Create a scheduled search that auto-assigns notables

# Example: Assign by security domain
| search index=notable status=0
| eval owner=case(
    security_domain="endpoint", "endpoint_team",
    security_domain="network", "network_team",
    security_domain="access", "identity_team",
    security_domain="threat", "threat_intel_team",
    1=1, "tier1_queue"
  )
| sendalert notable_update param.owner="$owner$"

# Example: Assign critical notables to senior analysts
| search index=notable status=0 urgency=critical
| eval owner=case(
    rule_name LIKE "%Ransomware%", "ir_team",
    rule_name LIKE "%Data Exfil%", "ir_team",
    1=1, "senior_analyst"
  )
| sendalert notable_update 
    param.owner="$owner$" 
    param.status="1"
    param.comment="Auto-assigned due to critical urgency"

# Example: Assign based on time/shift
| search index=notable status=0
| eval hour=strftime(now(), "%H")
| eval owner=case(
    hour>=6 AND hour<14, "shift_a_analyst",
    hour>=14 AND hour<22, "shift_b_analyst",
    1=1, "shift_c_analyst"
  )
| sendalert notable_update param.owner="$owner$"</pre>
            </div>

            <h3>Auto-Close Rules</h3>
            <div class="code-block">
                <div class="code-header">Automatic Notable Closure</div>
                <pre># Auto-close notables that match known false positive patterns
# Run as scheduled search every 15 minutes

# Pattern 1: Known scanner IPs
| search index=notable status=0 rule_name="Network Scan*"
| lookup known_scanners.csv src_ip OUTPUT scanner_name
| where isnotnull(scanner_name)
| sendalert notable_update 
    param.status="5"
    param.status_label="Resolved - False Positive"
    param.comment="Auto-closed: Known scanner - $scanner_name$"

# Pattern 2: Authorized testing
| search index=notable status=0 
| lookup authorized_testing.csv src_ip, user 
    OUTPUT test_id, test_end_date
| where isnotnull(test_id) AND now() < strptime(test_end_date, "%Y-%m-%d")
| sendalert notable_update
    param.status="3"
    param.status_label="Resolved - Benign"
    param.comment="Auto-closed: Authorized test $test_id$"

# Pattern 3: Service accounts with expected behavior
| search index=notable status=0 rule_name="Unusual Authentication*"
| lookup service_account_exceptions.csv user 
    OUTPUT exception_reason, approved_by
| where isnotnull(exception_reason)
| sendalert notable_update
    param.status="5"
    param.status_label="Resolved - False Positive"
    param.comment="Auto-closed: $exception_reason$ (Approved: $approved_by$)"</pre>
            </div>

            <h3>Escalation Automation</h3>
            <div class="code-block">
                <div class="code-header">SLA-Based Escalation</div>
                <pre># Escalate notables that exceed SLA thresholds
# Critical: 15 min, High: 1 hour, Medium: 4 hours

| search index=notable status IN (0, 1)
| eval age_minutes=round((now()-_time)/60, 0)
| eval sla_threshold=case(
    urgency="critical", 15,
    urgency="high", 60,
    urgency="medium", 240,
    1=1, 480
  )
| where age_minutes > sla_threshold
| eval escalation_level=case(
    age_minutes > sla_threshold*4, "manager",
    age_minutes > sla_threshold*2, "lead",
    1=1, "senior_analyst"
  )
| sendalert email 
    param.to="$escalation_level$@company.com"
    param.subject="SLA Breach: $rule_name$"
    param.message="Notable $event_id$ has exceeded SLA by 
                   $eval(age_minutes-sla_threshold)$ minutes"

# Also update the notable with escalation note
| sendalert notable_update
    param.comment="Auto-escalated to $escalation_level$ due to SLA breach"</pre>
            </div>
        </section>

        <section class="doc-section">
            <h2>Notable Event Reporting</h2>

            <h3>Operational Metrics</h3>
            <div class="code-block">
                <div class="code-header">SOC Performance Dashboard Queries</div>
                <pre># Mean Time to Acknowledge (MTTA)
index=notable_update earliest=-30d
| transaction event_id maxspan=7d
| eval first_assignment=mvindex(status, mvfind(status, "1"))
| where isnotnull(first_assignment)
| eval mtta_minutes=round((first_assignment_time - _time)/60, 1)
| stats avg(mtta_minutes) as avg_mtta, 
    median(mtta_minutes) as median_mtta,
    perc90(mtta_minutes) as p90_mtta by urgency

# Mean Time to Resolution (MTTR)
index=notable status IN (3, 4, 5) earliest=-30d
| eval resolution_time=_time
| join event_id [
    | search index=notable status=0
    | eval creation_time=_time
    | table event_id, creation_time
]
| eval mttr_hours=round((resolution_time - creation_time)/3600, 2)
| stats avg(mttr_hours) as avg_mttr,
    median(mttr_hours) as median_mttr,
    perc90(mttr_hours) as p90_mttr by urgency, status_label

# Alert Volume Trends
index=notable earliest=-30d
| timechart span=1d count by urgency

# Analyst Workload Distribution
index=notable status IN (0, 1, 2)
| stats count as open_notables, 
    count(eval(urgency="critical")) as critical,
    count(eval(urgency="high")) as high by owner
| sort - open_notables

# Closure Rate
index=notable earliest=-7d
| stats count(eval(status=0)) as new,
    count(eval(status IN (3,4,5))) as closed,
    count as total
| eval closure_rate=round(closed/total*100, 1)."%"

# False Positive Rate by Rule
index=notable earliest=-30d status IN (3, 4, 5)
| stats count as total,
    count(eval(status_label="Resolved - False Positive")) as fp,
    count(eval(status_label="Resolved - True Positive")) as tp
    by rule_name
| eval fp_rate=round(fp/total*100, 1)
| where total > 10
| sort - fp_rate
| head 20</pre>
            </div>

            <h3>Executive Reporting</h3>
            <div class="code-block">
                <div class="code-header">Weekly Security Summary</div>
                <pre># Weekly Notable Event Summary
index=notable earliest=-7d
| stats count as total_alerts,
    count(eval(status_label="Resolved - True Positive")) as incidents,
    count(eval(urgency="critical")) as critical_alerts,
    count(eval(urgency="high")) as high_alerts,
    dc(rule_name) as unique_detection_types,
    dc(src) as unique_sources,
    dc(dest) as unique_targets

# Top Threats Detected
index=notable earliest=-7d status_label="Resolved - True Positive"
| stats count by rule_name
| sort - count
| head 10

# Top Affected Assets
index=notable earliest=-7d urgency IN (critical, high)
| stats count, values(rule_name) as detection_types by dest
| lookup asset_lookup_by_str ip as dest 
    OUTPUT category as asset_category, owner
| sort - count
| head 10

# Trend Comparison (this week vs last week)
index=notable earliest=-14d
| eval week=if(_time > relative_time(now(), "-7d"), "This Week", "Last Week")
| stats count as alerts,
    count(eval(urgency="critical")) as critical,
    count(eval(status_label="Resolved - True Positive")) as incidents
    by week
| transpose
| rename column as metric, "This Week" as current, "Last Week" as previous
| eval change=round((current-previous)/previous*100, 1)."%"</pre>
            </div>
        </section>

        <section class="doc-section">
            <h2>Best Practices</h2>

            <div class="info-box">
                <h4>Notable Event Management Best Practices</h4>
                <ul>
                    <li><strong>Triage Queue Management:</strong> Process notables in urgency order; critical first, then high, etc.</li>
                    <li><strong>Documentation:</strong> Always add investigation notes before closing—future analysts will thank you</li>
                    <li><strong>Consistent Dispositions:</strong> Use standard status labels; avoid free-text variations</li>
                    <li><strong>Tuning Feedback:</strong> Track false positives and feed back to detection engineering</li>
                    <li><strong>SLA Compliance:</strong> Monitor time-to-acknowledge and time-to-resolution metrics</li>
                    <li><strong>Correlation:</strong> Before closing, check for related notables that might indicate larger campaign</li>
                    <li><strong>Risk Contextualization:</strong> Verify urgency calculation by checking asset/identity enrichment</li>
                    <li><strong>Avoid Alert Fatigue:</strong> Proactively tune high-volume/low-value detections</li>
                </ul>
            </div>

            <div class="info-box">
                <h4>Common Mistakes to Avoid</h4>
                <ul>
                    <li><strong>Closing without Investigation:</strong> Don't bulk-close without reviewing contributing events</li>
                    <li><strong>Ignoring Low Urgency:</strong> Low urgency doesn't mean no risk—review periodically</li>
                    <li><strong>Missing Correlation:</strong> Single notable might be part of attack chain—pivot to related events</li>
                    <li><strong>Inadequate Notes:</strong> "Looks fine" is not documentation—explain why it's benign</li>
                    <li><strong>Ignoring Repeat Offenders:</strong> Same rule, same entity, multiple times = needs tuning or investigation</li>
                    <li><strong>Not Updating Status:</strong> Notables sitting in "In Progress" for days lose visibility</li>
                </ul>
            </div>
        </section>

        <section class="doc-section">
            <h2>Interview Questions</h2>
            
            <div class="qa-box">
                <h4>Q: Walk me through your process for triaging a notable event in Splunk ES.</h4>
                <p><strong>A:</strong> I follow a structured approach: First, I review the notable details—rule name, urgency, and key entities (src, dest, user). I verify the urgency is appropriate by checking if the asset and identity lookups are enriching correctly. Then I read the rule description to understand what behavior was detected. I examine the contributing events to see the raw data that triggered the alert. For the key entities, I gather context: Is this asset known? What's the user's normal behavior? Does the IP have threat intel hits? I check for related notable events that might indicate a broader attack. Based on this analysis, I make a disposition decision—true positive requiring response, false positive needing tuning, or benign expected behavior. I document my investigation steps in the comments field regardless of outcome.</p>
            </div>

            <div class="qa-box">
                <h4>Q: How do you handle a sudden spike in notable events?</h4>
                <p><strong>A:</strong> First, I assess whether it's a real threat or a detection issue. I look for commonalities—same rule, same source, same time pattern. If one rule is responsible, I check if something changed in the environment (new system, configuration change) or if the detection logic has a problem. If multiple rules are firing on the same entity, it might be an actual attack worth urgent investigation. For operational response, I'd implement temporary throttling if it's clearly false positives while the tuning is addressed. I'd communicate with the team about the situation and prioritize any critical/high urgency items that need immediate attention. Post-incident, I'd create tuning tickets for problematic rules and possibly implement auto-close logic for confirmed false positive patterns to prevent recurrence.</p>
            </div>

            <div class="qa-box">
                <h4>Q: How would you measure and improve your team's notable event handling efficiency?</h4>
                <p><strong>A:</strong> I track several key metrics: Mean Time to Acknowledge (MTTA) shows how quickly we start investigating—target under 15 minutes for critical. Mean Time to Resolution (MTTR) measures total handling time. False positive rate by rule identifies detections needing tuning. Alert volume per analyst helps balance workload. I create dashboards showing these metrics trending over time. To improve efficiency, I implement auto-assignment rules to route notables to appropriate analysts immediately. I create standardized triage playbooks so analysts don't reinvent the wheel. I identify high-volume/low-value rules and either tune them or implement auto-close for known patterns. I hold regular review sessions to share knowledge about common scenarios and improve collective response speed.</p>
            </div>

            <div class="qa-box">
                <h4>Q: Describe how you would set up notable event automation for a SOC.</h4>
                <p><strong>A:</strong> I'd start with auto-assignment to eliminate manual queue sorting—route notables based on security domain, rule type, or entity to appropriate analysts or queues. Then I'd implement auto-enrichment that adds context automatically, like threat intel lookups and asset criticality, so analysts don't have to manually gather this information. For known false positive patterns, I'd create auto-close rules with proper documentation—things like authorized scanners, service accounts with expected behavior, or systems during maintenance windows. I'd set up SLA-based escalation to automatically alert leads when notables exceed response time thresholds. Finally, I'd create auto-notification for critical events to ensure immediate awareness through email, Slack, or PagerDuty. All automation should be well-documented and reviewed regularly to ensure it's not masking real threats.</p>
            </div>

            <div class="qa-box">
                <h4>Q: How do you ensure proper documentation and knowledge transfer in notable event handling?</h4>
                <p><strong>A:</strong> Documentation happens at multiple levels. For individual notables, I require meaningful comments before closure—not just "closed" but the investigation rationale. I use standardized disposition labels so reporting is consistent. For common scenarios, I create triage runbooks that document what to check and typical patterns for specific rule types. These live in a wiki and are referenced in the rule descriptions. I track false positive patterns in a tuning tracker so we can see trends and prioritize fixes. For knowledge transfer, I hold regular case reviews where we discuss interesting or challenging notables as a team. New analysts shadow experienced ones during their first weeks. I maintain a "lessons learned" document for significant incidents that becomes part of training materials. The goal is that any analyst should be able to pick up any notable and know what to do.</p>
            </div>
        </section>

        <div class="nav-buttons">
            <a href="risk-based-alerting.html" class="btn btn-secondary">← Risk-Based Alerting</a>
            <a href="threat-intel.html" class="btn btn-primary">Threat Intelligence →</a>
        </div>
    </main>

    <footer class="main-footer">
        <p>&copy; 2024 SOC Compendium. For educational purposes.</p>
    </footer>
</body>
</html>
