<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Data Ingestion Complete Guide | Splunk</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    <link rel="stylesheet" href="../assets/css/main.css">


</head>
<body>
    <div class="app-container">
        
    <!-- Sidebar -->
        <!-- Sidebar -->
        <!-- Sidebar -->
    <aside class="sidebar" id="sidebar">
        <div class="sidebar-header">
            <a href="../index.html" class="sidebar-brand">
                <i class="fas fa-shield-alt"></i>
                <span>Nik's SIEM</span>
            </a>
            <button class="sidebar-toggle" onclick="toggleSidebar()" title="Collapse sidebar">
                <i class="fas fa-chevron-left"></i>
            </button>
        </div>
        
        <div class="sidebar-content">
            <!-- Platforms Section -->
            <div class="sidebar-section">
                <div class="sidebar-section-title">Platforms</div>
                <ul class="sidebar-nav">
                    <li class="sidebar-nav-item"><a href="../xsiam/index.html" class="sidebar-nav-link"><span class="platform-dot"></span><span>XSIAM</span></a></li>
                    <li class="sidebar-nav-item"><a href="../splunk/index.html" class="sidebar-nav-link active"><span class="platform-dot"></span><span>Splunk</span></a></li>
                    <li class="sidebar-nav-item"><a href="../sentinel/index.html" class="sidebar-nav-link"><span class="platform-dot"></span><span>Sentinel</span></a></li>
                    <li class="sidebar-nav-item"><a href="../crowdstrike/index.html" class="sidebar-nav-link"><span class="platform-dot"></span><span>CrowdStrike</span></a></li>
                    <li class="sidebar-nav-item"><a href="../cortex/index.html" class="sidebar-nav-link"><span class="platform-dot"></span><span>Cortex XDR</span></a></li>
                    <li class="sidebar-nav-item"><a href="../mde/index.html" class="sidebar-nav-link"><span class="platform-dot"></span><span>Defender for Endpoint</span></a></li>
                    <li class="sidebar-nav-item"><a href="../operations/index.html" class="sidebar-nav-link"><span class="platform-dot"></span><span>SOC Operations</span></a></li>
                </ul>
            </div>

            <div class="sidebar-divider"></div>
            
            <!-- Getting Started -->
            <div class="sidebar-section">
                <div class="sidebar-section-title">Getting Started</div>
                <ul class="sidebar-nav">
                    <li class="sidebar-nav-item"><a href="index.html" class="sidebar-nav-link"><i class="fas fa-home"></i><span>Overview</span></a></li>
                    <li class="sidebar-nav-item"><a href="architecture.html" class="sidebar-nav-link"><i class="fas fa-sitemap"></i><span>Architecture</span></a></li>
                    <li class="sidebar-nav-item"><a href="apps-tas.html" class="sidebar-nav-link"><i class="fas fa-puzzle-piece"></i><span>Apps & TAs</span></a></li>
                </ul>
            </div>
            
            <!-- Data Collection -->
            <div class="sidebar-section">
                <div class="sidebar-section-title">Data Collection</div>
                <ul class="sidebar-nav">
                    <li class="sidebar-nav-item"><a href="data-ingestion.html" class="sidebar-nav-link"><i class="fas fa-database"></i><span>Data Ingestion</span></a></li>
                    <li class="sidebar-nav-item"><a href="forwarders.html" class="sidebar-nav-link"><i class="fas fa-paper-plane"></i><span>Forwarders</span></a></li>
                    <li class="sidebar-nav-item"><a href="custom-log-onboarding.html" class="sidebar-nav-link"><i class="fas fa-plus-circle"></i><span>Custom Log Onboarding</span></a></li>
                    <li class="sidebar-nav-item"><a href="parsing-flows.html" class="sidebar-nav-link"><i class="fas fa-stream"></i><span>Parsing & Props/Transforms</span></a></li>
                    <li class="sidebar-nav-item"><a href="retention-tiers.html" class="sidebar-nav-link"><i class="fas fa-archive"></i><span>Index & Retention</span></a></li>
                </ul>
            </div>
            
            <!-- Query & Analysis -->
            <div class="sidebar-section">
                <div class="sidebar-section-title">Query & Analysis</div>
                <ul class="sidebar-nav">
                    <li class="sidebar-nav-item"><a href="spl-fundamentals.html" class="sidebar-nav-link"><i class="fas fa-terminal"></i><span>SPL Fundamentals</span></a></li>
                    <li class="sidebar-nav-item"><a href="spl-intermediate.html" class="sidebar-nav-link"><i class="fas fa-code"></i><span>SPL Intermediate</span></a></li>
                    <li class="sidebar-nav-item"><a href="spl-advanced.html" class="sidebar-nav-link"><i class="fas fa-rocket"></i><span>SPL Advanced</span></a></li>
                    <li class="sidebar-nav-item"><a href="data-models.html" class="sidebar-nav-link"><i class="fas fa-cubes"></i><span>Data Models & CIM</span></a></li>
                    <li class="sidebar-nav-item"><a href="knowledge-objects.html" class="sidebar-nav-link"><i class="fas fa-brain"></i><span>Knowledge Objects</span></a></li>
                    <li class="sidebar-nav-item"><a href="dashboards.html" class="sidebar-nav-link"><i class="fas fa-chart-line"></i><span>Dashboards</span></a></li>
                </ul>
            </div>
            
            <!-- Detection & Response -->
            <div class="sidebar-section">
                <div class="sidebar-section-title">Detection & Response</div>
                <ul class="sidebar-nav">
                    <li class="sidebar-nav-item"><a href="es-overview.html" class="sidebar-nav-link"><i class="fas fa-shield-alt"></i><span>Enterprise Security</span></a></li>
                    <li class="sidebar-nav-item"><a href="correlation-searches.html" class="sidebar-nav-link"><i class="fas fa-project-diagram"></i><span>Correlation Searches</span></a></li>
                    <li class="sidebar-nav-item"><a href="notable-events.html" class="sidebar-nav-link"><i class="fas fa-bell"></i><span>Notable Events</span></a></li>
                    <li class="sidebar-nav-item"><a href="risk-based-alerting.html" class="sidebar-nav-link"><i class="fas fa-exclamation-triangle"></i><span>Risk-Based Alerting</span></a></li>
                    <li class="sidebar-nav-item"><a href="alerting-actions.html" class="sidebar-nav-link"><i class="fas fa-bolt"></i><span>Alerting & Actions</span></a></li>
                    <li class="sidebar-nav-item"><a href="threat-hunting.html" class="sidebar-nav-link"><i class="fas fa-search"></i><span>Threat Hunting</span></a></li>
                </ul>
            </div>
            
                        <!-- Security Use Cases -->
            <div class="sidebar-section">
                <div class="sidebar-section-title">Security Use Cases</div>
                <ul class="sidebar-nav">
                    <li class="sidebar-nav-item"><a href="security-usecases.html" class="sidebar-nav-link"><i class="fas fa-crosshairs"></i><span>Detection Use Cases</span></a></li>
                    <li class="sidebar-nav-item"><a href="enterprise-scenarios.html" class="sidebar-nav-link"><i class="fas fa-building"></i><span>Enterprise Scenarios</span></a></li>
                    <li class="sidebar-nav-item"><a href="retail-security.html" class="sidebar-nav-link"><i class="fas fa-shopping-cart"></i><span>Retail Security</span></a></li>
                    <li class="sidebar-nav-item"><a href="sap-security.html" class="sidebar-nav-link"><i class="fas fa-industry"></i><span>SAP Security</span></a></li>
                </ul>
            </div>
            
            <!-- Automation & Intel -->
            <div class="sidebar-section">
                <div class="sidebar-section-title">Automation & Intel</div>
                <ul class="sidebar-nav">
                    <li class="sidebar-nav-item"><a href="soar.html" class="sidebar-nav-link"><i class="fas fa-robot"></i><span>SOAR</span></a></li>
                    <li class="sidebar-nav-item"><a href="threat-intel.html" class="sidebar-nav-link"><i class="fas fa-skull-crossbones"></i><span>Threat Intelligence</span></a></li>
                </ul>
            </div>
            
            <!-- Operations -->
            <div class="sidebar-section">
                <div class="sidebar-section-title">Operations</div>
                <ul class="sidebar-nav">
                    <li class="sidebar-nav-item"><a href="troubleshooting.html" class="sidebar-nav-link"><i class="fas fa-wrench"></i><span>Troubleshooting</span></a></li>
                    <li class="sidebar-nav-item"><a href="performance-tuning.html" class="sidebar-nav-link"><i class="fas fa-tachometer-alt"></i><span>Performance Tuning</span></a></li>
                    <li class="sidebar-nav-item"><a href="monitoring-console.html" class="sidebar-nav-link"><i class="fas fa-heartbeat"></i><span>Monitoring Console</span></a></li>
                </ul>
            </div>
            
            <!-- Administration -->
            <div class="sidebar-section">
                <div class="sidebar-section-title">Administration</div>
                <ul class="sidebar-nav">
                    <li class="sidebar-nav-item"><a href="administration.html" class="sidebar-nav-link"><i class="fas fa-cog"></i><span>Administration</span></a></li>
                    <li class="sidebar-nav-item"><a href="security-rbac.html" class="sidebar-nav-link"><i class="fas fa-users-cog"></i><span>Security & RBAC</span></a></li>
                    <li class="sidebar-nav-item"><a href="clustering-ha.html" class="sidebar-nav-link"><i class="fas fa-server"></i><span>Clustering & HA</span></a></li>
                </ul>
            </div>
            
            <!-- Deployment -->
            <div class="sidebar-section">
                <div class="sidebar-section-title">Deployment</div>
                <ul class="sidebar-nav">
                    <li class="sidebar-nav-item"><a href="splunk-cloud.html" class="sidebar-nav-link"><i class="fas fa-cloud"></i><span>Splunk Cloud Basics</span></a></li>
                    <li class="sidebar-nav-item"><a href="splunk-cloud-mastery.html" class="sidebar-nav-link"><i class="fas fa-cloud-upload-alt"></i><span>Cloud Mastery</span></a></li>
                </ul>
            </div>
            
            <!-- Certification Prep -->
            <div class="sidebar-section">
                <div class="sidebar-section-title">Certification Prep</div>
                <ul class="sidebar-nav">
                    <li class="sidebar-nav-item"><a href="power-user-certification.html" class="sidebar-nav-link"><i class="fas fa-certificate"></i><span>Power User Cert</span></a></li>
                    <li class="sidebar-nav-item"><a href="admin-certification.html" class="sidebar-nav-link"><i class="fas fa-user-cog"></i><span>Admin Cert</span></a></li>
                    <li class="sidebar-nav-item"><a href="es-admin-certification.html" class="sidebar-nav-link"><i class="fas fa-shield-alt"></i><span>ES Admin Cert</span></a></li>
                    <li class="sidebar-nav-item"><a href="cloud-admin-certification.html" class="sidebar-nav-link"><i class="fas fa-cloud"></i><span>Cloud Admin Cert</span></a></li>
                    <li class="sidebar-nav-item"><a href="lab-exercises.html" class="sidebar-nav-link"><i class="fas fa-flask"></i><span>Lab Exercises</span></a></li>
                </ul>
            </div>
        </div>
    </aside>
    
    <div class="sidebar-overlay" id="sidebarOverlay" onclick="toggleSidebar()"></div>

        
        <!-- Main Content Wrapper -->
        <div class="main-wrapper" id="mainWrapper">
            <button class="mobile-toggle" onclick="toggleSidebar()" title="Open menu">
                <i class="fas fa-bars"></i>
            </button>
            
            <main class="main-content">
                <div class="breadcrumb">
                    <a href="../index.html">Home</a>
                    <span class="separator">/</span>
                    <a href="index.html">Splunk</a>
                    <span class="separator">/</span>
                    <span class="current">Data Ingestion</span>
                </div>

                <h1><i class="fas fa-database" style="color: #65a637;"></i> Splunk Data Ingestion Complete Guide</h1>
                <p class="lead">100% comprehensive guide to ALL data ingestion methods: inputs, forwarders, Technology Add-ons, HEC, props.conf, transforms.conf, and CIM compliance.</p>

                <div style="background: rgba(245, 158, 11, 0.15); border-left: 4px solid #f59e0b; padding: 1rem; border-radius: 8px; margin: 1rem 0;">
                    <strong>⚠️ Important Updates (January 2025)</strong>
                    <ul style="margin: 0.5rem 0;">
                        <li><strong>Splunk Enterprise 9.3.x</strong> is current version - Platform version compatibility varies by TA</li>
                        <li><strong>Splunk_TA_windows v5.0+</strong> introduced BREAKING CHANGES - Read upgrade docs before deploying</li>
                        <li><strong>Palo Alto TA migrated to Splunk-supported</strong> - Old "Palo Alto Networks Add-on" deprecated, use new "Splunk Add-on for Palo Alto Networks"</li>
                        <li><strong>CIM version 6.x</strong> is current - Some older TAs may need updates for ES 8.x compatibility</li>
                        <li><strong>Always test TAs in non-production first</strong> before deploying to production indexers</li>
                    </ul>
                </div>

                <div class="config-section"><div class="arch-diagram">
SPLUNK DATA INGESTION ARCHITECTURE
═══════════════════════════════════════════════════════════════════════════════

COMPLETE DATA FLOW:
─────────────────────────────────────────────────────────────────────────────

                          ┌─────────────────────┐
                          │   LOG SOURCES       │
                          │                     │
                          │  • Windows Events   │
                          │  • Linux Syslog     │
                          │  • Network Devices  │
                          │  • Cloud Services   │
                          │  • Custom Apps      │
                          │  • APIs             │
                          └──────────┬──────────┘
                                     │
              ┌──────────────────────┼──────────────────────┐
              │                      │                      │
              ▼                      ▼                      ▼
     ┌─────────────────┐   ┌─────────────────┐   ┌─────────────────┐
     │   Universal     │   │     Heavy       │   │    HTTP Event   │
     │   Forwarder     │   │   Forwarder     │   │   Collector     │
     │     (UF)        │   │     (HF)        │   │    (HEC)        │
     │                 │   │                 │   │                 │
     │  • Lightweight  │   │  • Full parsing │   │  • REST API     │
     │  • No parsing   │   │  • Routing      │   │  • JSON/Raw     │
     │  • Agent-based  │   │  • Aggregation  │   │  • Token auth   │
     └────────┬────────┘   └────────┬────────┘   └────────┬────────┘
              │                      │                      │
              └──────────────────────┼──────────────────────┘
                                     │
                                     ▼
                    ┌─────────────────────────────────────┐
                    │           INDEXER CLUSTER           │
                    │                                     │
                    │  ┌─────────────────────────────┐   │
                    │  │     PARSING PIPELINE        │   │
                    │  │                             │   │
                    │  │  1. Input Phase             │   │
                    │  │     └── inputs.conf         │   │
                    │  │                             │   │
                    │  │  2. Parsing Phase           │   │
                    │  │     └── props.conf          │   │
                    │  │         (LINE_BREAKER,      │   │
                    │  │          TIME_FORMAT, etc.) │   │
                    │  │                             │   │
                    │  │  3. Transform Phase         │   │
                    │  │     └── transforms.conf     │   │
                    │  │         (Field extraction,  │   │
                    │  │          routing, masking)  │   │
                    │  │                             │   │
                    │  │  4. Index Phase             │   │
                    │  │     └── indexes.conf        │   │
                    │  │         (Storage location,  │   │
                    │  │          retention)         │   │
                    │  │                             │   │
                    │  └─────────────────────────────┘   │
                    │                                     │
                    │  Indexes:                          │
                    │  ├── main                          │
                    │  ├── security                      │
                    │  ├── wineventlog                   │
                    │  ├── firewall                      │
                    │  └── custom_index                  │
                    └─────────────────┬───────────────────┘
                                      │
                                      ▼
                    ┌─────────────────────────────────────┐
                    │          SEARCH HEAD CLUSTER        │
                    │                                     │
                    │  Search-time field extractions      │
                    │  props.conf, transforms.conf        │
                    │  CIM data models                    │
                    │  Knowledge objects                  │
                    └─────────────────────────────────────┘

INPUT METHODS - COMPLETE LIST:
═══════════════════════════════════════════════════════════════════════════════

  INPUT TYPE          CONFIG                           USE CASE
  ─────────────────────────────────────────────────────────────────────────────
  monitor://          [monitor:///path/to/file]        File/directory monitoring
  WinEventLog://      [WinEventLog://Security]         Windows Event Logs
  tcp://              [tcp://514]                      Syslog over TCP
  udp://              [udp://514]                      Syslog over UDP
  splunktcp://        [splunktcp://9997]               Receive from forwarders
  http://             HTTP Event Collector             REST API ingestion
  script://           [script://./bin/myscript.sh]    Custom scripts
  perfmon://          [perfmon://CPU]                  Windows Performance
  WMI://              [WMI://Win32_Process]            Windows WMI
  MonitorNoHandle://  [MonitorNoHandle://path]        Files without file handle
  fschange://         [fschange://path]               File system changes
  batch://            [batch://path]                   One-time file ingestion
  fifo://             [fifo://path]                    Named pipes (Unix)
  modular input      [mymodularinput://name]          Custom modular inputs
                </div></div>

                <!-- COMPREHENSIVE END-TO-END FLOWS SECTION -->
                <h2 id="e2e-flows"><i class="fas fa-route"></i> End-to-End Flows (Interview-Ready)</h2>
                
                <p>This section provides <strong>complete, senior-level explanations</strong> of how data flows from source to dashboard. Understanding these flows is critical for interviews and real-world troubleshooting.</p>

                <div class="config-section"><div class="arch-diagram">
═══════════════════════════════════════════════════════════════════════════════
                    FLOW 1: WINDOWS EVENT LOGS TO SPLUNK
                         (The Most Common Enterprise Scenario)
═══════════════════════════════════════════════════════════════════════════════

ARCHITECTURE OVERVIEW:
─────────────────────────────────────────────────────────────────────────────

  [Windows Servers]                  [Splunk Infrastructure]
        │                                    │
        │                           ┌────────┴────────┐
        ▼                           ▼                 ▼
  ┌─────────────────┐         ┌──────────┐     ┌──────────┐
  │ Universal       │  TCP    │ Indexer  │     │ Search   │
  │ Forwarder       │ ──────► │ Cluster  │     │ Head     │
  │ + TA-Windows    │  9997   │          │     │ Cluster  │
  └─────────────────┘         └──────────┘     └──────────┘
        │                           │                 │
        │                           │                 │
  Collects locally           Indexes &          Search-time
  + Forwards                 Stores             extractions +
                                                CIM mapping

═══════════════════════════════════════════════════════════════════════════════
STEP 1: INSTALL SPLUNK UNIVERSAL FORWARDER (ON WINDOWS)
═══════════════════════════════════════════════════════════════════════════════

WHAT IT DOES:
─────────────────────────────────────────────────────────────────────────────
• Collects data locally from the Windows host
• Forwards it securely to Splunk Indexers (TCP 9997, TLS encrypted)
• Has minimal resource overhead (~50MB RAM, &lt;1% CPU typical)
• Runs as a Windows Service (SplunkForwarder)

WHY IT'S MANDATORY:
─────────────────────────────────────────────────────────────────────────────
• Windows hosts cannot "push" Event Logs on their own
• The UF is the agent that extracts events from Windows Event Log API
• Without UF, you have NO data from Windows
• Alternative: Windows Event Forwarding (WEF) + Heavy Forwarder
  (More complex, used in very large environments)

INSTALLATION OPTIONS:
─────────────────────────────────────────────────────────────────────────────
# Option 1: Silent install with deployment server registration
msiexec.exe /i splunkforwarder-9.3.0-x64.msi AGREETOLICENSE=Yes ^
    DEPLOYMENT_SERVER="deploy.company.com:8089" ^
    LAUNCHSPLUNK=1 SERVICESTARTTYPE=auto /quiet

# Option 2: Manual install (development/testing)
msiexec.exe /i splunkforwarder-9.3.0-x64.msi AGREETOLICENSE=Yes ^
    RECEIVING_INDEXER="indexer1.company.com:9997" ^
    LAUNCHSPLUNK=1 /quiet

# Option 3: Group Policy / SCCM / Intune deployment
# Package MSI with deployment server configuration
# Use transforms file for consistent settings

POST-INSTALL VERIFICATION:
─────────────────────────────────────────────────────────────────────────────
# Check service is running
Get-Service SplunkForwarder

# Check connectivity to indexer
Test-NetConnection -ComputerName indexer1.company.com -Port 9997

# Check UF logs
Get-Content "C:\Program Files\SplunkUniversalForwarder\var\log\splunk\splunkd.log" -Tail 50

═══════════════════════════════════════════════════════════════════════════════
STEP 2: DEPLOY SPLUNK ADD-ON FOR MICROSOFT WINDOWS (TA-WINDOWS)
═══════════════════════════════════════════════════════════════════════════════

WHAT THE TA CONTAINS:
─────────────────────────────────────────────────────────────────────────────
┌─────────────────────────────────────────────────────────────────────────┐
│  Splunk_TA_windows/                                                     │
│  │                                                                      │
│  ├── default/                                                           │
│  │   ├── inputs.conf      ← Defines WHAT to collect                    │
│  │   │                       (WinEventLog, PerfMon, Registry, etc.)    │
│  │   │                                                                  │
│  │   ├── props.conf       ← Defines HOW to parse                       │
│  │   │                       (Timestamp, line breaking, KV extraction) │
│  │   │                                                                  │
│  │   ├── transforms.conf  ← Advanced field extractions                 │
│  │   │                                                                  │
│  │   ├── eventtypes.conf  ← Event categorization                       │
│  │   │                       (Groups of events for CIM)                │
│  │   │                                                                  │
│  │   └── tags.conf        ← CIM data model mapping                     │
│  │                           (authentication, change, endpoint, etc.)  │
│  │                                                                      │
│  └── local/               ← YOUR customizations go here               │
│      └── inputs.conf      ← Enable specific inputs                     │
└─────────────────────────────────────────────────────────────────────────┘

WHERE TO INSTALL THE TA (CRITICAL!):
─────────────────────────────────────────────────────────────────────────────
┌──────────────────────┬───────────────────────────────────────────────────┐
│ Component            │ Why?                                              │
├──────────────────────┼───────────────────────────────────────────────────┤
│ Universal Forwarder  │ inputs.conf - tells UF WHAT to collect           │
│                      │ (No parsing happens here - UF can't parse)       │
├──────────────────────┼───────────────────────────────────────────────────┤
│ Indexers             │ props.conf/transforms.conf for INDEX-TIME        │
│                      │ extractions (if any defined)                     │
│                      │ Most TAs do search-time, so often optional       │
├──────────────────────┼───────────────────────────────────────────────────┤
│ Search Heads         │ props.conf, transforms.conf, eventtypes.conf,    │
│                      │ tags.conf - ALL search-time parsing, CIM,        │
│                      │ field aliases, lookups                           │
│                      │ THIS IS WHERE CIM COMPLIANCE HAPPENS!            │
└──────────────────────┴───────────────────────────────────────────────────┘

COMMON MISTAKE TO AVOID:
─────────────────────────────────────────────────────────────────────────────
❌ Installing TA only on forwarders → Fields won't extract on Search Head
❌ Installing TA only on Search Heads → No data collected (no inputs!)
✅ Install on BOTH forwarders (for inputs) AND Search Heads (for parsing)

═══════════════════════════════════════════════════════════════════════════════
STEP 3: ENABLE THE REQUIRED INPUTS
═══════════════════════════════════════════════════════════════════════════════

DEFAULT BEHAVIOR - IMPORTANT!
─────────────────────────────────────────────────────────────────────────────
• TA-Windows ships with all inputs DISABLED by default
• This is intentional - you choose what to collect
• You MUST enable inputs in local/inputs.conf

WHICH INPUTS TO ENABLE (Security Monitoring Focus):
─────────────────────────────────────────────────────────────────────────────
┌───────────────────────────────────────────────────────────────────────────┐
│  FILE: Splunk_TA_windows/local/inputs.conf                               │
├───────────────────────────────────────────────────────────────────────────┤
│                                                                           │
│  # ═══════════════════════════════════════════════════════════════════   │
│  # SECURITY EVENT LOG - CRITICAL                                         │
│  # ═══════════════════════════════════════════════════════════════════   │
│  [WinEventLog://Security]                                                 │
│  disabled = 0                                                             │
│  index = wineventlog                                                      │
│  # whitelist = 4624,4625,4648,4672,4688,4720,4726,4768,4769,4771        │
│  # ^ Uncomment to reduce volume (collect only important EventIDs)        │
│                                                                           │
│  # ═══════════════════════════════════════════════════════════════════   │
│  # SYSTEM EVENT LOG                                                       │
│  # ═══════════════════════════════════════════════════════════════════   │
│  [WinEventLog://System]                                                   │
│  disabled = 0                                                             │
│  index = wineventlog                                                      │
│  # Good for: Service changes (7045), driver loads, boot events           │
│                                                                           │
│  # ═══════════════════════════════════════════════════════════════════   │
│  # POWERSHELL LOGGING - HIGH VALUE                                        │
│  # Requires GPO: Turn on PowerShell Script Block Logging                 │
│  # ═══════════════════════════════════════════════════════════════════   │
│  [WinEventLog://Microsoft-Windows-PowerShell/Operational]                 │
│  disabled = 0                                                             │
│  index = wineventlog                                                      │
│                                                                           │
│  # ═══════════════════════════════════════════════════════════════════   │
│  # SYSMON - IF DEPLOYED                                                   │
│  # Note: Sysmon must be installed separately on endpoints                │
│  # ═══════════════════════════════════════════════════════════════════   │
│  [WinEventLog://Microsoft-Windows-Sysmon/Operational]                     │
│  disabled = 0                                                             │
│  index = sysmon                                                           │
│  renderXml = true   # Preserves full XML for better field extraction     │
│                                                                           │
└───────────────────────────────────────────────────────────────────────────┘

HOW TO DEPLOY INPUTS (Production):
─────────────────────────────────────────────────────────────────────────────
Method 1: Deployment Server (RECOMMENDED)
  • Create serverclass for Windows servers
  • Push TA with local/inputs.conf to all UFs
  • Changes propagate automatically

Method 2: Configuration Management (Ansible/Puppet/SCCM)
  • Template the inputs.conf
  • Deploy as part of UF package
  • Version control the config

Method 3: Manual (Development Only)
  • Edit inputs.conf on each forwarder
  • Restart SplunkForwarder service
  • NEVER do this in production!

═══════════════════════════════════════════════════════════════════════════════
STEP 4: DATA IS AUTOMATICALLY CIM-MAPPED
═══════════════════════════════════════════════════════════════════════════════

WHY CIM WORKS "AUTOMATICALLY" WITH TA-WINDOWS:
─────────────────────────────────────────────────────────────────────────────
Because TA-Windows already includes:

1. STANDARD SOURCETYPES
   • WinEventLog:Security → Recognized by CIM
   • WinEventLog:System   → Recognized by CIM
   • etc.

2. FIELD ALIASES (in props.conf)
   • TargetUserName → user (CIM field)
   • IpAddress → src (CIM field)
   • WorkstationName → dest (CIM field)

3. EVENTTYPES (in eventtypes.conf)
   • Categorizes events (e.g., "wineventlog_security_4624" for logons)

4. TAGS (in tags.conf)
   • Maps eventtypes to data models:
     - tag: authentication → Authentication data model
     - tag: change → Change data model
     - tag: endpoint → Endpoint data model

RESULT:
─────────────────────────────────────────────────────────────────────────────
• Enterprise Security correlation searches WORK immediately
• ES dashboards populate with your Windows data
• No manual field mapping needed for standard Windows logs

VERIFICATION:
─────────────────────────────────────────────────────────────────────────────
# Check if data appears in Authentication data model
| tstats count from datamodel=Authentication 
    where nodename=Authentication 
    by Authentication.user, Authentication.action
| head 10

# If this returns data → CIM is working!

═══════════════════════════════════════════════════════════════════════════════
WHY THIS IS "EASY" COMPARED TO OTHER SIEMS
═══════════════════════════════════════════════════════════════════════════════

SPLUNK ADVANTAGES:
─────────────────────────────────────────────────────────────────────────────
┌─────────────────────────────────────────────────────────────────────────┐
│                                                                         │
│  1. PREBUILT TAs FOR MOST PLATFORMS                                    │
│     • Windows, Linux, Network devices, Cloud, SaaS                     │
│     • Community and Splunk-supported options                           │
│     • Download → Install → Enable inputs → Done                        │
│                                                                         │
│  2. CLEAR SEPARATION OF CONCERNS                                       │
│                                                                         │
│     ┌──────────────┐   ┌──────────────┐   ┌──────────────┐            │
│     │  COLLECTION  │   │   PARSING    │   │  ANALYTICS   │            │
│     │     (UF)     │ → │    (TA)      │ → │    (ES)      │            │
│     └──────────────┘   └──────────────┘   └──────────────┘            │
│                                                                         │
│     Each component has ONE job                                          │
│     Easy to troubleshoot, scale, maintain                              │
│                                                                         │
│  3. MINIMAL CUSTOM ENGINEERING FOR STANDARD SOURCES                    │
│     • Windows? Use TA-Windows                                           │
│     • Palo Alto? Use TA-Palo Alto                                       │
│     • AWS? Use TA-AWS                                                   │
│     • Field extractions included                                        │
│     • CIM mapping included                                              │
│                                                                         │
│  This is why Splunk is "log-first" and "operations-friendly"          │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘

═══════════════════════════════════════════════════════════════════════════════
IMPORTANT CAVEATS (SENIOR-LEVEL INTERVIEW POINTS)
═══════════════════════════════════════════════════════════════════════════════

CAVEAT 1: NOT EVERYTHING IS PRECONFIGURED
─────────────────────────────────────────────────────────────────────────────
TA-Windows covers:
  ✅ Native Windows Event Logs (Security, System, Application)
  ✅ Windows Performance Counters
  ✅ Windows Registry
  ✅ Active Directory
  ✅ DNS Server logs

TA-Windows does NOT cover:
  ❌ Custom applications writing to Event Log
  ❌ Proprietary application logs (flat files)
  ❌ Third-party software logs
  ❌ IIS logs (separate TA exists)

FOR CUSTOM LOGS, YOU NEED:
  • Custom sourcetype in props.conf
  • Custom field extractions
  • Custom CIM mapping (FIELDALIAS, eventtypes, tags)
  • This is where "Splunk Admin" expertise is required

CAVEAT 2: SYSMON IS NOT "AUTOMATIC"
─────────────────────────────────────────────────────────────────────────────
Common misconception: "TA-Windows collects Sysmon automatically"

REALITY:
  1. Sysmon must be INSTALLED SEPARATELY on each endpoint
     • Download from Microsoft Sysinternals
     • Deploy via GPO, SCCM, or manual
     
  2. Sysmon requires a CONFIGURATION FILE
     • SwiftOnSecurity config is a good starting point
     • Tune for your environment to avoid noise
     
  3. TA-Windows provides the INPUT and PARSING
     • [WinEventLog://Microsoft-Windows-Sysmon/Operational]
     • But Sysmon must exist on the endpoint first!

  4. High-volume consideration
     • Sysmon generates LOTS of events
     • Process creation, network connections, file writes
     • Plan indexing capacity accordingly

INTERVIEW ANSWER:
  "Sysmon requires three things: installing the binary on endpoints,
   deploying a configuration, and enabling the input in Splunk.
   TA-Windows handles the parsing, but you still need to deploy Sysmon."

CAVEAT 3: TA-WINDOWS v5.0+ BREAKING CHANGES
─────────────────────────────────────────────────────────────────────────────
If upgrading from pre-5.0:
  • Source vs Sourcetype changed for XML events
  • Old searches using sourcetype= may break
  • Must update to use source= for some event types

BEFORE v5.0:
  sourcetype="XmlWinEventLog:Microsoft-Windows-Sysmon/Operational"

AFTER v5.0:
  source="XmlWinEventLog:Microsoft-Windows-Sysmon/Operational"

INTERVIEW ANSWER:
  "When upgrading TA-Windows, you need to review the release notes
   because version 5.0 introduced breaking changes to how XML
   events are sourced. Existing searches and correlation rules
   may need updates."

CAVEAT 4: DEPLOYMENT BEST PRACTICES
─────────────────────────────────────────────────────────────────────────────
IN PRODUCTION:
  ✅ Use a Deployment Server
     • Centralized configuration management
     • Push changes to all forwarders
     • Version control what's deployed

  ✅ Use Server Classes
     • Group servers by type (Windows, Linux, Domain Controllers)
     • Apply different configurations per group

  ✅ Version-control TA changes
     • Git repository for all TAs
     • Code review before deployment
     • Rollback capability

  ❌ NEVER manually configure each forwarder
     • Doesn't scale
     • No audit trail
     • Inconsistent configurations

INTERVIEW ANSWER:
  "In production, I use the Deployment Server with server classes
   to push consistent configurations. All TA modifications are
   version-controlled in Git, and changes go through review
   before deployment."
                </div></div>

                <div class="config-section"><div class="arch-diagram">
═══════════════════════════════════════════════════════════════════════════════
                    FLOW 2: NETWORK DEVICE LOGS (SYSLOG)
                         Palo Alto / Cisco / Fortinet → Splunk
═══════════════════════════════════════════════════════════════════════════════

ARCHITECTURE OVERVIEW:
─────────────────────────────────────────────────────────────────────────────

  [Network Devices]                      [Splunk Infrastructure]
  ┌─────────────────┐                    
  │  Palo Alto FW   │                    ┌─────────────────────┐
  │  Cisco ASA      │──── Syslog ───────►│  Heavy Forwarder    │
  │  Fortinet       │     TCP/UDP        │  + Vendor TA        │
  │  Switches       │     514/5514       │                     │
  └─────────────────┘                    │  Receives syslog    │
                                         │  Parses to types    │
                                         │  Forwards to idx    │
                                         └─────────┬───────────┘
                                                   │ TCP 9997
                                                   ▼
                                         ┌─────────────────────┐
                                         │  Indexer Cluster    │
                                         └─────────────────────┘

WHY HEAVY FORWARDER (NOT UNIVERSAL FORWARDER)?
─────────────────────────────────────────────────────────────────────────────
• Network devices PUSH logs (syslog) - they don't have agents
• Someone needs to RECEIVE and PARSE these logs
• Heavy Forwarder can:
  - Listen on syslog port (514, 5514, etc.)
  - Run the vendor TA for parsing
  - Route to different indexes based on content
  - Filter/mask data if needed

COMPARISON:
┌──────────────────────┬─────────────────────────────────────────────────────┐
│ UF for Windows       │ UF is installed ON the Windows server              │
│                      │ UF pulls from Windows Event Log API                │
├──────────────────────┼─────────────────────────────────────────────────────┤
│ HF for Syslog        │ HF is a central collector                          │
│                      │ Devices PUSH syslog TO the HF                      │
│                      │ HF listens on a port                               │
└──────────────────────┴─────────────────────────────────────────────────────┘

═══════════════════════════════════════════════════════════════════════════════
STEP-BY-STEP: PALO ALTO → SPLUNK
═══════════════════════════════════════════════════════════════════════════════

STEP 1: PREPARE HEAVY FORWARDER
─────────────────────────────────────────────────────────────────────────────
# Install Splunk Enterprise (NOT Universal Forwarder)
rpm -i splunk-9.3.0-x64.rpm
/opt/splunk/bin/splunk start --accept-license

# Disable local indexing (it's a forwarder, not an indexer)
# This saves license and resources
/opt/splunk/bin/splunk disable local-index

# Configure outputs to indexers
cat > /opt/splunk/etc/system/local/outputs.conf << 'EOF'
[tcpout]
defaultGroup = indexer_cluster

[tcpout:indexer_cluster]
server = idx1.company.com:9997, idx2.company.com:9997
compressed = true
useACK = true
EOF

STEP 2: INSTALL PALO ALTO TA
─────────────────────────────────────────────────────────────────────────────
# Download from Splunkbase: "Splunk Add-on for Palo Alto Networks" (7523)
# NOT the old deprecated "Palo Alto Networks Add-on" (2757)

cd /opt/splunk/etc/apps
tar -xvf /tmp/Splunk_TA_paloalto.tgz

# TA provides:
# - props.conf: How to parse PAN-OS log format
# - transforms.conf: Field extractions
# - eventtypes.conf: Event categorization  
# - tags.conf: CIM mapping (Network_Traffic, Intrusion_Detection, etc.)

STEP 3: CONFIGURE SYSLOG INPUT
─────────────────────────────────────────────────────────────────────────────
# Create inputs.conf
cat > /opt/splunk/etc/apps/Splunk_TA_paloalto/local/inputs.conf << 'EOF'
# Main syslog input for Palo Alto
[tcp://5514]
disabled = 0
connection_host = dns
sourcetype = pan:log
index = firewall

# Optional: Separate port per log type
# [tcp://5515]
# sourcetype = pan:traffic
# index = firewall_traffic
EOF

STEP 4: CONFIGURE PALO ALTO FIREWALL
─────────────────────────────────────────────────────────────────────────────
In Palo Alto GUI:

1. Create Syslog Server Profile:
   Device → Server Profiles → Syslog
   ┌─────────────────────────────────────────────────────────────────────┐
   │ Name: Splunk_HF                                                     │
   │ Syslog Server: 10.0.0.50 (Heavy Forwarder IP)                       │
   │ Transport: TCP (recommended) or UDP                                 │
   │ Port: 5514                                                          │
   │ Format: BSD                                                         │
   │ Facility: LOG_USER                                                  │
   └─────────────────────────────────────────────────────────────────────┘

2. Create Log Forwarding Profile:
   Objects → Log Forwarding
   ┌─────────────────────────────────────────────────────────────────────┐
   │ Name: Forward_to_Splunk                                             │
   │ Traffic Log → Syslog: Splunk_HF                                     │
   │ Threat Log → Syslog: Splunk_HF                                      │
   │ URL Log → Syslog: Splunk_HF                                         │
   │ WildFire Log → Syslog: Splunk_HF                                    │
   └─────────────────────────────────────────────────────────────────────┘

3. Apply to Security Rules:
   Policies → Security → [each rule]
   Actions tab → Log Forwarding: Forward_to_Splunk

4. Commit changes

STEP 5: VERIFY DATA FLOW
─────────────────────────────────────────────────────────────────────────────
# On Search Head:

# Check data arriving
index=firewall earliest=-15m
| stats count by sourcetype

# Expected sourcetypes:
# pan:traffic  - Firewall session logs
# pan:threat   - IPS/AV detections
# pan:system   - System events
# pan:config   - Configuration changes

# Verify CIM compliance
| tstats count from datamodel=Network_Traffic 
    where nodename=All_Traffic.Traffic
    by All_Traffic.Traffic.action
| head 10

═══════════════════════════════════════════════════════════════════════════════
IMPORTANT CAVEATS FOR NETWORK DEVICE INGESTION
═══════════════════════════════════════════════════════════════════════════════

CAVEAT 1: SYSLOG IS UNRELIABLE (UDP)
─────────────────────────────────────────────────────────────────────────────
• UDP syslog can DROP messages under load
• TCP syslog is more reliable but requires more resources
• TLS syslog (port 6514) adds encryption but more complexity

RECOMMENDATION:
  Use TCP syslog for security-critical logs
  Use UDP only for high-volume, lower-priority logs

CAVEAT 2: TIMESTAMP PARSING ISSUES
─────────────────────────────────────────────────────────────────────────────
Common problem: Events indexed at wrong time

CAUSES:
  • Device timezone misconfigured
  • Syslog relay adds its own timestamp
  • Network latency

SOLUTION:
  • Configure devices to use UTC
  • Check TZ setting in props.conf
  • Use device timestamp, not syslog header timestamp

CAVEAT 3: HIGH VOLUME CONSIDERATIONS
─────────────────────────────────────────────────────────────────────────────
Firewall traffic logs can be MASSIVE:
  • 1 Gbps firewall = ~100GB/day in logs
  • 10 Gbps firewall = ~1TB/day possible

SOLUTIONS:
  • Filter on firewall (only log deny + specific allow)
  • Use Summary Indexing for aggregated data
  • Consider Splunk Edge Processor for filtering
  • Route high-volume logs to separate index with shorter retention

CAVEAT 4: LOAD BALANCING FOR ENTERPRISE SCALE
─────────────────────────────────────────────────────────────────────────────
For large environments (100+ firewalls):
  • Deploy multiple Heavy Forwarders
  • Use HAProxy or F5 for load balancing
  • Configure firewall to send to VIP

                      ┌───────────────┐
  [Firewalls] ───────►│ Load Balancer │
                      │   (VIP)       │
                      └───────┬───────┘
                              │
                ┌─────────────┼─────────────┐
                ▼             ▼             ▼
          ┌─────────┐   ┌─────────┐   ┌─────────┐
          │   HF1   │   │   HF2   │   │   HF3   │
          └─────────┘   └─────────┘   └─────────┘
                </div></div>

                <div class="config-section"><div class="arch-diagram">
═══════════════════════════════════════════════════════════════════════════════
                    FLOW 3: CLOUD/API-BASED SOURCES
                         CrowdStrike, AWS, Azure, O365 → Splunk
═══════════════════════════════════════════════════════════════════════════════

ARCHITECTURE OVERVIEW:
─────────────────────────────────────────────────────────────────────────────

                    [Cloud/SaaS Providers]
                    ┌────────────────────┐
                    │ CrowdStrike API    │
                    │ AWS CloudTrail     │
                    │ Azure Event Hub    │
                    │ O365 Management    │
                    └────────┬───────────┘
                             │ HTTPS
                             ▼
                    ┌────────────────────┐
                    │ Heavy Forwarder    │  ← Runs modular input
                    │ + Vendor TA        │  ← TA contains Python scripts
                    │                    │  ← Scripts poll API
                    └────────┬───────────┘
                             │ TCP 9997
                             ▼
                    ┌────────────────────┐
                    │ Indexer Cluster    │
                    └────────────────────┘

WHY HEAVY FORWARDER (NOT UNIVERSAL FORWARDER)?
─────────────────────────────────────────────────────────────────────────────
• API-based TAs contain Python code (modular inputs)
• UF cannot run Python scripts - it's too lightweight
• HF has full Python runtime and can execute modular inputs

WHAT HAPPENS:
  1. TA contains modular input script (Python)
  2. Script authenticates to cloud API (OAuth2, API key, etc.)
  3. Script polls for new events
  4. Events are ingested into Splunk with proper sourcetype
  5. TA's props/transforms parse the JSON

═══════════════════════════════════════════════════════════════════════════════
STEP-BY-STEP: CROWDSTRIKE → SPLUNK
═══════════════════════════════════════════════════════════════════════════════

STEP 1: CREATE API CREDENTIALS IN CROWDSTRIKE
─────────────────────────────────────────────────────────────────────────────
In Falcon Console:
  Support → API Clients and Keys → Add new API Client

  Name: Splunk_Integration
  Scopes (minimum required):
    ✅ Event Streams: Read
    ✅ Detections: Read
    ✅ Incidents: Read

  Save:
    Client ID: xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
    Client Secret: xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
    Cloud: US-1 (api.crowdstrike.com)

STEP 2: INSTALL TA ON HEAVY FORWARDER
─────────────────────────────────────────────────────────────────────────────
# Download from Splunkbase: 
# "CrowdStrike Falcon Event Streams Technical Add-On" (5082)

cd /opt/splunk/etc/apps
tar -xvf /tmp/TA-crowdstrike-falcon-event-streams.tgz

# Restart Splunk
/opt/splunk/bin/splunk restart

STEP 3: CONFIGURE VIA SPLUNK WEB
─────────────────────────────────────────────────────────────────────────────
On Heavy Forwarder: https://heavy-forwarder:8000

1. Apps → CrowdStrike Falcon Event Streams → Configuration

2. Account Tab:
   ┌─────────────────────────────────────────────────────────────────────┐
   │ Account Name: CrowdStrike_Prod                                      │
   │ Client ID: [your_client_id]                                         │
   │ Client Secret: [your_client_secret]                                 │
   │ CrowdStrike Cloud: US-1                                             │
   └─────────────────────────────────────────────────────────────────────┘

3. Inputs Tab → Create New Input:
   ┌─────────────────────────────────────────────────────────────────────┐
   │ Name: crowdstrike_events                                            │
   │ Account: CrowdStrike_Prod                                           │
   │ Index: crowdstrike                                                  │
   │ App ID: splunk_events_prod  ← MUST BE UNIQUE per environment!       │
   │ Event Types: DetectionSummaryEvent, IncidentSummaryEvent,           │
   │              AuthActivityAuditEvent                                 │
   └─────────────────────────────────────────────────────────────────────┘

STEP 4: VERIFY DATA FLOW
─────────────────────────────────────────────────────────────────────────────
# Check data arriving
index=crowdstrike earliest=-1h
| stats count by sourcetype

# Expected:
# crowdstrike:events:sensor

# Check specific detections
index=crowdstrike sourcetype="crowdstrike:events:sensor" 
    event_simpleName="DetectionSummaryEvent"
| table _time, ComputerName, UserName, DetectName, Severity

═══════════════════════════════════════════════════════════════════════════════
IMPORTANT CAVEATS FOR API-BASED INGESTION
═══════════════════════════════════════════════════════════════════════════════

CAVEAT 1: API RATE LIMITS
─────────────────────────────────────────────────────────────────────────────
• Cloud providers impose rate limits
• Too many requests = 429 Too Many Requests
• TA should handle backoff, but check logs

MONITORING:
  index=_internal sourcetype=splunkd source=*ta_crowdstrike*
  | search "429" OR "rate limit"

CAVEAT 2: CREDENTIAL SECURITY
─────────────────────────────────────────────────────────────────────────────
• API keys stored in Splunk credential storage
• Encrypted at rest, but Heavy Forwarder must be secure
• Consider: Hashicorp Vault integration for secrets

BEST PRACTICE:
  • Minimum required scopes only
  • Rotate credentials regularly
  • Audit API key usage in source system

CAVEAT 3: APP ID CONFLICTS
─────────────────────────────────────────────────────────────────────────────
CrowdStrike-specific issue:
  • Each Event Streams connection requires unique AppID
  • If two Splunk environments use same AppID → conflicts
  • One will work, one will get no data

SOLUTION:
  • Use environment-specific AppID: splunk_prod, splunk_dev
  • Document which AppID is used where

CAVEAT 4: NETWORK CONNECTIVITY
─────────────────────────────────────────────────────────────────────────────
Heavy Forwarder needs OUTBOUND HTTPS access:
  • api.crowdstrike.com (US-1)
  • api.us-2.crowdstrike.com (US-2)
  • api.eu-1.crowdstrike.com (EU-1)

If behind proxy:
  Configure proxy in TA settings or Splunk server.conf
                </div></div>

                <div class="config-section"><div class="arch-diagram">
═══════════════════════════════════════════════════════════════════════════════
                    FLOW 4: CUSTOM APPLICATION LOGS
                         When There's No Pre-Built TA
═══════════════════════════════════════════════════════════════════════════════

SCENARIO:
─────────────────────────────────────────────────────────────────────────────
Your company has a custom application that writes logs like:

  2024-01-15T10:30:45.123Z INFO  [OrderService] user=john.doe action=checkout 
      order_id=ORD-12345 amount=99.99 status=completed duration_ms=234
  
  2024-01-15T10:30:46.456Z ERROR [PaymentService] user=jane.smith action=payment 
      order_id=ORD-12346 amount=50.00 status=failed error="card_declined"

No TA exists for this. You need to build custom ingestion.

═══════════════════════════════════════════════════════════════════════════════
STEP 1: UNDERSTAND THE LOG FORMAT
═══════════════════════════════════════════════════════════════════════════════

Before writing any config, analyze the log:

QUESTIONS TO ANSWER:
  1. How are events separated? (newline, pattern)
  2. Where is the timestamp? What format?
  3. What fields exist? Key=value? JSON? Delimited?
  4. What CIM data model does this map to? (Web? Change? Custom?)

ANALYSIS OF OUR SAMPLE:
  • Events separated by: newline
  • Timestamp: ISO 8601 at start → %Y-%m-%dT%H:%M:%S.%3N%Z
  • Fields: Mixed format (level, service in brackets, key=value)
  • CIM mapping: Could be Web (for checkout) or Change

═══════════════════════════════════════════════════════════════════════════════
STEP 2: CREATE A TECHNOLOGY ADD-ON
═══════════════════════════════════════════════════════════════════════════════

# Create directory structure
mkdir -p /opt/splunk/etc/apps/TA_custom_orderapp/default
mkdir -p /opt/splunk/etc/apps/TA_custom_orderapp/local
mkdir -p /opt/splunk/etc/apps/TA_custom_orderapp/metadata

# Create app.conf
cat > /opt/splunk/etc/apps/TA_custom_orderapp/default/app.conf << 'EOF'
[install]
state = enabled

[launcher]
description = Technology Add-on for Custom Order Application
version = 1.0.0

[ui]
is_visible = false
label = TA-CustomOrderApp
EOF

═══════════════════════════════════════════════════════════════════════════════
STEP 3: CREATE props.conf FOR PARSING
═══════════════════════════════════════════════════════════════════════════════

cat > /opt/splunk/etc/apps/TA_custom_orderapp/default/props.conf << 'EOF'
# ═══════════════════════════════════════════════════════════════════════════
# SOURCETYPE DEFINITION
# ═══════════════════════════════════════════════════════════════════════════

[custom:orderapp]

# ─────────────────────────────────────────────────────────────────────────
# TIMESTAMP CONFIGURATION
# ─────────────────────────────────────────────────────────────────────────
TIME_FORMAT = %Y-%m-%dT%H:%M:%S.%3N%Z
TIME_PREFIX = ^
MAX_TIMESTAMP_LOOKAHEAD = 30
TZ = UTC

# ─────────────────────────────────────────────────────────────────────────
# EVENT BREAKING
# ─────────────────────────────────────────────────────────────────────────
SHOULD_LINEMERGE = false
LINE_BREAKER = ([\r\n]+)
TRUNCATE = 10000

# ─────────────────────────────────────────────────────────────────────────
# FIELD EXTRACTIONS
# ─────────────────────────────────────────────────────────────────────────
# Log level (INFO, ERROR, WARN)
EXTRACT-level = ^\S+\s+(?P<level>\w+)

# Service name in brackets
EXTRACT-service = \[(?P<service>\w+)\]

# Key=value pairs
EXTRACT-user = user=(?P<user>[^\s]+)
EXTRACT-action = action=(?P<action>[^\s]+)
EXTRACT-order_id = order_id=(?P<order_id>[^\s]+)
EXTRACT-amount = amount=(?P<amount>[\d.]+)
EXTRACT-status = status=(?P<status>[^\s]+)
EXTRACT-duration = duration_ms=(?P<duration_ms>\d+)
EXTRACT-error = error="(?P<error_message>[^"]+)"

# ─────────────────────────────────────────────────────────────────────────
# CIM FIELD MAPPING
# ─────────────────────────────────────────────────────────────────────────
# Map to CIM standard field names
FIELDALIAS-user_cim = user AS src_user
FIELDALIAS-service_cim = service AS app

# Computed fields for CIM
EVAL-vendor = "MyCompany"
EVAL-product = "OrderApp"
EVAL-vendor_product = "MyCompany OrderApp"

# Map status to CIM action field
EVAL-action_cim = case(
    status=="completed", "success",
    status=="failed", "failure",
    1==1, status
)
EOF

═══════════════════════════════════════════════════════════════════════════════
STEP 4: CREATE eventtypes.conf AND tags.conf FOR CIM
═══════════════════════════════════════════════════════════════════════════════

# eventtypes.conf - Categorize events
cat > /opt/splunk/etc/apps/TA_custom_orderapp/default/eventtypes.conf << 'EOF'
[orderapp_transaction]
search = sourcetype="custom:orderapp" action IN ("checkout", "payment", "refund")

[orderapp_error]
search = sourcetype="custom:orderapp" level="ERROR"
EOF

# tags.conf - Map to CIM data models
cat > /opt/splunk/etc/apps/TA_custom_orderapp/default/tags.conf << 'EOF'
[eventtype=orderapp_transaction]
web = enabled
application = enabled

[eventtype=orderapp_error]
alert = enabled
EOF

═══════════════════════════════════════════════════════════════════════════════
STEP 5: CREATE inputs.conf (ON FORWARDER)
═══════════════════════════════════════════════════════════════════════════════

cat > /opt/splunk/etc/apps/TA_custom_orderapp/local/inputs.conf << 'EOF'
[monitor:///var/log/orderapp/application.log]
disabled = 0
sourcetype = custom:orderapp
index = application

# Optional: Monitor rotated logs too
[monitor:///var/log/orderapp/application.log.*]
disabled = 0
sourcetype = custom:orderapp
index = application
EOF

═══════════════════════════════════════════════════════════════════════════════
STEP 6: DEPLOY AND VERIFY
═══════════════════════════════════════════════════════════════════════════════

DEPLOYMENT:
  1. Push TA to forwarders (for inputs.conf) via Deployment Server
  2. Push TA to Search Heads (for parsing/CIM) via Deployer
  3. Restart Splunk on all affected components

VERIFICATION:
  # Check data arriving with correct sourcetype
  index=application sourcetype="custom:orderapp" earliest=-1h
  | table _time, level, service, user, action, status
  
  # Verify field extractions
  index=application sourcetype="custom:orderapp" earliest=-1h
  | fieldsummary
  | where count > 0
  | table field, count, distinct_count
  
  # Verify CIM tags applied
  index=application sourcetype="custom:orderapp" earliest=-1h
  | head 1
  | eval mytags=mvjoin(tag, ", ")
  | table mytags
  # Should show: web, application (or your assigned tags)

═══════════════════════════════════════════════════════════════════════════════
INTERVIEW POINTS FOR CUSTOM LOG INGESTION
═══════════════════════════════════════════════════════════════════════════════

WHAT MAKES YOU "SENIOR-LEVEL":
─────────────────────────────────────────────────────────────────────────────
1. You understand that prebuilt TAs don't cover everything
2. You know how to analyze log format before writing config
3. You can create props.conf from scratch
4. You know CIM field names and how to map to them
5. You version-control your custom TAs
6. You test in dev before deploying to production

COMMON INTERVIEW QUESTION:
  "You have a new application that needs to be onboarded to Splunk.
   Walk me through your process."

GOOD ANSWER:
  "First, I analyze the log format - timestamp location, event delimiters,
   and field structure. Then I create a Technology Add-on with props.conf
   for parsing. I determine which CIM data model applies and create
   eventtypes and tags for CIM compliance. I test locally, then deploy
   via Deployment Server to forwarders and Deployer to Search Heads.
   The TA is version-controlled in Git."
                </div></div>

                <h2 id="forwarders"><i class="fas fa-share"></i> Forwarder Types</h2>

                <div class="config-section"><div class="arch-diagram">
FORWARDER COMPARISON
═══════════════════════════════════════════════════════════════════════════════

UNIVERSAL FORWARDER (UF):
─────────────────────────────────────────────────────────────────────────────

  ┌─────────────────────────────────────────────────────────────────────────┐
  │  CHARACTERISTICS:                                                       │
  │  • Lightweight (~50MB RAM)                                             │
  │  • No local parsing (sends raw data)                                   │
  │  • No search capability                                                │
  │  • Free - no license required                                          │
  │  • Deployed on endpoints/servers                                       │
  │                                                                         │
  │  USE CASES:                                                            │
  │  ✅ Windows servers collecting Event Logs                             │
  │  ✅ Linux servers collecting syslog/app logs                          │
  │  ✅ Any endpoint where lightweight agent needed                       │
  │                                                                         │
  │  LIMITATIONS:                                                          │
  │  ❌ Cannot perform complex parsing                                     │
  │  ❌ Cannot route based on content                                      │
  │  ❌ No search capability                                               │
  └─────────────────────────────────────────────────────────────────────────┘

  Installation (Windows):
  msiexec.exe /i splunkforwarder-9.x.x-x64.msi AGREETOLICENSE=Yes /quiet

  Installation (Linux):
  tar -xvf splunkforwarder-9.x.x-Linux-x86_64.tgz -C /opt
  /opt/splunkforwarder/bin/splunk start --accept-license

HEAVY FORWARDER (HF):
─────────────────────────────────────────────────────────────────────────────

  ┌─────────────────────────────────────────────────────────────────────────┐
  │  CHARACTERISTICS:                                                       │
  │  • Full Splunk instance (indexing disabled)                            │
  │  • Can parse, filter, route data                                       │
  │  • Requires Splunk license                                             │
  │  • Higher resource usage                                               │
  │                                                                         │
  │  USE CASES:                                                            │
  │  ✅ Syslog aggregation point                                          │
  │  ✅ Parse and route based on content                                  │
  │  ✅ Filter/mask sensitive data before indexing                        │
  │  ✅ Protocol conversion (e.g., syslog → Splunk)                       │
  │  ✅ Collect from APIs that need processing                            │
  │                                                                         │
  │  ARCHITECTURE:                                                         │
  │                                                                         │
  │  ┌───────────────┐     Syslog      ┌─────────────────┐                │
  │  │   Firewall    │ ──────────────► │ Heavy Forwarder │                │
  │  │   Router      │                 │                 │                │
  │  │   Switch      │                 │ • Parse logs    │                │
  │  └───────────────┘                 │ • Add metadata  │                │
  │                                    │ • Route to index│                │
  │                                    └────────┬────────┘                │
  │                                             │ TCP 9997                │
  │                                             ▼                         │
  │                                    ┌─────────────────┐                │
  │                                    │    Indexers     │                │
  │                                    └─────────────────┘                │
  └─────────────────────────────────────────────────────────────────────────┘

DEPLOYMENT SERVER:
─────────────────────────────────────────────────────────────────────────────

  ┌─────────────────────────────────────────────────────────────────────────┐
  │  PURPOSE: Centrally manage forwarder configurations                    │
  │                                                                         │
  │  ┌─────────────────────────────────────────────────────────────────┐   │
  │  │           Deployment Server                                      │   │
  │  │  /opt/splunk/etc/deployment-apps/                               │   │
  │  │  ├── TA-windows/                                                │   │
  │  │  │   └── local/inputs.conf                                      │   │
  │  │  ├── TA-linux/                                                  │   │
  │  │  │   └── local/inputs.conf                                      │   │
  │  │  └── outputs_to_indexers/                                       │   │
  │  │      └── local/outputs.conf                                     │   │
  │  └─────────────────────────────────────────────────────────────────┘   │
  │                              │                                          │
  │              ┌───────────────┼───────────────┐                         │
  │              ▼               ▼               ▼                         │
  │         ┌────────┐      ┌────────┐      ┌────────┐                    │
  │         │  UF 1  │      │  UF 2  │      │  UF 3  │                    │
  │         │Windows │      │Windows │      │ Linux  │                    │
  │         └────────┘      └────────┘      └────────┘                    │
  │                                                                         │
  │  serverclass.conf defines which apps go to which forwarders           │
  └─────────────────────────────────────────────────────────────────────────┘
                </div></div>

                <h2 id="inputs"><i class="fas fa-upload"></i> Input Configuration</h2>

                <div class="file-block">
                    <div class="header">inputs.conf - Windows Event Logs (Complete)</div>
<pre># ═══════════════════════════════════════════════════════════════════════════
# WINDOWS EVENT LOG COLLECTION - COMPREHENSIVE
# Location: $SPLUNK_HOME/etc/apps/myapp/local/inputs.conf
# ═══════════════════════════════════════════════════════════════════════════

# Security Events - Critical for SOC
[WinEventLog://Security]
disabled = 0
index = wineventlog
sourcetype = WinEventLog:Security
# Whitelist critical Event IDs to reduce volume
whitelist = 4624,4625,4634,4648,4672,4688,4689,4697,4698,4699,4700,4701,4702,4719,4720,4722,4723,4724,4725,4726,4727,4728,4729,4730,4731,4732,4733,4734,4735,4737,4738,4740,4741,4742,4743,4744,4745,4746,4747,4748,4749,4750,4751,4752,4753,4754,4755,4756,4757,4758,4759,4760,4761,4762,4763,4764,4767,4768,4769,4770,4771,4772,4773,4774,4775,4776,4777,4778,4779,4780,4781,4782,4783,4784,4785,4786,4787,4788,4789,4790,4791,4792,4793,4794,4795,4796,4797,4798,4799,4800,4801,4802,4803,5038,5140,5142,5143,5144,5145,5156,5157

# System Events
[WinEventLog://System]
disabled = 0
index = wineventlog
sourcetype = WinEventLog:System
# Service changes, driver loads, etc.
whitelist = 7034,7035,7036,7040,7045,1001,6005,6006

# Application Events
[WinEventLog://Application]
disabled = 0
index = wineventlog
sourcetype = WinEventLog:Application

# PowerShell Script Block Logging (CRITICAL for detection)
[WinEventLog://Microsoft-Windows-PowerShell/Operational]
disabled = 0
index = wineventlog
sourcetype = WinEventLog:PowerShell:Operational
whitelist = 4103,4104,4105,4106

# Sysmon (if installed) - GOLD STANDARD for endpoint telemetry
[WinEventLog://Microsoft-Windows-Sysmon/Operational]
disabled = 0
index = sysmon
sourcetype = XmlWinEventLog:Microsoft-Windows-Sysmon/Operational
renderXml = true

# Windows Defender
[WinEventLog://Microsoft-Windows-Windows Defender/Operational]
disabled = 0
index = wineventlog
sourcetype = WinEventLog:Microsoft-Windows-Windows Defender/Operational

# Task Scheduler (persistence detection)
[WinEventLog://Microsoft-Windows-TaskScheduler/Operational]
disabled = 0
index = wineventlog
sourcetype = WinEventLog:TaskScheduler
whitelist = 106,140,141,142,200,201

# WMI Activity (persistence, lateral movement)
[WinEventLog://Microsoft-Windows-WMI-Activity/Operational]
disabled = 0
index = wineventlog
sourcetype = WinEventLog:WMI

# DNS Client (DNS-based attacks)
[WinEventLog://Microsoft-Windows-DNS-Client/Operational]
disabled = 0
index = wineventlog
sourcetype = WinEventLog:DNS-Client

# Windows Firewall
[WinEventLog://Microsoft-Windows-Windows Firewall With Advanced Security/Firewall]
disabled = 0
index = wineventlog
sourcetype = WinEventLog:Firewall</pre>
                </div>

                <div class="file-block">
                    <div class="header">inputs.conf - Linux Syslog Collection</div>
<pre># ═══════════════════════════════════════════════════════════════════════════
# LINUX SYSLOG COLLECTION
# ═══════════════════════════════════════════════════════════════════════════

# Standard syslog files
[monitor:///var/log/syslog]
disabled = 0
index = linux
sourcetype = syslog

[monitor:///var/log/messages]
disabled = 0
index = linux
sourcetype = syslog

# Authentication logs (CRITICAL)
[monitor:///var/log/auth.log]
disabled = 0
index = linux
sourcetype = linux_secure

[monitor:///var/log/secure]
disabled = 0
index = linux
sourcetype = linux_secure

# Audit logs (if auditd installed)
[monitor:///var/log/audit/audit.log]
disabled = 0
index = linux
sourcetype = linux_audit

# Cron logs
[monitor:///var/log/cron]
disabled = 0
index = linux
sourcetype = cron

# Boot log
[monitor:///var/log/boot.log]
disabled = 0
index = linux
sourcetype = linux_boot

# Apache/Nginx logs
[monitor:///var/log/apache2/access.log]
disabled = 0
index = web
sourcetype = access_combined

[monitor:///var/log/nginx/access.log]
disabled = 0
index = web
sourcetype = nginx:access</pre>
                </div>

                <div class="file-block">
                    <div class="header">inputs.conf - Syslog Listener (Heavy Forwarder)</div>
<pre># ═══════════════════════════════════════════════════════════════════════════
# SYSLOG LISTENER - RECEIVE FROM NETWORK DEVICES
# Deploy on Heavy Forwarder
# ═══════════════════════════════════════════════════════════════════════════

# TCP Syslog (recommended - reliable)
[tcp://514]
disabled = 0
connection_host = dns
sourcetype = syslog
index = network

# UDP Syslog (faster but can lose events)
[udp://514]
disabled = 0
connection_host = dns
sourcetype = syslog
index = network

# TLS Encrypted Syslog (secure)
[tcp-ssl://6514]
disabled = 0
connection_host = dns
sourcetype = syslog
index = network

# Dedicated port for specific vendor (Palo Alto)
[tcp://5514]
disabled = 0
connection_host = dns
sourcetype = pan:traffic
index = firewall

# Dedicated port for Cisco ASA
[udp://5515]
disabled = 0
connection_host = dns
sourcetype = cisco:asa
index = firewall</pre>
                </div>

                <h2 id="hec"><i class="fas fa-code"></i> HTTP Event Collector (HEC)</h2>

                <div class="config-section"><div class="arch-diagram">
HTTP EVENT COLLECTOR - API-BASED INGESTION
═══════════════════════════════════════════════════════════════════════════════

ARCHITECTURE:
─────────────────────────────────────────────────────────────────────────────

  ┌─────────────────────────────────────────────────────────────────────────┐
  │                                                                         │
  │  Application/Script                                                     │
  │         │                                                               │
  │         │  POST https://splunk-hec.company.com:8088/services/collector │
  │         │  Headers:                                                     │
  │         │    Authorization: Splunk {HEC-TOKEN}                         │
  │         │    Content-Type: application/json                            │
  │         │                                                               │
  │         │  Body:                                                        │
  │         │  {                                                            │
  │         │    "time": 1704067200,                                       │
  │         │    "host": "webserver01",                                    │
  │         │    "source": "myapp",                                        │
  │         │    "sourcetype": "myapp:json",                               │
  │         │    "index": "application",                                   │
  │         │    "event": {                                                │
  │         │      "message": "User login successful",                     │
  │         │      "user": "john.doe",                                     │
  │         │      "ip": "10.0.0.100"                                      │
  │         │    }                                                          │
  │         │  }                                                            │
  │         │                                                               │
  │         ▼                                                               │
  │  ┌─────────────────────────────────────────┐                           │
  │  │  HTTP Event Collector (Port 8088)       │                           │
  │  │                                          │                           │
  │  │  • Token authentication                 │                           │
  │  │  • TLS encryption                       │                           │
  │  │  • Acknowledgement (optional)           │                           │
  │  └─────────────────────────────────────────┘                           │
  │         │                                                               │
  │         ▼                                                               │
  │  Indexer → Index: application                                          │
  │                                                                         │
  └─────────────────────────────────────────────────────────────────────────┘

HEC ENDPOINTS:
─────────────────────────────────────────────────────────────────────────────

  /services/collector/event        POST single or batch events (JSON)
  /services/collector/raw          POST raw text data
  /services/collector/event/1.0    Versioned endpoint
  /services/collector/ack          Check acknowledgement status
  /services/collector/health       Health check endpoint

HEC TOKEN CREATION (via CLI):
─────────────────────────────────────────────────────────────────────────────

  # Create HEC token
  ./splunk http-event-collector create my-token -uri https://localhost:8089 \
    -index main -sourcetype _json -auth admin:password

  # List tokens
  ./splunk http-event-collector list -uri https://localhost:8089 -auth admin:password

  # Enable/disable token
  ./splunk http-event-collector enable my-token -uri https://localhost:8089
  ./splunk http-event-collector disable my-token -uri https://localhost:8089
                </div>

                <h3>HEC Examples</h3>
                <div class="arch-diagram"><pre># ═══════════════════════════════════════════════════════════════════════════
# HEC - SEND EVENTS (Various Languages)
# ═══════════════════════════════════════════════════════════════════════════

# CURL - Single Event
curl -k https://splunk-hec.company.com:8088/services/collector/event \
  -H "Authorization: Splunk YOUR-HEC-TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"event": "Hello from curl!", "sourcetype": "manual"}'

# CURL - Batch Events
curl -k https://splunk-hec.company.com:8088/services/collector/event \
  -H "Authorization: Splunk YOUR-HEC-TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"event": {"action": "login", "user": "alice"}}
{"event": {"action": "logout", "user": "bob"}}'

# PYTHON
import requests
import json

HEC_URL = "https://splunk-hec.company.com:8088/services/collector/event"
HEC_TOKEN = "YOUR-HEC-TOKEN"

def send_to_splunk(events):
    headers = {
        "Authorization": f"Splunk {HEC_TOKEN}",
        "Content-Type": "application/json"
    }
    
    payload = "\n".join([json.dumps({"event": e}) for e in events])
    
    response = requests.post(HEC_URL, headers=headers, data=payload, verify=False)
    return response.json()

# Usage
events = [
    {"action": "login", "user": "alice", "ip": "10.0.0.1"},
    {"action": "file_access", "user": "alice", "file": "/etc/passwd"}
]
result = send_to_splunk(events)

# POWERSHELL
$HecUrl = "https://splunk-hec.company.com:8088/services/collector/event"
$HecToken = "YOUR-HEC-TOKEN"

$headers = @{
    "Authorization" = "Splunk $HecToken"
    "Content-Type" = "application/json"
}

$event = @{
    event = @{
        action = "login"
        user = "alice"
        ip = "10.0.0.1"
    }
    sourcetype = "myapp:json"
    index = "application"
} | ConvertTo-Json

Invoke-RestMethod -Uri $HecUrl -Method Post -Headers $headers -Body $event -SkipCertificateCheck</pre></div>

                <h2 id="tas"><i class="fas fa-puzzle-piece"></i> Technology Add-ons (TAs)</h2>

                <div class="ta-category">
                    <h3>What are Technology Add-ons?</h3>
                    <p>TAs provide pre-built parsing configurations, field extractions, and CIM compliance for specific log sources. They contain:</p>
                    <ul>
                        <li><strong>props.conf</strong> - Parsing rules, time format, line breaking</li>
                        <li><strong>transforms.conf</strong> - Field extractions, lookups, routing</li>
                        <li><strong>eventtypes.conf</strong> - Event categorization</li>
                        <li><strong>tags.conf</strong> - CIM tagging for data models</li>
                        <li><strong>inputs.conf</strong> - Sample input configurations (usually disabled by default)</li>
                    </ul>
                    <p><strong>Where to install TAs:</strong></p>
                    <ul>
                        <li><strong>Search Heads</strong> - For search-time field extractions, lookups, CIM compliance</li>
                        <li><strong>Indexers</strong> - For index-time extractions (if TA has INDEXED_EXTRACTIONS)</li>
                        <li><strong>Heavy Forwarders</strong> - For parsing before forwarding, modular inputs</li>
                        <li><strong>Universal Forwarders</strong> - Usually just inputs.conf (no parsing capability)</li>
                    </ul>
                </div>

                <h3>Essential Security TAs (Splunkbase) - Current Versions January 2025</h3>
                <table>
                    <thead><tr><th>Category</th><th>Add-on Name (Splunkbase ID)</th><th>Key Sourcetypes</th><th>Notes</th></tr></thead>
                    <tbody>
                        <tr><td><strong>Windows</strong></td><td><strong>Splunk Add-on for Microsoft Windows</strong> (742)<br><em>⚠️ v5.0+ has BREAKING CHANGES</em></td><td>WinEventLog:Security, WinEventLog:System, XmlWinEventLog:*, WinRegistry, WindowsUpdateLog, WinHostMon</td><td>Install on: SH, IDX, HF, UF. Version 5.0 changed source vs sourcetype - read migration guide!</td></tr>
                        <tr><td><strong>Sysmon</strong></td><td><strong>Splunk Add-on for Sysmon</strong> (5709)<br><em>Splunk-supported version</em></td><td>XmlWinEventLog:Microsoft-Windows-Sysmon/Operational (use source= not sourcetype= with TA-windows v5+)</td><td>Install on: SH, IDX. Maps to Endpoint, Network Traffic, Change data models. NOT same as community TA-microsoft-sysmon.</td></tr>
                        <tr><td><strong>Linux/Unix</strong></td><td><strong>Splunk Add-on for Unix and Linux</strong> (833)</td><td>syslog, linux_secure, linux_audit, linux_bootlog, Unix:ListeningPorts, Unix:Service, ps, top, df</td><td>Install on: SH, IDX, HF, UF. Includes scripted inputs for system inventory.</td></tr>
                        <tr><td><strong>Palo Alto</strong></td><td><strong>Splunk Add-on for Palo Alto Networks</strong> (7523)<br><em>NEW Splunk-supported version (Oct 2024)</em></td><td>pan:traffic, pan:threat, pan:system, pan:config, pan:globalprotect, pan:hipmatch, pan:userid, pan:decryption, pan:iot_security, pan:cortex_xdr</td><td>⚠️ Old "Palo Alto Networks Add-on" (2757) DEPRECATED. Migrate to new Splunk-supported version. Different CIM mappings!</td></tr>
                        <tr><td><strong>Fortinet</strong></td><td><strong>Fortinet FortiGate Add-on for Splunk</strong> (2846)</td><td>fgt_traffic, fgt_utm, fgt_event, fgt_log</td><td>Install on: SH, IDX, HF. Vendor-supported. Works with FortiAnalyzer syslog export.</td></tr>
                        <tr><td><strong>Cisco ASA</strong></td><td><strong>Splunk Add-on for Cisco ASA</strong> (1620)</td><td>cisco:asa</td><td>Install on: SH, IDX, HF. Parses ASA syslog (port 514 default).</td></tr>
                        <tr><td><strong>CrowdStrike</strong></td><td><strong>CrowdStrike Falcon Event Streams TA</strong> (5082)<br><em>v3.5.0 Oct 2024</em></td><td>crowdstrike:events:sensor, crowdstrike:events:incident</td><td>Install on: SH, HF. Requires CrowdStrike API credentials. Uses Event Streams API.</td></tr>
                        <tr><td><strong>CrowdStrike Devices</strong></td><td><strong>CrowdStrike Falcon Devices TA</strong> (5570)<br><em>v3.3.0 Nov 2024</em></td><td>crowdstrike:device:json</td><td>Pull device inventory from Falcon. Good for asset enrichment.</td></tr>
                        <tr><td><strong>AWS</strong></td><td><strong>Splunk Add-on for AWS</strong> (1876)</td><td>aws:cloudtrail, aws:cloudwatch, aws:cloudwatch:guardduty, aws:s3:accesslogs, aws:config:rule, aws:vpc:flow</td><td>Install on: SH, HF. Uses SQS-based S3 input. Requires IAM role/keys.</td></tr>
                        <tr><td><strong>Azure/Entra</strong></td><td><strong>Splunk Add-on for Microsoft Cloud Services</strong> (3110)</td><td>mscs:azure:eventhub, azure:aad:signin, azure:aad:audit, azure:monitor:aad, azure:activity</td><td>Install on: SH, HF. Uses Event Hub ingestion for Azure. Now covers Microsoft Entra ID (formerly Azure AD).</td></tr>
                        <tr><td><strong>Microsoft Security</strong></td><td><strong>Splunk Add-on for Microsoft Security</strong> (6207)</td><td>ms:defender:incident, ms:defender:alert</td><td>NEW: Collects from Microsoft 365 Defender. Replaces deprecated "Microsoft 365 Defender Add-on".</td></tr>
                        <tr><td><strong>O365</strong></td><td><strong>Splunk Add-on for Microsoft Office 365</strong> (4055)</td><td>o365:management:activity</td><td>Install on: SH, HF. Uses Management Activity API. Covers Exchange, SharePoint, Teams, etc.</td></tr>
                        <tr><td><strong>Okta</strong></td><td><strong>Splunk Add-on for Okta</strong> (6049)</td><td>OktaIM2:log</td><td>Install on: SH, HF. Uses System Log API. Splunk-supported.</td></tr>
                        <tr><td><strong>Zscaler</strong></td><td><strong>Zscaler App for Splunk</strong> (3866)</td><td>zscalernss-web, zscalernss-fw, zscalernss-dns</td><td>Uses Nanolog Streaming Service (NSS). Vendor-supported.</td></tr>
                        <tr><td><strong>CIM</strong></td><td><strong>Splunk Common Information Model</strong> (1621)<br><em>CIM 6.x for ES 8.x</em></td><td>N/A (provides data models)</td><td>Install on: SH only. Required for Enterprise Security. Contains Authentication, Network Traffic, Endpoint, Web, Email, etc. data models.</td></tr>
                        <tr><td><strong>ES Content Update</strong></td><td><strong>Splunk ES Content Update (ESCU)</strong> (3449)</td><td>N/A (provides detection rules)</td><td>Pre-packaged detections mapped to MITRE ATT&CK. Use with Enterprise Security.</td></tr>
                    </tbody>
                </table>

                <div class="info-box" style="background: rgba(59, 130, 246, 0.1); border-left: 4px solid #3b82f6; padding: 1rem; margin: 1rem 0;">
                    <strong>💡 Pro Tips for TAs:</strong>
                    <ul style="margin: 0.5rem 0;">
                        <li>Always check Splunkbase for the latest version before deploying</li>
                        <li>Read the "Upgrade" section in documentation - many TAs have breaking changes between major versions</li>
                        <li>Test in dev/staging environment before pushing to production</li>
                        <li>For clustered environments, deploy TAs via Cluster Master (now called Manager) to indexers, via Deployer to search heads</li>
                        <li>Use Deployment Server to push TAs and inputs.conf to Universal Forwarders</li>
                    </ul>
                </div>

                <h3>TA Installation - Step by Step</h3>
                <div class="arch-diagram"><pre># ═══════════════════════════════════════════════════════════════════════════
# TA INSTALLATION WORKFLOW
# ═══════════════════════════════════════════════════════════════════════════

# OPTION 1: Splunk Web (Single Server or Search Head)
# 1. Apps → Find More Apps → Search for TA name
# 2. Click Install → Enter splunk.com credentials
# 3. Restart Splunk if prompted

# OPTION 2: Command Line
cd /opt/splunk/etc/apps
tar -xvf /path/to/downloaded-ta.tgz
/opt/splunk/bin/splunk restart

# OPTION 3: Deployment Server (for Universal Forwarders)
# 1. Extract TA to deployment-apps directory
cd /opt/splunk/etc/deployment-apps
tar -xvf /path/to/Splunk_TA_windows.tgz

# 2. Create/edit serverclass.conf
cat >> /opt/splunk/etc/system/local/serverclass.conf << 'EOF'
[serverClass:Windows_Servers]
whitelist.0 = *.yourdomain.com
whitelist.1 = 10.0.0.*

[serverClass:Windows_Servers:app:Splunk_TA_windows]
restartSplunkWeb = 0
restartSplunkd = 1
stateOnClient = enabled
EOF

# 3. Reload deployment server
/opt/splunk/bin/splunk reload deploy-server

# OPTION 4: Cluster Master/Manager (for Indexer Cluster)
# Extract to master-apps directory
cd /opt/splunk/etc/master-apps
tar -xvf /path/to/Splunk_TA_windows.tgz

# Apply bundle to peers
/opt/splunk/bin/splunk apply cluster-bundle

# OPTION 5: Deployer (for Search Head Cluster)
cd /opt/splunk/etc/shcluster/apps
tar -xvf /path/to/Splunk_TA_windows.tgz
/opt/splunk/bin/splunk apply shcluster-bundle -target https://sh1:8089 -auth admin:password</pre></div>

                <h3>Deep Dive: What's Inside a Technology Add-on</h3>
                <p>Understanding TA internals is crucial for troubleshooting and customization. Here's exactly what you'll find:</p>
                
                <div class="config-section"><div class="arch-diagram">
TA DIRECTORY STRUCTURE - COMPLETE BREAKDOWN
═══════════════════════════════════════════════════════════════════════════════

$SPLUNK_HOME/etc/apps/Splunk_TA_windows/
│
├── default/                      # Vendor-provided configs (DO NOT EDIT)
│   ├── app.conf                  # App metadata, version info
│   ├── inputs.conf               # Sample inputs (usually disabled)
│   ├── props.conf                # Parsing rules, field extractions
│   ├── transforms.conf           # Advanced extractions, lookups
│   ├── eventtypes.conf           # Event categorization
│   ├── tags.conf                 # CIM tagging
│   └── data/
│       └── ui/
│           └── views/            # Any included dashboards
│
├── local/                        # YOUR customizations go here
│   ├── inputs.conf               # Enable/customize inputs
│   └── props.conf                # Override extractions if needed
│
├── lookups/                      # CSV lookup files
│   ├── action_lookup.csv         # Maps raw values to CIM values
│   ├── severity_lookup.csv       
│   └── vendor_product_lookup.csv
│
├── metadata/
│   ├── default.meta              # Permissions for default/
│   └── local.meta                # Permissions for local/
│
├── bin/                          # Scripts for modular inputs
│   └── (Python scripts for API-based collection)
│
├── README/                       # Documentation
│   └── inputs.conf.spec          # All available input options
│
└── static/                       # Icons, images
    └── appIcon.png

KEY RULE: Never edit files in default/ - always create/edit in local/
          Splunk merges local/ over default/ at runtime
                </div>

                <h3>Real Example: Examining Splunk_TA_windows</h3>
                <div class="arch-diagram"><pre># ═══════════════════════════════════════════════════════════════════════════
# HANDS-ON: Explore the Windows TA
# ═══════════════════════════════════════════════════════════════════════════

# List all files in the TA
ls -la /opt/splunk/etc/apps/Splunk_TA_windows/

# View the default inputs (these are typically disabled)
cat /opt/splunk/etc/apps/Splunk_TA_windows/default/inputs.conf

# OUTPUT SHOWS SOMETHING LIKE:
# [WinEventLog://Application]
# disabled = 1
# 
# [WinEventLog://Security]  
# disabled = 1
# ...

# View parsing rules (props.conf)
cat /opt/splunk/etc/apps/Splunk_TA_windows/default/props.conf | head -100

# OUTPUT SHOWS:
# [WinEventLog:Security]
# KV_MODE = xml
# SHOULD_LINEMERGE = false
# TIME_FORMAT = %Y-%m-%dT%H:%M:%S
# ...

# View field aliases for CIM compliance
grep "FIELDALIAS" /opt/splunk/etc/apps/Splunk_TA_windows/default/props.conf

# OUTPUT SHOWS:
# FIELDALIAS-user = TargetUserName AS user
# FIELDALIAS-src = IpAddress AS src
# FIELDALIAS-dest = WorkstationName AS dest
# ...</pre></div>

                <h3>Hands-On: Configure Windows TA for Security Monitoring</h3>
                <div class="config-section"><div class="arch-diagram">
SCENARIO: Configure Splunk_TA_windows to collect security-relevant events only
═══════════════════════════════════════════════════════════════════════════════

GOAL: Collect only important security events to reduce volume while maintaining
      visibility for SOC operations.

STEP 1: Create local inputs.conf
────────────────────────────────────────────────────────────────────────────────

  Location: /opt/splunk/etc/apps/Splunk_TA_windows/local/inputs.conf

  # ═══════════════════════════════════════════════════════════════════════════
  # WINDOWS SECURITY EVENTS - OPTIMIZED FOR SOC
  # ═══════════════════════════════════════════════════════════════════════════

  [WinEventLog://Security]
  disabled = 0
  index = wineventlog
  # Only collect security-relevant events (reduces volume by 80%+)
  whitelist = 4624,4625,4626,4627,4634,4647,4648,4649,4672,4673,4674,4675,\
              4688,4689,4697,4698,4699,4700,4701,4702,4703,4704,4705,4706,\
              4707,4713,4714,4715,4716,4717,4718,4719,4720,4722,4723,4724,\
              4725,4726,4727,4728,4729,4730,4731,4732,4733,4734,4735,4737,\
              4738,4739,4740,4741,4742,4743,4744,4745,4746,4747,4748,4749,\
              4750,4751,4752,4753,4754,4755,4756,4757,4758,4759,4760,4761,\
              4762,4763,4764,4765,4766,4767,4768,4769,4770,4771,4772,4773,\
              4774,4776,4777,4778,4779,4780,4781,4782,4793,4797,4798,4799,\
              4800,4801,4802,4803,4864,4865,4866,4867,4868,4869,4870,4871,\
              4872,4873,4874,4875,4876,4877,4878,4879,4880,4881,4882,4883,\
              4884,4885,4886,4887,4888,4889,4890,4891,4892,4893,4894,4895,\
              4896,4897,4898,4899,4900,4901,4902,4903,4904,4905,4906,4907,\
              4908,4909,4910,4911,4912,4913,4964,4985,5024,5025,5027,5028,\
              5029,5030,5031,5032,5033,5034,5035,5037,5038,5039,5040,5041,\
              5042,5043,5044,5045,5046,5047,5048,5049,5050,5051,5056,5057,\
              5058,5059,5060,5061,5136,5137,5138,5139,5140,5141,5142,5143,\
              5144,5145,5146,5147,5148,5149,5150,5151,5152,5153,5154,5155,\
              5156,5157,5158,5159

  # CRITICAL EVENTS TO NEVER MISS:
  # 4624 - Successful Logon
  # 4625 - Failed Logon
  # 4648 - Explicit Credential Logon (Pass-the-Hash indicator)
  # 4672 - Special Privileges Assigned (Admin logon)
  # 4688 - Process Creation (requires command line auditing)
  # 4720 - User Account Created
  # 4726 - User Account Deleted
  # 4728/4732/4756 - User Added to Security Group
  # 4768 - Kerberos TGT Requested
  # 4769 - Kerberos Service Ticket Requested
  # 4771 - Kerberos Pre-Auth Failed

STEP 2: Enable PowerShell Logging
────────────────────────────────────────────────────────────────────────────────

  # PowerShell Script Block Logging (requires GPO to enable on endpoints)
  [WinEventLog://Microsoft-Windows-PowerShell/Operational]
  disabled = 0
  index = wineventlog
  whitelist = 4103,4104,4105,4106
  # 4103 - Module Logging
  # 4104 - Script Block Logging (CRITICAL - shows full scripts)
  # 4105 - Script Block Start
  # 4106 - Script Block Stop

STEP 3: Enable Sysmon Collection (if Sysmon is deployed)
────────────────────────────────────────────────────────────────────────────────

  [WinEventLog://Microsoft-Windows-Sysmon/Operational]
  disabled = 0
  index = sysmon
  renderXml = true
  # renderXml=true preserves full XML for better field extraction
  
  # IMPORTANT for TA-windows v5+:
  # Use source= not sourcetype= in searches:
  # source="XmlWinEventLog:Microsoft-Windows-Sysmon/Operational"

STEP 4: Verify Configuration
────────────────────────────────────────────────────────────────────────────────

  # On the forwarder, check effective config
  /opt/splunk/bin/splunk btool inputs list --debug | grep -A5 "WinEventLog"
  
  # Check for config errors
  /opt/splunk/bin/splunk btool check
  
  # Restart forwarder to apply
  /opt/splunk/bin/splunk restart
                </div></div>

                <h3>Hands-On: Configure CrowdStrike Falcon TA</h3>
                <div class="config-section"><div class="arch-diagram">
SCENARIO: Integrate CrowdStrike Falcon with Splunk
═══════════════════════════════════════════════════════════════════════════════

ARCHITECTURE:

  [CrowdStrike Cloud]                    [Splunk]
        │                                    │
        │  Event Streams API                 │
        │  (OAuth2 authenticated)            │
        ▼                                    │
  ┌─────────────────┐                        │
  │ Heavy Forwarder │ ───TCP 9997──────────► │
  │ + Event Streams │                        │
  │   TA installed  │                        │
  └─────────────────┘                        │

PREREQUISITES:
  1. CrowdStrike Falcon subscription with API access
  2. API Client credentials (Client ID + Secret)
  3. Splunk Heavy Forwarder (NOT Universal Forwarder - TA needs Python)

STEP 1: Create CrowdStrike API Client
────────────────────────────────────────────────────────────────────────────────

  # In Falcon Console:
  # Support → API Clients and Keys → Add new API Client
  #
  # Name: Splunk_Integration
  # Scopes (minimum required):
  #   - Event Streams: Read
  #   - Detections: Read  
  #   - Incidents: Read
  #
  # Save the Client ID and Client Secret - you'll need these!
  #
  # Note your CrowdStrike cloud:
  #   US-1: api.crowdstrike.com
  #   US-2: api.us-2.crowdstrike.com
  #   EU-1: api.eu-1.crowdstrike.com
  #   US-GOV: api.laggar.gcw.crowdstrike.com

STEP 2: Install TA on Heavy Forwarder
────────────────────────────────────────────────────────────────────────────────

  # Download from Splunkbase:
  # "CrowdStrike Falcon Event Streams Technical Add-On" (App ID: 5082)
  
  cd /opt/splunk/etc/apps
  tar -xvf /tmp/TA-crowdstrike-falcon-event-streams.tgz
  
  # Also install on Search Heads for field extractions
  # (copy the same tgz to search heads)

STEP 3: Configure via Splunk Web
────────────────────────────────────────────────────────────────────────────────

  # On Heavy Forwarder, access Splunk Web:
  # https://heavy-forwarder:8000
  
  # Navigate to:
  # Apps → CrowdStrike Falcon Event Streams → Configuration
  
  # Account Tab:
  #   Account Name: CrowdStrike_Prod
  #   Client ID: [your_client_id]
  #   Client Secret: [your_client_secret]
  #   CrowdStrike Cloud: US-1 (or your cloud)
  
  # Inputs Tab:
  #   Click "Create New Input"
  #   Name: crowdstrike_events
  #   Account: CrowdStrike_Prod
  #   Index: crowdstrike
  #   App ID: splunk_events_prod (unique per environment!)
  #   Event Types to Collect: DetectionSummaryEvent, IncidentSummaryEvent,
  #                           AuthActivityAuditEvent, UserActivityAuditEvent

STEP 4: Verify Data Flow
────────────────────────────────────────────────────────────────────────────────

  # On Search Head:
  
  # Check if data is arriving
  index=crowdstrike earliest=-1h
  | stats count by sourcetype
  
  # Expected sourcetypes:
  # crowdstrike:events:sensor
  # crowdstrike:events:incident
  
  # View recent detections
  index=crowdstrike sourcetype="crowdstrike:events:sensor" 
      event_simpleName="DetectionSummaryEvent"
  | table _time, ComputerName, UserName, DetectName, Severity, Tactic, Technique
  
  # Check CIM compliance (Endpoint data model)
  | tstats count from datamodel=Endpoint.Processes by Processes.dest

STEP 5: Troubleshooting
────────────────────────────────────────────────────────────────────────────────

  # Check TA internal logs
  index=_internal sourcetype=splunkd component=ExecProcessor 
      crowdstrike OR falcon
  | head 50
  
  # Check for API errors
  index=_internal source=*ta_crowdstrike* log_level=ERROR
  
  # Common issues:
  # 1. "401 Unauthorized" - Wrong Client ID/Secret or expired token
  # 2. "403 Forbidden" - Missing API scopes
  # 3. "No data" - AppID already in use by another input
  # 4. "Connection refused" - Firewall blocking api.crowdstrike.com
                </div></div>

                <h3>Hands-On: Configure Palo Alto TA (Syslog)</h3>
                <div class="config-section"><div class="arch-diagram">
SCENARIO: Collect Palo Alto Firewall logs via Syslog
═══════════════════════════════════════════════════════════════════════════════

IMPORTANT: As of 2024, migrate to the NEW Splunk-supported TA:
           "Splunk Add-on for Palo Alto Networks" (7523)
           NOT the deprecated "Palo Alto Networks Add-on" (2757)

ARCHITECTURE:

  [Palo Alto FW/Panorama]  ──Syslog TCP 514──►  [Heavy Forwarder]
                                                     │
                                     ┌───────────────┴───────────────┐
                                     │  Splunk_TA_paloalto          │
                                     │  - Receives syslog            │
                                     │  - Parses to pan:* sourcetypes│
                                     │  - Applies CIM mapping        │
                                     └───────────────┬───────────────┘
                                                     │
                                              TCP 9997
                                                     │
                                                     ▼
                                              [Indexers]

STEP 1: Install TA on Heavy Forwarder
────────────────────────────────────────────────────────────────────────────────

  # Download NEW Splunk-supported version from Splunkbase (App ID: 7523)
  cd /opt/splunk/etc/apps
  tar -xvf /tmp/Splunk_Add-on_for_Palo_Alto_Networks.tgz

STEP 2: Configure Syslog Input
────────────────────────────────────────────────────────────────────────────────

  # Create local inputs.conf
  mkdir -p /opt/splunk/etc/apps/Splunk_TA_paloalto/local
  
  cat > /opt/splunk/etc/apps/Splunk_TA_paloalto/local/inputs.conf << 'EOF'
  # Palo Alto syslog input
  [tcp://514]
  disabled = 0
  connection_host = dns
  sourcetype = pan:log
  index = firewall
  
  # Optional: Use dedicated port per log type for easier troubleshooting
  # [tcp://5514]
  # disabled = 0
  # sourcetype = pan:traffic
  # index = firewall
  # 
  # [tcp://5515]
  # disabled = 0
  # sourcetype = pan:threat
  # index = firewall
  EOF

STEP 3: Configure Palo Alto to Send Logs
────────────────────────────────────────────────────────────────────────────────

  # In Palo Alto GUI (Panorama or Firewall):
  
  # 1. Create Syslog Server Profile
  #    Device → Server Profiles → Syslog
  #    Name: Splunk_HF
  #    Servers:
  #      Name: splunk-hf-01
  #      Syslog Server: 10.0.0.50 (Heavy Forwarder IP)
  #      Transport: TCP
  #      Port: 514
  #      Format: BSD
  #      Facility: LOG_USER
  
  # 2. Create Log Forwarding Profile
  #    Objects → Log Forwarding
  #    Name: Forward_to_Splunk
  #    Log Type: Traffic
  #      - Syslog: Splunk_HF
  #    Log Type: Threat
  #      - Syslog: Splunk_HF
  #    Log Type: URL
  #      - Syslog: Splunk_HF
  #    Log Type: WildFire
  #      - Syslog: Splunk_HF
  
  # 3. Apply Log Forwarding to Security Rules
  #    Policies → Security
  #    Edit each rule → Actions tab → Log Forwarding: Forward_to_Splunk
  
  # 4. Commit changes
  #    Commit (top right)

STEP 4: Verify on Splunk
────────────────────────────────────────────────────────────────────────────────

  # Check raw data arriving
  index=firewall earliest=-15m
  | stats count by sourcetype
  
  # Expected sourcetypes:
  # pan:traffic  - Firewall traffic logs
  # pan:threat   - Threat/IPS logs
  # pan:system   - System events
  # pan:config   - Configuration changes
  # pan:url      - URL filtering logs
  # pan:wildfire - WildFire verdict logs
  
  # View traffic summary
  index=firewall sourcetype=pan:traffic earliest=-1h
  | stats count by action, app, rule
  | sort -count
  
  # View threat detections
  index=firewall sourcetype=pan:threat earliest=-24h
  | stats count by threatid, threat_name, severity
  | sort -count
  
  # Check CIM compliance
  | tstats count from datamodel=Network_Traffic 
      where nodename=All_Traffic by All_Traffic.action

SOURCETYPE TO LOG TYPE MAPPING:
────────────────────────────────────────────────────────────────────────────────

  Sourcetype        │ Palo Alto Log Type      │ Contains
  ──────────────────┼─────────────────────────┼────────────────────────────
  pan:traffic       │ Traffic                 │ Allow/deny, bytes, sessions
  pan:threat        │ Threat                  │ IPS, antivirus, anti-spyware
  pan:url           │ URL Filtering           │ URL categories, actions
  pan:wildfire      │ WildFire Submissions    │ File verdicts
  pan:data          │ Data Filtering          │ DLP matches
  pan:config        │ Configuration           │ Config changes
  pan:system        │ System                  │ System events, HA failover
  pan:globalprotect │ GlobalProtect           │ VPN connections
  pan:hipmatch      │ HIP Match               │ Host compliance
  pan:userid        │ User-ID                 │ User mapping events
  pan:decryption    │ Decryption              │ SSL decryption events
  pan:iot_security  │ IoT Security            │ Device identification
  pan:cortex_xdr    │ Cortex XDR              │ XDR alerts (via API)
                </div></div>

                <h3>TA Troubleshooting Guide</h3>
                <div class="config-section"><div class="arch-diagram">
TA TROUBLESHOOTING FLOWCHART
═══════════════════════════════════════════════════════════════════════════════

PROBLEM: TA not working / No data / Wrong field extractions

START HERE
    │
    ▼
┌─────────────────────────────────────────────────────────────────────────────┐
│ STEP 1: Is the TA installed correctly?                                      │
│                                                                             │
│   # List installed apps                                                     │
│   /opt/splunk/bin/splunk display app                                        │
│                                                                             │
│   # Check if TA directory exists                                            │
│   ls -la /opt/splunk/etc/apps/ | grep -i "ta\|add-on"                       │
│                                                                             │
│   # Verify app is enabled                                                   │
│   cat /opt/splunk/etc/apps/Splunk_TA_windows/local/app.conf                 │
│   # Should NOT have: state = disabled                                       │
└───────────────────────────────────────────────────┬─────────────────────────┘
                                                    │
                                                    ▼
┌─────────────────────────────────────────────────────────────────────────────┐
│ STEP 2: Is the input enabled?                                               │
│                                                                             │
│   # Check effective input configuration                                      │
│   /opt/splunk/bin/splunk btool inputs list --debug | grep -A10 "WinEventLog"│
│                                                                             │
│   # Look for: disabled = 0 (means enabled)                                  │
│   # Look for: disabled = 1 (means disabled - won't collect!)                │
│                                                                             │
│   # Check which file is setting the value (--debug shows file path)         │
│   # Files are merged in order: default/ then local/                         │
└───────────────────────────────────────────────────┬─────────────────────────┘
                                                    │
                                                    ▼
┌─────────────────────────────────────────────────────────────────────────────┐
│ STEP 3: Are there configuration errors?                                     │
│                                                                             │
│   # Run btool check for syntax errors                                       │
│   /opt/splunk/bin/splunk btool check                                        │
│                                                                             │
│   # Check splunkd.log for errors                                            │
│   tail -100 /opt/splunk/var/log/splunk/splunkd.log | grep -i error          │
│                                                                             │
│   # Check for props.conf issues                                             │
│   /opt/splunk/bin/splunk btool props list --debug | grep -i "your_source"   │
└───────────────────────────────────────────────────┬─────────────────────────┘
                                                    │
                                                    ▼
┌─────────────────────────────────────────────────────────────────────────────┐
│ STEP 4: Is data being indexed?                                              │
│                                                                             │
│   # Check _internal for data flow                                           │
│   index=_internal source=*metrics.log group=per_sourcetype_thruput          │
│   | stats sum(kb) by series                                                 │
│                                                                             │
│   # Check if events exist (any index, any time)                             │
│   index=* sourcetype="WinEventLog:Security" earliest=-1h                    │
│   | stats count                                                             │
│                                                                             │
│   # If no data: check forwarder connectivity                                │
│   index=_internal sourcetype=splunkd group=tcpin_connections                │
│   | stats latest(connectionType) by hostname                                │
└───────────────────────────────────────────────────┬─────────────────────────┘
                                                    │
                                                    ▼
┌─────────────────────────────────────────────────────────────────────────────┐
│ STEP 5: Are field extractions working?                                      │
│                                                                             │
│   # Check if expected fields exist                                          │
│   index=wineventlog sourcetype="WinEventLog:Security" earliest=-1h          │
│   | fieldsummary                                                            │
│   | where count > 0                                                         │
│   | table field, count, distinct_count                                      │
│                                                                             │
│   # If fields missing, check props.conf is being applied                    │
│   /opt/splunk/bin/splunk btool props list WinEventLog:Security --debug      │
│                                                                             │
│   # Verify TA is installed on Search Heads (not just forwarders!)           │
│   # Field extractions happen at search time on Search Heads                 │
└───────────────────────────────────────────────────┬─────────────────────────┘
                                                    │
                                                    ▼
┌─────────────────────────────────────────────────────────────────────────────┐
│ STEP 6: Is CIM mapping working?                                             │
│                                                                             │
│   # Check if data model acceleration is enabled                             │
│   | rest /services/datamodel/config                                         │
│   | search name="Authentication"                                            │
│                                                                             │
│   # Check if data appears in data model                                     │
│   | tstats count from datamodel=Authentication by Authentication.action     │
│                                                                             │
│   # If no data in datamodel, check tags                                     │
│   index=wineventlog sourcetype="WinEventLog:Security" EventCode=4624        │
│   | head 1                                                                  │
│   | eval mytags=mvjoin(tag, ", ")                                           │
│   | table mytags                                                            │
│   # Should show: authentication, success (or similar CIM tags)              │
└─────────────────────────────────────────────────────────────────────────────┘

COMMON TA ISSUES AND FIXES:
═══════════════════════════════════════════════════════════════════════════════

Issue: "Fields not extracting"
────────────────────────────────────────
Cause: TA not installed on Search Heads
Fix:   Install TA on all Search Heads (field extractions are search-time)

Issue: "Wrong timestamp / events in future"
────────────────────────────────────────
Cause: TIME_FORMAT mismatch or wrong timezone
Fix:   Check props.conf TIME_FORMAT matches actual log format
       Check TZ setting in props.conf

Issue: "Data going to wrong index"
────────────────────────────────────────
Cause: inputs.conf missing index= setting
Fix:   Add explicit index= to inputs.conf in local/

Issue: "CrowdStrike/API TA not collecting"
────────────────────────────────────────
Cause: Wrong credentials, network blocked, or duplicate AppID
Fix:   Check _internal logs for TA, verify API credentials,
       ensure unique AppID per Splunk environment

Issue: "Splunk_TA_windows v5 broke my searches"
────────────────────────────────────────
Cause: v5 changed source vs sourcetype handling
Fix:   Change searches from sourcetype= to source= for XML events
       Read: docs.splunk.com/Documentation/WindowsAddOn/5.0.1/User/Upgrade

Issue: "props.conf changes not taking effect"
────────────────────────────────────────
Cause: Searching old events (props.conf only affects new events)
Fix:   Wait for new events, or test with: earliest=-5m
       For existing data, re-index or use SEDCMD at search time
                </div></div>

                <h2 id="parsing"><i class="fas fa-cogs"></i> Custom Parsing (props.conf & transforms.conf)</h2>

                <div class="config-section"><div class="arch-diagram">
PARSING CONFIGURATION FILES
═══════════════════════════════════════════════════════════════════════════════

props.conf - SOURCETYPE DEFINITION
─────────────────────────────────────────────────────────────────────────────

  ┌─────────────────────────────────────────────────────────────────────────┐
  │  [sourcetype_name]                                                      │
  │                                                                         │
  │  # EVENT BREAKING                                                       │
  │  LINE_BREAKER = ([\r\n]+)          # Where to break events             │
  │  SHOULD_LINEMERGE = false          # Don't merge lines                 │
  │  TRUNCATE = 10000                  # Max event size                    │
  │                                                                         │
  │  # TIMESTAMP                                                            │
  │  TIME_FORMAT = %Y-%m-%dT%H:%M:%S   # Timestamp format                  │
  │  TIME_PREFIX = timestamp=          # Text before timestamp             │
  │  MAX_TIMESTAMP_LOOKAHEAD = 30      # How far to look                   │
  │  TZ = UTC                          # Timezone                          │
  │                                                                         │
  │  # FIELD EXTRACTIONS                                                    │
  │  KV_MODE = auto                    # Key=value extraction mode         │
  │  EXTRACT-myfield = regex           # Inline regex extraction           │
  │  REPORT-fields = transform_name    # Use transforms.conf               │
  │                                                                         │
  │  # CATEGORIZATION                                                       │
  │  FIELDALIAS-user = src_user AS user  # Field aliases for CIM          │
  │  EVAL-action = if(status="200","allowed","blocked")  # Computed field │
  │                                                                         │
  └─────────────────────────────────────────────────────────────────────────┘

transforms.conf - ADVANCED TRANSFORMATIONS
─────────────────────────────────────────────────────────────────────────────

  ┌─────────────────────────────────────────────────────────────────────────┐
  │  [transform_name]                                                       │
  │                                                                         │
  │  # REGEX EXTRACTION                                                     │
  │  REGEX = user=(?&lt;user&gt;\S+)\s+action=(?&lt;action&gt;\S+)                     │
  │  FORMAT = user::$1 action::$2                                          │
  │                                                                         │
  │  # ROUTING (send to different index)                                   │
  │  REGEX = sourcetype=pan:threat                                         │
  │  DEST_KEY = _MetaData:Index                                            │
  │  FORMAT = security                                                      │
  │                                                                         │
  │  # MASKING SENSITIVE DATA                                              │
  │  REGEX = (password=)\S+                                                 │
  │  FORMAT = $1XXXXX                                                       │
  │  DEST_KEY = _raw                                                        │
  │                                                                         │
  │  # LOOKUP                                                               │
  │  filename = users.csv                                                   │
  │  match_type = WILDCARD(user)                                           │
  │                                                                         │
  └─────────────────────────────────────────────────────────────────────────┘
                </div></div>

                <h3>Custom Sourcetype Example</h3>
                <div class="file-block">
                    <div class="header">props.conf - Custom Application Logs</div>
<pre># ═══════════════════════════════════════════════════════════════════════════
# CUSTOM APPLICATION LOG PARSING
# ═══════════════════════════════════════════════════════════════════════════

[myapp:json]
# JSON logs - automatic field extraction
KV_MODE = json
TIME_FORMAT = %Y-%m-%dT%H:%M:%S.%3N%Z
TIME_PREFIX = "timestamp"\s*:\s*"
MAX_TIMESTAMP_LOOKAHEAD = 40
SHOULD_LINEMERGE = false
LINE_BREAKER = ([\r\n]+)

# CIM Field Aliases for Authentication data model
FIELDALIAS-user = username AS user
FIELDALIAS-src = client_ip AS src
FIELDALIAS-action = event_type AS action
EVAL-app = "MyApplication"

# Tag for CIM
LOOKUP-action = myapp_action_lookup action OUTPUT action_type

[myapp:syslog]
# Syslog format: 2024-01-15 10:30:00 INFO [user=john] [action=login] Message here
TIME_FORMAT = %Y-%m-%d %H:%M:%S
TIME_PREFIX = ^
MAX_TIMESTAMP_LOOKAHEAD = 20
SHOULD_LINEMERGE = false

# Field extractions
EXTRACT-level = ^[^[\n]*\s+(?P&lt;level&gt;\w+)\s+\[
EXTRACT-user = \[user=(?P&lt;user&gt;[^\]]+)\]
EXTRACT-action = \[action=(?P&lt;action&gt;[^\]]+)\]
EXTRACT-message = \]\s+(?P&lt;message&gt;.+)$

# CIM compliance
FIELDALIAS-src_user = user AS src_user
EVAL-vendor = "MyCompany"
EVAL-product = "MyApp"</pre>
                </div>

                <div class="file-block">
                    <div class="header">transforms.conf - Advanced Transformations</div>
<pre># ═══════════════════════════════════════════════════════════════════════════
# ADVANCED TRANSFORMATIONS
# ═══════════════════════════════════════════════════════════════════════════

# Route firewall logs to security index
[route_firewall_to_security]
REGEX = .
DEST_KEY = _MetaData:Index
FORMAT = security

# Mask credit card numbers
[mask_credit_card]
REGEX = (\d{4})[- ]?(\d{4})[- ]?(\d{4})[- ]?(\d{4})
FORMAT = XXXX-XXXX-XXXX-$4
DEST_KEY = _raw

# Mask passwords in logs
[mask_password]
REGEX = (password|passwd|pwd)\s*[=:]\s*\S+
FORMAT = $1=********
DEST_KEY = _raw

# Extract complex nested fields
[extract_json_nested]
REGEX = "user":\s*\{"name":\s*"(?P&lt;user_name&gt;[^"]+)",\s*"email":\s*"(?P&lt;user_email&gt;[^"]+)"\}

# Lookup for enrichment
[user_department_lookup]
filename = user_departments.csv
match_type = WILDCARD(user)

# Null queue (drop events)
[setnull]
REGEX = .
DEST_KEY = queue
FORMAT = nullQueue</pre>
                </div></div>

                <h2 id="cim"><i class="fas fa-layer-group"></i> CIM (Common Information Model)</h2>

                <div class="config-section"><div class="arch-diagram">
CIM DATA MODELS - NORMALIZE FOR ENTERPRISE SECURITY
═══════════════════════════════════════════════════════════════════════════════

PURPOSE:
─────────────────────────────────────────────────────────────────────────────

  Same event type from different sources → Normalized field names
  
  Windows Logon:          Linux auth.log:           Okta:
  ─────────────────       ─────────────────        ─────────────────
  TargetUserName: john    user: john               actor.alternateId: john
  IpAddress: 10.0.0.1     rhost: 10.0.0.1          client.ipAddress: 10.0.0.1
  LogonType: 10           -                         -
  
  After CIM normalization (Authentication data model):
  ─────────────────────────────────────────────────────────────────────────────
  user = john
  src = 10.0.0.1
  action = success
  app = Windows/Linux/Okta

KEY CIM DATA MODELS:
─────────────────────────────────────────────────────────────────────────────

  DATA MODEL              TYPICAL SOURCES               KEY FIELDS
  ─────────────────────────────────────────────────────────────────────────────
  Authentication          Windows Security, Linux auth, user, src, dest, action,
                          VPN, Okta, Azure AD          app, signature
  
  Network_Traffic         Firewall, Router, IDS/IPS,   src_ip, dest_ip, dest_port,
                          NetFlow                      action, bytes_in, bytes_out
  
  Endpoint                EDR, AV, Sysmon              dest, user, file_name,
                                                       file_path, signature
  
  Web                     Proxy, WAF, Web server       src, dest, url, http_method,
                                                       status, user_agent
  
  Email                   Email gateway, O365          src_user, recipient, subject,
                                                       file_name, action
  
  Malware                 AV, EDR, Sandbox             dest, file_name, file_hash,
                                                       signature, action
  
  Change                  Windows, Linux, Network      user, object, action,
                                                       status, object_category

MAKING DATA CIM COMPLIANT:
─────────────────────────────────────────────────────────────────────────────

  props.conf:
  ┌─────────────────────────────────────────────────────────────────────────┐
  │  [my:sourcetype]                                                        │
  │  # Field aliases - map source fields to CIM                            │
  │  FIELDALIAS-user = src_user AS user                                    │
  │  FIELDALIAS-src = source_ip AS src                                     │
  │  FIELDALIAS-action = result AS action                                  │
  │                                                                         │
  │  # Computed fields                                                      │
  │  EVAL-vendor = "MyVendor"                                              │
  │  EVAL-product = "MyProduct"                                            │
  │  EVAL-action = case(status="200","allowed",status="403","blocked")    │
  └─────────────────────────────────────────────────────────────────────────┘

  tags.conf:
  ┌─────────────────────────────────────────────────────────────────────────┐
  │  [eventtype=my_authentication_event]                                    │
  │  authentication = enabled                                               │
  │  default = enabled                                                      │
  └─────────────────────────────────────────────────────────────────────────┘

  eventtypes.conf:
  ┌─────────────────────────────────────────────────────────────────────────┐
  │  [my_authentication_event]                                              │
  │  search = sourcetype="my:sourcetype" action IN ("login","logout")      │
  └─────────────────────────────────────────────────────────────────────────┘
                </div></div>

                <h3>Complete CIM Compliance Example: Custom VPN Logs</h3>
                <div class="config-section"><div class="arch-diagram">
HANDS-ON: MAKE CUSTOM VPN LOGS CIM-COMPLIANT FOR ENTERPRISE SECURITY
═══════════════════════════════════════════════════════════════════════════════

SAMPLE VPN LOG FORMAT:

  2024-01-15T10:30:45Z vpn-gw-01 CONNECT user=john.doe@company.com client_ip=203.0.113.50 
      assigned_ip=10.100.0.25 tunnel=SSL-VPN duration=0 status=connected
  2024-01-15T11:45:32Z vpn-gw-01 AUTH_FAIL user=jane.smith@company.com client_ip=192.0.2.100 
      reason="invalid_password" attempts=3 status=failed
  2024-01-15T12:00:00Z vpn-gw-01 DISCONNECT user=john.doe@company.com client_ip=203.0.113.50 
      assigned_ip=10.100.0.25 tunnel=SSL-VPN duration=5355 bytes_sent=15234567 status=disconnected

STEP 1: Create props.conf for Parsing + CIM Mapping
────────────────────────────────────────────────────────────────────────────────

# File: $SPLUNK_HOME/etc/apps/TA_custom_vpn/local/props.conf

[custom:vpn]
# Timestamp extraction
TIME_FORMAT = %Y-%m-%dT%H:%M:%S%Z
TIME_PREFIX = ^
MAX_TIMESTAMP_LOOKAHEAD = 25
TZ = UTC

# Event breaking
SHOULD_LINEMERGE = false
LINE_BREAKER = ([\r\n]+)

# Field extractions
EXTRACT-host = ^\d{4}-\d{2}-\d{2}T[\d:]+Z\s+(?P<vpn_gateway>\S+)
EXTRACT-action_type = ^\S+\s+\S+\s+(?P<action_type>\S+)
EXTRACT-user = user=(?P<src_user>[^\s]+)
EXTRACT-client_ip = client_ip=(?P<client_ip>[\d.]+)
EXTRACT-assigned_ip = assigned_ip=(?P<assigned_ip>[\d.]+)
EXTRACT-tunnel = tunnel=(?P<tunnel_type>\S+)
EXTRACT-duration = duration=(?P<session_duration>\d+)
EXTRACT-bytes = bytes_sent=(?P<bytes_sent>\d+)
EXTRACT-status = status=(?P<status>\S+)
EXTRACT-reason = reason="(?P<failure_reason>[^"]+)"
EXTRACT-attempts = attempts=(?P<attempt_count>\d+)

# ═══════════════════════════════════════════════════════════════════════════
# CIM FIELD MAPPING - Authentication Data Model
# ═══════════════════════════════════════════════════════════════════════════

# Map to CIM standard field names
FIELDALIAS-user = src_user AS user
FIELDALIAS-src = client_ip AS src
FIELDALIAS-dest = vpn_gateway AS dest
FIELDALIAS-src_ip = client_ip AS src_ip
FIELDALIAS-dest_ip = assigned_ip AS dest_ip

# Computed fields for CIM compliance
EVAL-vendor = "CustomVendor"
EVAL-vendor_product = "CustomVendor VPN Gateway"
EVAL-product = "VPN Gateway"
EVAL-app = "vpn"

# Map status to CIM action field
EVAL-action = case(
    status=="connected", "success",
    status=="failed", "failure",
    status=="disconnected", "success",
    1==1, "unknown"
)

# Map to CIM signature fields
EVAL-signature = action_type
EVAL-signature_id = case(
    action_type=="CONNECT", "100",
    action_type=="DISCONNECT", "101",
    action_type=="AUTH_FAIL", "200",
    1==1, "0"
)

# Authentication type for CIM
EVAL-authentication_method = tunnel_type

STEP 2: Create eventtypes.conf for CIM Categorization
────────────────────────────────────────────────────────────────────────────────

# File: $SPLUNK_HOME/etc/apps/TA_custom_vpn/local/eventtypes.conf

[custom_vpn_authentication]
search = sourcetype="custom:vpn" (action_type="CONNECT" OR action_type="AUTH_FAIL")

[custom_vpn_session]
search = sourcetype="custom:vpn" (action_type="CONNECT" OR action_type="DISCONNECT")

[custom_vpn_failed_auth]
search = sourcetype="custom:vpn" action_type="AUTH_FAIL"

STEP 3: Create tags.conf for Data Model Assignment
────────────────────────────────────────────────────────────────────────────────

# File: $SPLUNK_HOME/etc/apps/TA_custom_vpn/local/tags.conf

# Tag for Authentication data model
[eventtype=custom_vpn_authentication]
authentication = enabled

# Tag for Network Session data model
[eventtype=custom_vpn_session]
network = enabled
session = enabled

# Tag failed authentications specifically
[eventtype=custom_vpn_failed_auth]
authentication = enabled
failure = enabled

STEP 4: Verify CIM Compliance
────────────────────────────────────────────────────────────────────────────────

# Search for data with CIM fields
index=vpn sourcetype="custom:vpn" earliest=-1h
| table _time, user, src, dest, action, app, vendor_product

# Verify tags are applied
index=vpn sourcetype="custom:vpn" earliest=-1h
| head 10
| eval mytags=mvjoin(tag, ", ")
| table _time, action_type, mytags

# Should show: authentication, network, session

# Verify data appears in Authentication data model
| tstats count from datamodel=Authentication 
    where Authentication.app="vpn"
    by Authentication.user Authentication.action

# If above returns data → CIM compliance achieved!

WHY THIS MATTERS FOR ENTERPRISE SECURITY:
────────────────────────────────────────────────────────────────────────────────

  Without CIM:
  ───────────────────────────────────────
  • Each data source uses different field names
  • ES correlation rules won't find your data
  • Dashboards won't populate
  • You miss security detections!

  With CIM:
  ───────────────────────────────────────
  • All auth sources use: user, src, dest, action
  • ES "Excessive Failed Logins" rule works across ALL sources
  • Identity dashboards include your VPN data
  • Correlation searches find your data automatically
                </div></div>

                <h3>CIM Quick Reference - Required Fields by Data Model</h3>
                <table>
                    <thead><tr><th>Data Model</th><th>Required Fields</th><th>Recommended Fields</th><th>Required Tags</th></tr></thead>
                    <tbody>
                        <tr>
                            <td><strong>Authentication</strong></td>
                            <td><code>action</code>, <code>app</code>, <code>user</code></td>
                            <td><code>src</code>, <code>dest</code>, <code>src_user</code>, <code>dest_user</code>, <code>signature</code></td>
                            <td><code>authentication</code></td>
                        </tr>
                        <tr>
                            <td><strong>Network_Traffic</strong></td>
                            <td><code>action</code>, <code>src_ip</code>, <code>dest_ip</code></td>
                            <td><code>src_port</code>, <code>dest_port</code>, <code>transport</code>, <code>bytes_in</code>, <code>bytes_out</code>, <code>app</code></td>
                            <td><code>network</code>, <code>communicate</code></td>
                        </tr>
                        <tr>
                            <td><strong>Endpoint.Processes</strong></td>
                            <td><code>dest</code>, <code>process_name</code></td>
                            <td><code>user</code>, <code>process_id</code>, <code>parent_process</code>, <code>process_path</code>, <code>process_hash</code></td>
                            <td><code>process</code>, <code>report</code></td>
                        </tr>
                        <tr>
                            <td><strong>Endpoint.Filesystem</strong></td>
                            <td><code>dest</code>, <code>file_name</code></td>
                            <td><code>file_path</code>, <code>file_hash</code>, <code>action</code>, <code>user</code></td>
                            <td><code>endpoint</code>, <code>filesystem</code></td>
                        </tr>
                        <tr>
                            <td><strong>Web</strong></td>
                            <td><code>action</code>, <code>dest</code></td>
                            <td><code>src</code>, <code>url</code>, <code>http_method</code>, <code>status</code>, <code>user</code>, <code>user_agent</code></td>
                            <td><code>web</code></td>
                        </tr>
                        <tr>
                            <td><strong>Email</strong></td>
                            <td><code>action</code></td>
                            <td><code>src_user</code>, <code>recipient</code>, <code>subject</code>, <code>file_name</code>, <code>url</code></td>
                            <td><code>email</code></td>
                        </tr>
                        <tr>
                            <td><strong>Malware</strong></td>
                            <td><code>action</code>, <code>signature</code></td>
                            <td><code>dest</code>, <code>file_name</code>, <code>file_hash</code>, <code>user</code>, <code>category</code></td>
                            <td><code>malware</code>, <code>attack</code></td>
                        </tr>
                        <tr>
                            <td><strong>Change</strong></td>
                            <td><code>action</code>, <code>object</code></td>
                            <td><code>user</code>, <code>object_category</code>, <code>status</code>, <code>dest</code></td>
                            <td><code>change</code></td>
                        </tr>
                    </tbody>
                </table>

                <div class="info-box" style="background: rgba(59, 130, 246, 0.1); border-left: 4px solid #3b82f6; padding: 1rem; margin: 1rem 0;">
                    <strong>💡 Interview Tip:</strong> When asked about CIM in interviews, explain:
                    <ol style="margin: 0.5rem 0;">
                        <li><strong>What</strong>: CIM provides standardized field names so different data sources can be analyzed together</li>
                        <li><strong>Why</strong>: Enterprise Security correlation rules use CIM fields - without CIM compliance, your data won't trigger detections</li>
                        <li><strong>How</strong>: Use FIELDALIAS for mapping, EVAL for computed fields, eventtypes.conf + tags.conf for data model assignment</li>
                        <li><strong>Verify</strong>: Use <code>| tstats</code> to confirm data appears in accelerated data models</li>
                    </ol>
                </div>

                <h2 id="indexes"><i class="fas fa-hdd"></i> Index Configuration</h2>

                <!-- COMPREHENSIVE STORAGE TIERS SECTION -->
                <div class="config-section"><div class="arch-diagram">
═══════════════════════════════════════════════════════════════════════════════
     SPLUNK STORAGE TIERS & DATA LIFECYCLE (INTERVIEW-READY)
═══════════════════════════════════════════════════════════════════════════════

UNDERSTANDING SPLUNK STORAGE TIERS:
─────────────────────────────────────────────────────────────────────────────
Splunk uses a tiered storage model where data "ages" through different
storage tiers based on time and usage patterns.

  ┌─────────────────────────────────────────────────────────────────────────┐
  │                     DATA LIFECYCLE FLOW                                  │
  │                                                                          │
  │   INGEST ──► HOT ──► WARM ──► COLD ──► FROZEN ──► (DELETE or ARCHIVE)  │
  │              │        │        │         │                               │
  │              ▼        ▼        ▼         ▼                               │
  │           Minutes   Hours    Days     Months                             │
  │           to Hours  to Days  to Months to Years                          │
  │                                                                          │
  │   COST:   $$$$$    $$$$     $$$       $         (storage cost)          │
  │   SPEED:  Fastest  Fast     Medium    Must Thaw                         │
  └─────────────────────────────────────────────────────────────────────────┘

═══════════════════════════════════════════════════════════════════════════════
WHY DO STORAGE TIERS COST DIFFERENTLY? (THE TECHNICAL TRUTH)
═══════════════════════════════════════════════════════════════════════════════

Understanding WHY helps you make architecture decisions and ace interviews.

WHAT HAPPENS DURING SPLUNK INDEXING (WHY IT'S EXPENSIVE):
─────────────────────────────────────────────────────────────────────────────

  ┌─────────────────────────────────────────────────────────────────────────┐
  │                     SPLUNK INDEXING PIPELINE                             │
  │                                                                          │
  │   RAW EVENT ──────────────────────────────────────────────────────────► │
  │      │                                                                   │
  │      ▼                                                                   │
  │   ┌─────────────────────────────────────────────────────────────────┐   │
  │   │ 1. PARSING (props.conf)                                         │   │
  │   │    • Line breaking (separate events)                            │   │
  │   │    • Timestamp extraction                                       │   │
  │   │    • Host/source/sourcetype assignment                          │   │
  │   └─────────────────────────────────────────────────────────────────┘   │
  │      │                                                                   │
  │      ▼                                                                   │
  │   ┌─────────────────────────────────────────────────────────────────┐   │
  │   │ 2. SEGMENTATION (THIS IS EXPENSIVE!)                            │   │
  │   │    • Break event into searchable segments (tokens)              │   │
  │   │    • Create segment dictionary                                  │   │
  │   │    • Build inverted index for each segment                      │   │
  │   │    • Example: "user=admin" → segments: "user", "admin"          │   │
  │   └─────────────────────────────────────────────────────────────────┘   │
  │      │                                                                   │
  │      ▼                                                                   │
  │   ┌─────────────────────────────────────────────────────────────────┐   │
  │   │ 3. INDEX CREATION (BUCKET)                                      │   │
  │   │    • rawdata/ → Compressed raw events (journal.gz)              │   │
  │   │    • db/ → Time-based indexes (tsidx files)                     │   │
  │   │    • Bloom filters for fast "not found" responses               │   │
  │   └─────────────────────────────────────────────────────────────────┘   │
  │      │                                                                   │
  │      ▼                                                                   │
  │   ┌─────────────────────────────────────────────────────────────────┐   │
  │   │ 4. HOT BUCKET (ACTIVE WRITES)                                   │   │
  │   │    • Stored on fast SSD (NVMe)                                  │   │
  │   │    • Can receive new events                                     │   │
  │   │    • Multiple replicas for search affinity                      │   │
  │   └─────────────────────────────────────────────────────────────────┘   │
  └─────────────────────────────────────────────────────────────────────────┘

WHAT'S IN A BUCKET? (File Structure)
─────────────────────────────────────────────────────────────────────────────

  bucket_name/
    ├── rawdata/
    │   └── journal.gz          ← Compressed raw events (~10:1 ratio)
    │
    ├── db/
    │   ├── *.tsidx             ← Time-series index files (THE INDEX!)
    │   ├── Hosts.data          ← Host metadata
    │   ├── Sources.data        ← Source metadata
    │   ├── SourceTypes.data    ← Sourcetype metadata
    │   └── bloomfilter         ← Fast "term not in bucket" checks
    │
    └── .splunk/
        └── bucket_info.csv     ← Bucket metadata

INDEX SIZE vs RAW SIZE:
  • Raw data (journal.gz): 10-15% of original
  • Index files (tsidx): 10-100% of raw size (depends on cardinality)
  • TOTAL: Can be 20-115% of original data size!
  
  High cardinality = BIG indexes (unique IPs, UUIDs, etc.)
  Low cardinality = Small indexes (status codes, event types)

WHY HOT/WARM IS FAST (AND EXPENSIVE):
─────────────────────────────────────────────────────────────────────────────

  ┌─────────────────────────────────────────────────────────────────────────┐
  │ SEARCH: index=security "failed login"                                   │
  │                                                                          │
  │   1. Check bloom filter → Bucket MIGHT contain "failed" and "login"    │
  │   2. Search tsidx → Find which events contain these segments           │
  │   3. Retrieve from rawdata → Get actual events from journal.gz         │
  │                                                                          │
  │ ALL THIS DATA IS ON FAST SSD:                                           │
  │   • Bloom filter: ~1KB per bucket (instant check)                       │
  │   • tsidx: Sequential read from SSD (milliseconds)                      │
  │   • rawdata: Decompress specific events (fast on SSD)                   │
  │                                                                          │
  │ RESULT: Sub-second searches across billions of events                   │
  └─────────────────────────────────────────────────────────────────────────┘

WHY COLD IS SLOWER (BUT STILL SEARCHABLE):
─────────────────────────────────────────────────────────────────────────────

  ┌─────────────────────────────────────────────────────────────────────────┐
  │ COLD STORAGE (coldPath)                                                 │
  │                                                                          │
  │ • Same bucket structure (rawdata/, db/, bloom)                          │
  │ • BUT stored on slower media (HDD, NAS, SAN)                            │
  │ • Index files still exist → Still searchable                            │
  │ • Just SLOWER to read                                                   │
  │                                                                          │
  │ TRADE-OFF:                                                               │
  │   HDD: ~$0.03/GB vs SSD: ~$0.20/GB (7x cheaper!)                        │
  │   Search: seconds instead of milliseconds (acceptable for old data)    │
  └─────────────────────────────────────────────────────────────────────────┘

WHY FROZEN IS CHEAPEST (NOT DIRECTLY SEARCHABLE):
─────────────────────────────────────────────────────────────────────────────

  ┌─────────────────────────────────────────────────────────────────────────┐
  │ FROZEN DATA                                                              │
  │                                                                          │
  │ Default behavior: DATA IS DELETED when frozen!                          │
  │                                                                          │
  │ To keep frozen data, configure:                                         │
  │   coldToFrozenDir = /archive/splunk/frozen                              │
  │   OR                                                                     │
  │   coldToFrozenScript = /opt/splunk/bin/archive_to_s3.py                 │
  │                                                                          │
  │ WHAT HAPPENS:                                                            │
  │   • Bucket moved/copied to archive location                             │
  │   • ❌ INDEX FILES (tsidx) ARE DELETED!                                 │
  │   • ✅ rawdata/journal.gz is kept                                       │
  │   • Result: Raw data preserved but NOT searchable                       │
  │                                                                          │
  │ TO SEARCH: Must "THAW" (rebuild indexes)                                │
  └─────────────────────────────────────────────────────────────────────────┘

═══════════════════════════════════════════════════════════════════════════════
HOW TO SEARCH FROZEN DATA (THAWING)
═══════════════════════════════════════════════════════════════════════════════

When you need to search archived/frozen data:

STEP 1: LOCATE FROZEN BUCKET
─────────────────────────────────────────────────────────────────────────────
  # Frozen buckets are named like:
  # db_<newest_time>_<oldest_time>_<id>
  
  ls /archive/splunk/frozen/security/
  # db_1704067200_1703980800_12
  # db_1703980800_1703894400_11

STEP 2: COPY TO THAWED DIRECTORY
─────────────────────────────────────────────────────────────────────────────
  # Copy bucket to thawedPath (defined in indexes.conf)
  cp -r /archive/splunk/frozen/security/db_1704067200_1703980800_12 \
        $SPLUNK_DB/security/thaweddb/

STEP 3: REBUILD INDEXES
─────────────────────────────────────────────────────────────────────────────
  # Splunk automatically rebuilds tsidx when it detects bucket in thaweddb
  # OR force rebuild:
  
  $SPLUNK_HOME/bin/splunk rebuild /path/to/bucket

STEP 4: SEARCH
─────────────────────────────────────────────────────────────────────────────
  # Thawed data is now searchable!
  index=security earliest=01/01/2024:00:00:00 latest=01/02/2024:00:00:00

STEP 5: CLEAN UP (Important!)
─────────────────────────────────────────────────────────────────────────────
  # Thawed data stays forever - you must manually remove it
  rm -rf $SPLUNK_DB/security/thaweddb/db_1704067200_1703980800_12

THAWING COSTS/TIME:
  • Rebuilding tsidx takes CPU time (minutes to hours per bucket)
  • Thawed data counts against license (if you search it!)
  • Uses disk space (until you delete)

═══════════════════════════════════════════════════════════════════════════════
SMARTSTORE: MODERN APPROACH (S3 BACKEND)
═══════════════════════════════════════════════════════════════════════════════

SmartStore changes everything - warm/cold data lives in S3!

TRADITIONAL vs SMARTSTORE:
─────────────────────────────────────────────────────────────────────────────

TRADITIONAL:
  ┌──────────────────────────────────────────────────────────────────────────┐
  │ INDEXER LOCAL STORAGE                                                    │
  │                                                                          │
  │   Hot (SSD) ──► Warm (SSD) ──► Cold (HDD) ──► Frozen (Archive/Delete)  │
  │                                                                          │
  │ PROBLEM: Need LOTS of local storage on each indexer                     │
  │          Scaling = Adding expensive disk to every indexer               │
  └──────────────────────────────────────────────────────────────────────────┘

SMARTSTORE:
  ┌──────────────────────────────────────────────────────────────────────────┐
  │ INDEXER (Minimal Local Storage)     │     S3 / AZURE BLOB / GCS         │
  │                                     │                                    │
  │   Hot (SSD) ──► Cache (SSD) ────────┼────► Remote Store                 │
  │        │              │             │       (Warm/Cold data)             │
  │        │              │             │                                    │
  │        ▼              │             │       • Unlimited capacity         │
  │   Write to S3 ────────┼─────────────┼────► • ~$0.02/GB/month            │
  │   immediately         │             │       • Highly durable            │
  │                       │             │                                    │
  │   Cache evicts old ◄──┘             │                                    │
  │   data as needed                    │                                    │
  └─────────────────────────────────────┴────────────────────────────────────┘

HOW SMARTSTORE SEARCH WORKS:
─────────────────────────────────────────────────────────────────────────────

  ┌─────────────────────────────────────────────────────────────────────────┐
  │ SEARCH: index=security earliest=-30d                                    │
  │                                                                          │
  │   1. Check LOCAL CACHE for required buckets                             │
  │                                                                          │
  │   2. If bucket NOT in cache:                                            │
  │      → Download from S3 to local cache                                  │
  │      → This adds latency (seconds for first query)                      │
  │                                                                          │
  │   3. Search proceeds on cached data                                     │
  │                                                                          │
  │   4. Cache management:                                                   │
  │      → LRU eviction (least recently used)                               │
  │      → Frequently searched data stays in cache                          │
  │      → Rarely searched data fetched on demand                           │
  │                                                                          │
  │ RESULT: Near-infinite storage, searches work, but "cold" data has       │
  │         initial latency when not cached                                  │
  └─────────────────────────────────────────────────────────────────────────┘

SMARTSTORE CONFIGURATION:
─────────────────────────────────────────────────────────────────────────────

# server.conf - Define remote storage
[cachemanager]
max_cache_size = 500  # GB of local cache

# indexes.conf - Enable SmartStore for index
[volume:s3_remote]
storageType = remote
path = s3://my-splunk-bucket
remote.s3.endpoint = https://s3.amazonaws.com
remote.s3.access_key = ACCESS_KEY
remote.s3.secret_key = SECRET_KEY

[security]
remotePath = volume:s3_remote/security
repFactor = auto
maxDataSize = auto

SMARTSTORE COSTS:
─────────────────────────────────────────────────────────────────────────────
  • S3 storage: ~$0.023/GB/month (vs $0.10-0.20/GB for SSD)
  • S3 requests: ~$0.005/1000 GET requests
  • Data transfer: $0.09/GB (within region)
  
  TOTAL SAVINGS: 60-80% reduction in storage costs for large deployments

═══════════════════════════════════════════════════════════════════════════════
INTERVIEW QUESTIONS - SPLUNK STORAGE
═══════════════════════════════════════════════════════════════════════════════

Q: "Explain Splunk storage tiers and why they cost differently"
─────────────────────────────────────────────────────────────────────────────
A: "Splunk has four tiers: hot, warm, cold, and frozen. Hot buckets are
    actively receiving new events and stored on fast SSD with full indexes
    for instant searches. Warm is read-only but same fast storage. Cold
    moves to cheaper HDD - still searchable because indexes exist, just
    slower disk access. Frozen is the cheapest because by default the
    index files are deleted, only raw compressed data is kept. To search
    frozen data, you must 'thaw' it - copy back and rebuild the indexes.
    The cost difference is primarily storage media cost plus whether
    indexes are maintained."

Q: "What is SmartStore and when would you use it?"
─────────────────────────────────────────────────────────────────────────────
A: "SmartStore is Splunk's cloud-native storage architecture where warm
    and cold data lives in S3 instead of local indexer disk. Indexers keep
    a local cache for hot data and frequently searched warm data. When you
    search data not in cache, it's fetched from S3 on demand. Use SmartStore
    when you have large data volumes and want to reduce storage costs - S3
    is about $0.02/GB versus $0.20/GB for SSD. It also simplifies scaling
    since indexers don't need massive local storage. The trade-off is
    slightly higher latency for searches that hit uncached data."

Q: "How do you search frozen/archived data in Splunk?"
─────────────────────────────────────────────────────────────────────────────
A: "Frozen data must be 'thawed' before searching. First, locate the frozen
    bucket in your archive - buckets are named with time range and ID. Copy
    the bucket to the thawedPath directory configured in indexes.conf.
    Splunk automatically detects it and rebuilds the tsidx index files -
    this takes time depending on bucket size. Once rebuilt, the data appears
    in searches. Important: thawed data stays forever until you manually
    delete it, and searching thawed data counts against your license."

STORAGE TIER DETAILS:
─────────────────────────────────────────────────────────────────────────────

┌─────────────┬───────────────────────────────────────────────────────────────┐
│ HOT BUCKET  │ ACTIVE INDEXING                                               │
├─────────────┼───────────────────────────────────────────────────────────────┤
│ Storage     │ Fast SSD (NVMe recommended)                                   │
│ Purpose     │ Active writes, real-time ingestion                            │
│ Searchable  │ Yes - Fastest search performance                              │
│ Location    │ homePath in indexes.conf (local to indexer)                   │
│ Duration    │ Until bucket is full or time limit reached                    │
│ Indexes     │ ✅ Full tsidx indexes maintained                              │
│                                                                              │
│ SIZING:     │ Hot bucket size = (daily ingest / # hot buckets) × days      │
│             │ Recommend: 2-7 days of data in hot                            │
└─────────────┴───────────────────────────────────────────────────────────────┘

┌─────────────┬───────────────────────────────────────────────────────────────┐
│ WARM BUCKET │ RECENT SEARCHABLE DATA                                        │
├─────────────┼───────────────────────────────────────────────────────────────┤
│ Storage     │ SSD or fast HDD                                               │
│ Purpose     │ Recently indexed data, still frequently searched              │
│ Searchable  │ Yes - Good search performance                                 │
│ Location    │ homePath (same as hot, different bucket state)                │
│ Duration    │ Configurable - typically 7-30 days                            │
│ Indexes     │ ✅ Full tsidx indexes maintained                              │
│                                                                              │
│ KEY:        │ Data rolls from hot to warm when hot bucket is "rolled"       │
│             │ Warm data is read-only (no new writes)                        │
└─────────────┴───────────────────────────────────────────────────────────────┘

┌─────────────┬───────────────────────────────────────────────────────────────┐
│ COLD BUCKET │ ARCHIVED SEARCHABLE DATA                                      │
├─────────────┼───────────────────────────────────────────────────────────────┤
│ Storage     │ Cheap HDD, SAN, NAS (slower storage OK)                       │
│ Purpose     │ Long-term retention, compliance, forensics                    │
│ Searchable  │ Yes - Slower search, but still queryable                      │
│ Location    │ coldPath in indexes.conf (can be different volume)            │
│ Duration    │ Until frozen (based on frozenTimePeriodInSecs)                │
│ Indexes     │ ✅ Full tsidx indexes maintained                              │
│                                                                              │
│ COST:       │ HDD: ~$0.03/GB vs SSD: ~$0.20/GB (7x cheaper!)               │
│             │ Consider moving coldPath to NAS/SAN for cost savings          │
└─────────────┴───────────────────────────────────────────────────────────────┘

┌─────────────┬───────────────────────────────────────────────────────────────┐
│ FROZEN      │ ARCHIVED (NOT DIRECTLY SEARCHABLE)                            │
├─────────────┼───────────────────────────────────────────────────────────────┤
│ Storage     │ S3, Azure Blob, GCS, NAS, or DELETED                          │
│ Purpose     │ Compliance archive, disaster recovery                         │
│ Searchable  │ ❌ NO - Must be "thawed" to search                            │
│ Location    │ coldToFrozenDir or coldToFrozenScript                         │
│ Duration    │ Until manually deleted                                        │
│ Indexes     │ ❌ tsidx files DELETED (only rawdata kept)                    │
│                                                                              │
│ TO SEARCH:  │ Copy to thawedPath → Splunk rebuilds indexes → Search        │
│                                                                              │
│ SMARTSTORE: │ Modern approach - S3 backend for warm/cold                    │
│             │ Local cache for hot, S3 for everything else                   │
│             │ Search still works (data fetched from S3 as needed)           │
└─────────────┴───────────────────────────────────────────────────────────────┘

═══════════════════════════════════════════════════════════════════════════════
ENTERPRISE LOG PLACEMENT STRATEGY
═══════════════════════════════════════════════════════════════════════════════

WHICH LOGS GO WHERE? (INDEX + RETENTION STRATEGY)
─────────────────────────────────────────────────────────────────────────────

The key principle: SEPARATE INDEXES based on:
  1. Retention requirements (compliance vs operational)
  2. Search patterns (frequently searched vs rarely)
  3. Volume (high volume = shorter retention OR separate index)
  4. Data value (high value = longer retention)

┌───────────────────────────────────────────────────────────────────────────────┐
│                    RECOMMENDED INDEX STRUCTURE                                 │
├───────────────────┬──────────────────┬──────────────┬─────────────────────────┤
│ INDEX NAME        │ LOG TYPES        │ RETENTION    │ RATIONALE               │
├───────────────────┼──────────────────┼──────────────┼─────────────────────────┤
│ security          │ Auth logs        │ 365+ days    │ Compliance, forensics   │
│                   │ AD events        │              │ PCI/SOX may require     │
│                   │ PAM events       │              │ 1+ year retention       │
├───────────────────┼──────────────────┼──────────────┼─────────────────────────┤
│ endpoint_edr      │ EDR alerts       │ 180-365 days │ High value detection    │
│                   │ CrowdStrike      │              │ data, investigation     │
│                   │ Carbon Black     │              │                         │
│                   │ Cortex XDR       │              │                         │
├───────────────────┼──────────────────┼──────────────┼─────────────────────────┤
│ endpoint_telemetry│ Process events   │ 30-90 days   │ HIGH VOLUME but useful  │
│                   │ Network events   │              │ for hunting/forensics   │
│                   │ File events      │              │ Shorter retention OK    │
│                   │ Registry events  │              │                         │
├───────────────────┼──────────────────┼──────────────┼─────────────────────────┤
│ sysmon            │ Sysmon events    │ 90 days      │ Very high value for     │
│                   │ (all Event IDs)  │              │ detection engineering   │
├───────────────────┼──────────────────┼──────────────┼─────────────────────────┤
│ wineventlog       │ Windows Security │ 180 days     │ 4624, 4625, 4688, etc   │
│                   │ Windows System   │              │ Core Windows events     │
│                   │ Windows App      │              │                         │
├───────────────────┼──────────────────┼──────────────┼─────────────────────────┤
│ firewall          │ PAN traffic      │ 30-90 days   │ VERY HIGH VOLUME        │
│                   │ Fortinet         │              │ Keep short, or filter   │
│                   │ Cisco ASA        │              │ Only log denies/alerts  │
├───────────────────┼──────────────────┼──────────────┼─────────────────────────┤
│ proxy             │ Web proxy logs   │ 30-90 days   │ High volume, useful     │
│                   │ Zscaler          │              │ for investigations      │
│                   │ BlueCoat         │              │                         │
├───────────────────┼──────────────────┼──────────────┼─────────────────────────┤
│ dns               │ DNS queries      │ 30 days      │ EXTREMELY HIGH VOLUME   │
│                   │ DNS responses    │              │ Consider sampling or    │
│                   │                  │              │ summary indexing        │
├───────────────────┼──────────────────┼──────────────┼─────────────────────────┤
│ email             │ O365 message     │ 90-180 days  │ Phishing investigation  │
│                   │ trace            │              │ BEC detection           │
│                   │ Exchange logs    │              │                         │
├───────────────────┼──────────────────┼──────────────┼─────────────────────────┤
│ cloud             │ AWS CloudTrail   │ 180-365 days │ Cloud security          │
│                   │ Azure Activity   │              │ Compliance required     │
│                   │ GCP Audit        │              │                         │
├───────────────────┼──────────────────┼──────────────┼─────────────────────────┤
│ network_ids       │ Suricata/Snort   │ 90 days      │ IDS/IPS alerts          │
│                   │ Zeek logs        │              │ Network detection       │
└───────────────────┴──────────────────┴──────────────┴─────────────────────────┘

═══════════════════════════════════════════════════════════════════════════════
EDR/XDR DATA FLOW TO SPLUNK (CRITICAL FOR INTERVIEWS!)
═══════════════════════════════════════════════════════════════════════════════

HOW EDR/XDR DATA GETS TO SPLUNK:
─────────────────────────────────────────────────────────────────────────────

OPTION 1: DIRECT API INTEGRATION (RECOMMENDED)
─────────────────────────────────────────────────────────────────────────────

  ┌─────────────────┐        ┌─────────────────┐        ┌─────────────────┐
  │  EDR CONSOLE    │        │ HEAVY FORWARDER │        │    SPLUNK       │
  │                 │        │                 │        │    INDEXERS     │
  │ • CrowdStrike   │  API   │ Modular Input   │ TCP    │                 │
  │ • Cortex XDR    │ ─────► │ (TA runs here)  │ ─────► │ index=edr       │
  │ • Carbon Black  │        │                 │ 9997   │                 │
  │ • SentinelOne   │        │ Polls API       │        │                 │
  └─────────────────┘        │ Formats data    │        └─────────────────┘
                              └─────────────────┘

  CROWDSTRIKE EXAMPLE:
    TA: CrowdStrike Falcon Event Streams (Splunkbase ID: 5082)
    
    DATA TYPES INGESTED:
    ┌────────────────────────┬───────────────────────────────────────────────┐
    │ Sourcetype             │ What It Contains                              │
    ├────────────────────────┼───────────────────────────────────────────────┤
    │ crowdstrike:events:    │ Detection alerts, incidents                   │
    │ sensor                 │ (HIGH VALUE - keep 365 days)                  │
    ├────────────────────────┼───────────────────────────────────────────────┤
    │ crowdstrike:events:    │ Process creation, network connections         │
    │ endpoint               │ (HIGH VOLUME - keep 30-90 days)               │
    ├────────────────────────┼───────────────────────────────────────────────┤
    │ crowdstrike:events:    │ Indicators of Compromise                      │
    │ ioc                    │ (MEDIUM VOLUME - keep 180 days)               │
    └────────────────────────┴───────────────────────────────────────────────┘

  CORTEX XDR EXAMPLE:
    TA: Palo Alto Networks Add-on for Splunk
    
    DATA TYPES INGESTED:
    ┌────────────────────────┬───────────────────────────────────────────────┐
    │ Sourcetype             │ What It Contains                              │
    ├────────────────────────┼───────────────────────────────────────────────┤
    │ pan:xdr:alert          │ XDR alerts and incidents                      │
    │                        │ (HIGH VALUE - keep 365 days)                  │
    ├────────────────────────┼───────────────────────────────────────────────┤
    │ pan:xdr:endpoint       │ Endpoint telemetry events                     │
    │                        │ (HIGH VOLUME - keep 30-60 days)               │
    └────────────────────────┴───────────────────────────────────────────────┘

OPTION 2: SYSLOG FORWARDING
─────────────────────────────────────────────────────────────────────────────

  ┌─────────────────┐        ┌─────────────────┐        ┌─────────────────┐
  │  EDR CONSOLE    │ Syslog │ HEAVY FORWARDER │ TCP    │    SPLUNK       │
  │                 │ ─────► │ (rsyslog)       │ ─────► │    INDEXERS     │
  │ Configure       │ 514    │                 │ 9997   │                 │
  │ syslog output   │        │ TA for parsing  │        │                 │
  └─────────────────┘        └─────────────────┘        └─────────────────┘

  WHEN TO USE:
    • EDR doesn't have good API integration
    • Need real-time streaming
    • Legacy EDR products

OPTION 3: FILE-BASED INGESTION
─────────────────────────────────────────────────────────────────────────────

  ┌─────────────────┐        ┌─────────────────┐        ┌─────────────────┐
  │  EDR CONSOLE    │  SFTP  │ HEAVY FORWARDER │ TCP    │    SPLUNK       │
  │                 │ ─────► │                 │ ─────► │    INDEXERS     │
  │ Export logs     │ or     │ monitor://      │ 9997   │                 │
  │ to file share   │ NFS    │ path/to/logs    │        │                 │
  └─────────────────┘        └─────────────────┘        └─────────────────┘

  WHEN TO USE:
    • Batch processing acceptable
    • Air-gapped environments
    • Legacy systems

═══════════════════════════════════════════════════════════════════════════════
MICROSOFT DEFENDER XDR → SPLUNK
═══════════════════════════════════════════════════════════════════════════════

SPECIAL CASE: Microsoft has native Sentinel integration, but for Splunk:

OPTION A: MICROSOFT GRAPH SECURITY API
─────────────────────────────────────────────────────────────────────────────
  TA: Microsoft 365 Defender Add-on for Splunk

  ┌─────────────────────┐                    ┌─────────────────────────────┐
  │ Microsoft 365       │                    │ Splunk                      │
  │ Defender Portal     │                    │                             │
  │                     │  Graph API         │ ┌─────────────────────────┐ │
  │ • MDE Alerts        │ ─────────────────► │ │ Heavy Forwarder         │ │
  │ • MDO Alerts        │                    │ │ (runs modular input)    │ │
  │ • MCAS Alerts       │                    │ └───────────┬─────────────┘ │
  │ • Defender Identity │                    │             │               │
  └─────────────────────┘                    │             ▼               │
                                              │ ┌─────────────────────────┐ │
                                              │ │ Index: microsoft365     │ │
                                              │ │ defender                │ │
                                              │ └─────────────────────────┘ │
                                              └─────────────────────────────┘

OPTION B: AZURE EVENT HUB STREAMING
─────────────────────────────────────────────────────────────────────────────
  For high-volume raw telemetry (Advanced Hunting tables)

  ┌─────────────────────┐     ┌─────────────────┐     ┌─────────────────┐
  │ Defender XDR        │     │ Azure Event Hub │     │ Splunk HEC      │
  │                     │────►│                 │────►│                 │
  │ Streaming API       │     │ Stream events   │     │ HTTP input      │
  │ (raw tables)        │     │ to consumer     │     │                 │
  └─────────────────────┘     └─────────────────┘     └─────────────────┘

  RAW TABLES AVAILABLE:
  ┌─────────────────────────┬─────────────────────────────────────────────┐
  │ DeviceProcessEvents     │ All process creation (VERY HIGH VOLUME)     │
  │ DeviceNetworkEvents     │ All network connections (VERY HIGH VOLUME)  │
  │ DeviceFileEvents        │ File operations (HIGH VOLUME)               │
  │ DeviceLogonEvents       │ Logon events (MEDIUM VOLUME)                │
  │ EmailEvents             │ Email metadata (MEDIUM VOLUME)              │
  │ IdentityLogonEvents     │ Entra ID sign-ins (MEDIUM VOLUME)           │
  └─────────────────────────┴─────────────────────────────────────────────┘

  RETENTION RECOMMENDATION:
    • Alerts: 365 days (compliance, investigation)
    • Raw telemetry: 30-90 days (very high volume, hunting only)

═══════════════════════════════════════════════════════════════════════════════
COST OPTIMIZATION STRATEGIES
═══════════════════════════════════════════════════════════════════════════════

Splunk licensing is based on DAILY INGESTION VOLUME. Key strategies:

1. FILTER AT SOURCE
   • Don't send debug/info logs if you only need warnings/errors
   • Whitelist specific Event IDs instead of all events
   • Filter noisy firewall "allow" logs, keep "deny" logs

2. USE SEPARATE INDEXES WITH DIFFERENT RETENTION
   • High-value, low-volume → Long retention (365 days)
   • High-volume, low-value → Short retention (30 days)
   • Don't keep firewall traffic logs for a year!

3. SUMMARY INDEXING
   • Create summary indexes for long-term trends
   • Keep raw data 30 days, summaries for 365 days
   • Example: Daily count of auth failures by user (not every event)

4. SMARTSTORE (S3 BACKEND)
   • Reduces local storage costs dramatically
   • Hot data stays local (fast), warm/cold goes to S3
   • Search still works (fetches from S3 as needed)

5. DATA SAMPLING FOR EXTREME VOLUMES
   • For DNS logs: Sample 1 in 10 queries
   • For NetFlow: Summarize instead of raw
   • Document what's sampled for investigators!

═══════════════════════════════════════════════════════════════════════════════
INTERVIEW QUESTIONS & ANSWERS
═══════════════════════════════════════════════════════════════════════════════

Q: "Explain Splunk storage tiers and when data moves between them"
─────────────────────────────────────────────────────────────────────────────
A: "Splunk has four storage tiers: hot, warm, cold, and frozen. Hot is for
    active indexing on fast SSD storage. When a hot bucket fills or ages,
    it rolls to warm - still on fast storage but read-only. Warm data ages
    to cold based on time or size thresholds - this can be on cheaper
    storage since it's less frequently searched. Finally, frozen data is
    either deleted or archived to S3/SmartStore. The key is matching
    storage cost to access frequency - hot data needs speed, cold data
    needs capacity."

Q: "How would you structure indexes for a large enterprise?"
─────────────────────────────────────────────────────────────────────────────
A: "I'd separate indexes by retention requirement, volume, and search
    pattern. Security and compliance logs like authentication events get
    their own index with 365-day retention. High-volume logs like firewall
    traffic get a separate index with 30-day retention. EDR alerts go to
    one index with long retention, but EDR telemetry - which is massive -
    gets a separate index with shorter retention. This lets me tune storage
    costs while meeting compliance requirements."

Q: "How does EDR/XDR data flow into Splunk?"
─────────────────────────────────────────────────────────────────────────────
A: "Typically via API integration using a Technology Add-on. For CrowdStrike,
    I install the Falcon Event Streams TA on a Heavy Forwarder. The TA's
    modular input polls CrowdStrike's Streaming API for detection events
    and endpoint telemetry. The data comes in as structured JSON with
    sourcetypes like crowdstrike:events:sensor. I route alerts to an EDR
    index with long retention, and high-volume telemetry to a separate
    index with shorter retention. For Microsoft Defender, I'd use the
    Graph Security API integration or Event Hub streaming for raw tables."
                </div>

                <div class="file-block">
                    <div class="header">indexes.conf - Complete Example</div>
<pre># ═══════════════════════════════════════════════════════════════════════════
# INDEX CONFIGURATION
# Location: $SPLUNK_HOME/etc/apps/myapp/local/indexes.conf
# ═══════════════════════════════════════════════════════════════════════════

# Security logs - long retention
[security]
homePath = $SPLUNK_DB/security/db
coldPath = $SPLUNK_DB/security/colddb
thawedPath = $SPLUNK_DB/security/thaweddb
frozenTimePeriodInSecs = 31536000  # 365 days
maxTotalDataSizeMB = 500000        # 500 GB

# Windows Event Logs
[wineventlog]
homePath = $SPLUNK_DB/wineventlog/db
coldPath = $SPLUNK_DB/wineventlog/colddb
thawedPath = $SPLUNK_DB/wineventlog/thaweddb
frozenTimePeriodInSecs = 15552000  # 180 days
maxTotalDataSizeMB = 1000000       # 1 TB

# Sysmon (high value endpoint data)
[sysmon]
homePath = $SPLUNK_DB/sysmon/db
coldPath = $SPLUNK_DB/sysmon/colddb
thawedPath = $SPLUNK_DB/sysmon/thaweddb
frozenTimePeriodInSecs = 7776000   # 90 days
maxTotalDataSizeMB = 500000

# Firewall logs - shorter retention, high volume
[firewall]
homePath = $SPLUNK_DB/firewall/db
coldPath = $SPLUNK_DB/firewall/colddb
thawedPath = $SPLUNK_DB/firewall/thaweddb
frozenTimePeriodInSecs = 2592000   # 30 days
maxTotalDataSizeMB = 2000000       # 2 TB

# Linux/Unix logs
[linux]
homePath = $SPLUNK_DB/linux/db
coldPath = $SPLUNK_DB/linux/colddb
thawedPath = $SPLUNK_DB/linux/thaweddb
frozenTimePeriodInSecs = 7776000   # 90 days
maxTotalDataSizeMB = 200000

# SmartStore configuration (S3 backend for frozen)
[volume:s3_remote]
storageType = remote
path = s3://my-splunk-bucket/frozen
remote.s3.endpoint = https://s3.amazonaws.com

[security]
remotePath = volume:s3_remote/security</pre>
                </div>

                <h2><i class="fas fa-chart-line"></i> Monitoring Ingestion Health</h2>

                <div class="arch-diagram"><pre># ═══════════════════════════════════════════════════════════════════════════
# INGESTION MONITORING QUERIES
# ═══════════════════════════════════════════════════════════════════════════

# Daily ingestion by index
index=_internal source=*license_usage.log type=Usage
| timechart span=1d sum(b) AS bytes by idx
| eval GB = bytes / 1024 / 1024 / 1024
| fields - bytes

# Ingestion by sourcetype
index=_internal source=*license_usage.log type=Usage
| stats sum(b) AS bytes by st
| eval GB = round(bytes / 1024 / 1024 / 1024, 2)
| sort -GB

# Forwarder health
index=_internal sourcetype=splunkd group=tcpin_connections
| stats latest(connectionType) AS type, latest(version) AS version, 
        max(_time) AS lastSeen by hostname
| eval status = if(lastSeen > relative_time(now(), "-15m"), "Active", "Inactive")

# Events per second by sourcetype
index=* earliest=-1h
| timechart span=1m count by sourcetype
| addtotals

# Find silent sources (no data in last hour)
| rest /services/data/inputs/all
| search disabled=0
| eval lastEventTime = strftime(lastEventReceivedTime, "%Y-%m-%d %H:%M:%S")
| where lastEventReceivedTime < relative_time(now(), "-1h")

# Index queue health (parsing bottleneck)
index=_internal source=*metrics.log group=queue
| timechart span=1m avg(current_size) by name</pre></div>

                <h2 id="hands-on"><i class="fas fa-laptop-code"></i> Hands-On Real-World Scenarios</h2>
                
                <p>These end-to-end scenarios walk you through common real-world tasks so you can gain practical experience even without a lab environment.</p>

                <div class="config-section"><div class="arch-diagram">
SCENARIO 1: ONBOARD WINDOWS SERVERS TO SPLUNK (Complete Workflow)
═══════════════════════════════════════════════════════════════════════════════

ARCHITECTURE:
  
  [Windows Servers]     [Deployment Server]     [Indexer Cluster]     [Search Head]
        │                      │                       │                    │
        │ 1. Install UF        │                       │                    │
        │───────────────────►  │                       │                    │
        │                      │                       │                    │
        │ 2. Receive config    │                       │                    │
        │ ◄───────────────────│                       │                    │
        │                      │                       │                    │
        │ 3. Forward events ───────────────────────────►                    │
        │                      │                       │                    │
        │                      │                       │ 4. Query ◄─────────│
        │                      │                       │───────────────────►│

STEP-BY-STEP:

Step 1: Install Universal Forwarder on Windows Server
────────────────────────────────────────────────────────────────────────────

  # Download from splunk.com/download/universalforwarder
  # Run installer (GUI or command line)
  
  msiexec.exe /i splunkforwarder-9.3.0-x64-release.msi AGREETOLICENSE=Yes ^
    RECEIVING_INDEXER="indexer1.company.com:9997" ^
    DEPLOYMENT_SERVER="deploy.company.com:8089" ^
    LAUNCHSPLUNK=1 SERVICESTARTTYPE=auto /quiet
  
  # After install, UF will phone home to deployment server

Step 2: Create App on Deployment Server
────────────────────────────────────────────────────────────────────────────

  # On Deployment Server: /opt/splunk/etc/deployment-apps/
  
  mkdir -p /opt/splunk/etc/deployment-apps/TA_windows_inputs/local
  
  # Create inputs.conf
  cat > /opt/splunk/etc/deployment-apps/TA_windows_inputs/local/inputs.conf << 'EOF'
  # Security Events - filtered to important events
  [WinEventLog://Security]
  disabled = 0
  index = wineventlog
  whitelist = 4624,4625,4648,4672,4688,4720,4726,4732,4756,4768,4769,4771
  
  # Sysmon (if installed)
  [WinEventLog://Microsoft-Windows-Sysmon/Operational]
  disabled = 0
  index = sysmon
  renderXml = true
  
  # PowerShell Script Block Logging
  [WinEventLog://Microsoft-Windows-PowerShell/Operational]
  disabled = 0
  index = wineventlog
  whitelist = 4103,4104
  EOF

Step 3: Create Server Class
────────────────────────────────────────────────────────────────────────────

  # Edit /opt/splunk/etc/system/local/serverclass.conf
  
  [serverClass:Windows_Servers]
  whitelist.0 = *.company.com
  machineTypesFilter = windows-*
  
  [serverClass:Windows_Servers:app:Splunk_TA_windows]
  restartSplunkd = 1
  stateOnClient = enabled
  
  [serverClass:Windows_Servers:app:TA_windows_inputs]
  restartSplunkd = 1
  stateOnClient = enabled
  
  # Reload deployment server
  /opt/splunk/bin/splunk reload deploy-server

Step 4: Verify Data Flow
────────────────────────────────────────────────────────────────────────────

  # On Search Head - run these SPL queries
  
  # Check if events are arriving
  index=wineventlog earliest=-15m
  | stats count by host, sourcetype
  
  # Check specific event types
  index=wineventlog EventCode=4624 earliest=-1h
  | stats count by host, Logon_Type
  
  # Verify Sysmon
  index=sysmon earliest=-1h
  | stats count by EventCode, host
                </div></div>

                <div class="config-section"><div class="arch-diagram">
SCENARIO 2: ONBOARD PALO ALTO FIREWALL (Syslog to Heavy Forwarder)
═══════════════════════════════════════════════════════════════════════════════

ARCHITECTURE:

  [Palo Alto FW]  ──TCP 514──►  [Heavy Forwarder]  ──TCP 9997──►  [Indexers]
                               (rsyslog + Splunk)
                               • Receives syslog
                               • Parses with TA
                               • Forwards to indexers

Step 1: Configure Heavy Forwarder to Receive Syslog
────────────────────────────────────────────────────────────────────────────

  # Install Splunk Enterprise on Heavy Forwarder VM (not UF!)
  # Download from splunk.com
  
  rpm -i splunk-9.3.0-x64.rpm
  /opt/splunk/bin/splunk start --accept-license
  
  # Disable local indexing (it's a forwarder)
  /opt/splunk/bin/splunk disable local-index

Step 2: Install Palo Alto TA on Heavy Forwarder
────────────────────────────────────────────────────────────────────────────

  # Download from Splunkbase: "Splunk Add-on for Palo Alto Networks" (7523)
  cd /opt/splunk/etc/apps
  tar -xvf /tmp/Splunk_Add-on_for_Palo_Alto_Networks.tgz

Step 3: Configure Syslog Input
────────────────────────────────────────────────────────────────────────────

  # Create inputs.conf
  cat > /opt/splunk/etc/apps/Splunk_TA_paloalto/local/inputs.conf << 'EOF'
  [tcp://514]
  disabled = 0
  connection_host = dns
  sourcetype = pan:log
  index = firewall
  EOF
  
  # Restart Splunk
  /opt/splunk/bin/splunk restart

Step 4: Configure Palo Alto to Send Logs
────────────────────────────────────────────────────────────────────────────

  # In Palo Alto GUI:
  # Device → Server Profiles → Syslog
  # 
  # Name: Splunk_Syslog
  # Server: [Heavy Forwarder IP]
  # Transport: TCP
  # Port: 514
  # Format: BSD
  # Facility: LOG_USER
  #
  # Then: Objects → Log Forwarding → Create profile
  # Send Traffic, Threat, System logs to Splunk_Syslog profile
  #
  # Apply to Security Rules: Rules → Select rule → Actions → Log Forwarding

Step 5: Configure Outputs to Indexers
────────────────────────────────────────────────────────────────────────────

  # On Heavy Forwarder
  cat > /opt/splunk/etc/system/local/outputs.conf << 'EOF'
  [tcpout]
  defaultGroup = indexer_cluster
  
  [tcpout:indexer_cluster]
  server = indexer1.company.com:9997, indexer2.company.com:9997
  compressed = true
  useACK = true
  EOF
  
  /opt/splunk/bin/splunk restart

Step 6: Verify Data
────────────────────────────────────────────────────────────────────────────

  # On Search Head
  index=firewall sourcetype=pan:traffic earliest=-1h
  | stats count by action, app
  | sort -count
  
  # Check CIM compliance
  | tstats count from datamodel=Network_Traffic by All_Traffic.action
                </div></div>

                <div class="config-section"><div class="arch-diagram">
SCENARIO 3: TROUBLESHOOT "NO DATA" ISSUES
═══════════════════════════════════════════════════════════════════════════════

TROUBLESHOOTING FLOWCHART:

  No data in Splunk?
        │
        ├─► Check Forwarder
        │      │
        │      ├─► Is UF running? → splunkd status
        │      ├─► Is input enabled? → btool inputs list --debug
        │      ├─► Network connectivity? → telnet indexer 9997
        │      └─► Check internal logs → index=_internal host=forwarder
        │
        ├─► Check Network
        │      │
        │      ├─► Firewall blocking? → Test port 9997
        │      ├─► DNS resolution? → nslookup indexer
        │      └─► Load balancer? → Check health check
        │
        ├─► Check Indexer
        │      │
        │      ├─► Receiving port listening? → netstat -an | grep 9997
        │      ├─► Queue blocked? → index=_internal group=queue
        │      └─► Index exists? → | rest /services/data/indexes
        │
        └─► Check Parsing
               │
               ├─► Wrong sourcetype? → btool props list
               ├─► Time parsing? → Check TIME_FORMAT in props.conf
               └─► Line breaking? → Check LINE_BREAKER

DIAGNOSTIC QUERIES:

  # 1. Check if forwarder is connected
  index=_internal sourcetype=splunkd group=tcpin_connections
  | stats latest(connectionType) latest(version) by hostname
  | sort hostname
  
  # 2. Check internal forwarder logs
  index=_internal host=<forwarder_hostname> log_level=ERROR
  | head 50
  
  # 3. Check if data is hitting any index
  | tstats count where index=* by index, sourcetype
  
  # 4. Check parsing queue
  index=_internal source=*metrics.log group=queue name=parsingQueue
  | timechart avg(current_size) avg(largest_size)
  
  # 5. Find events with bad timestamps (future dates)
  index=* earliest=+1d
  | stats count by index, sourcetype
  
  # 6. Check license usage
  index=_internal source=*license_usage.log type=Usage
  | stats sum(b) as bytes by pool
  | eval GB = round(bytes/1024/1024/1024, 2)
                </div></div>

                <div class="config-section"><div class="arch-diagram">
SCENARIO 4: CREATE CUSTOM SOURCETYPE FOR APPLICATION LOGS
═══════════════════════════════════════════════════════════════════════════════

SAMPLE LOG FORMAT:

  2024-01-15T10:30:45.123Z INFO  [UserService] user=john.doe action=login ip=10.0.0.50 result=success duration=0.234
  2024-01-15T10:30:46.456Z ERROR [AuthService] user=jane.smith action=login ip=10.0.0.51 result=failure reason="invalid_password"
  2024-01-15T10:31:00.789Z WARN  [RateLimiter] user=john.doe action=api_call ip=10.0.0.50 result=throttled calls=150

Step 1: Create Custom App Structure
────────────────────────────────────────────────────────────────────────────

  mkdir -p /opt/splunk/etc/apps/TA_myapp/local
  mkdir -p /opt/splunk/etc/apps/TA_myapp/default
  mkdir -p /opt/splunk/etc/apps/TA_myapp/metadata
  
  # Create app.conf
  cat > /opt/splunk/etc/apps/TA_myapp/default/app.conf << 'EOF'
  [install]
  state = enabled
  
  [launcher]
  description = Technology Add-on for MyApp logs
  version = 1.0.0
  
  [ui]
  is_visible = false
  label = TA-myapp
  EOF

Step 2: Create props.conf for Parsing
────────────────────────────────────────────────────────────────────────────

  cat > /opt/splunk/etc/apps/TA_myapp/local/props.conf << 'EOF'
  [myapp:log]
  # Timestamp settings
  TIME_FORMAT = %Y-%m-%dT%H:%M:%S.%3N%Z
  TIME_PREFIX = ^
  MAX_TIMESTAMP_LOOKAHEAD = 30
  TZ = UTC
  
  # Event breaking
  SHOULD_LINEMERGE = false
  LINE_BREAKER = ([\r\n]+)
  TRUNCATE = 10000
  
  # Field extractions (inline regex)
  EXTRACT-level = ^[^\s]+\s+(?P<level>\w+)
  EXTRACT-service = \[(?P<service>\w+)\]
  EXTRACT-user = user=(?P<user>[^\s]+)
  EXTRACT-action = action=(?P<action>[^\s]+)
  EXTRACT-src_ip = ip=(?P<src_ip>[^\s]+)
  EXTRACT-result = result=(?P<result>[^\s]+)
  EXTRACT-duration = duration=(?P<duration>[\d.]+)
  EXTRACT-reason = reason="(?P<reason>[^"]+)"
  
  # CIM field aliases for Authentication data model
  FIELDALIAS-src = src_ip AS src
  FIELDALIAS-app = service AS app
  EVAL-vendor = "MyCompany"
  EVAL-product = "MyApp"
  EVAL-vendor_product = "MyCompany MyApp"
  EOF

Step 3: Create tags.conf for CIM Compliance
────────────────────────────────────────────────────────────────────────────

  cat > /opt/splunk/etc/apps/TA_myapp/local/tags.conf << 'EOF'
  [eventtype=myapp_authentication]
  authentication = enabled
  EOF
  
  cat > /opt/splunk/etc/apps/TA_myapp/local/eventtypes.conf << 'EOF'
  [myapp_authentication]
  search = sourcetype="myapp:log" action IN ("login", "logout", "authenticate")
  EOF

Step 4: Create inputs.conf
────────────────────────────────────────────────────────────────────────────

  cat > /opt/splunk/etc/apps/TA_myapp/local/inputs.conf << 'EOF'
  [monitor:///var/log/myapp/application.log]
  disabled = 0
  sourcetype = myapp:log
  index = application
  EOF

Step 5: Test and Verify
────────────────────────────────────────────────────────────────────────────

  # Restart Splunk
  /opt/splunk/bin/splunk restart
  
  # Search for data
  index=application sourcetype=myapp:log earliest=-1h
  | table _time, level, service, user, action, result
  
  # Verify CIM compliance
  | tstats count from datamodel=Authentication by Authentication.action
                </div></div>

                <!-- Decision Trees -->
                <h2><i class="fas fa-project-diagram"></i> Data Ingestion Decision Trees</h2>
                
                <div class="config-section">
                    <h3>Decision Tree: Ingestion Method Selection</h3>
                    <div class="arch-diagram">
Decision Tree: Selecting the Right Data Ingestion Method
═══════════════════════════════════════════════════════════════════════════════

START: What type of data source?
│
├─► [File-based logs on server]
│   │
│   └─► Can you install software on the server?
│       │
│       ├─► YES → Is it Windows or Linux?
│       │         │
│       │         ├─► Windows → Install Universal Forwarder
│       │         │             Configure inputs.conf for file monitoring
│       │         │             Use deployment server for management
│       │         │
│       │         └─► Linux → Install Universal Forwarder
│       │                     OR use Syslog forwarding (rsyslog/syslog-ng)
│       │                     UF preferred for reliability and buffering
│       │
│       └─► NO → Is there a centralized log aggregator?
│                 │
│                 ├─► YES → Forward to Splunk from aggregator
│                 │         (Syslog, HEC, or S2S from aggregator)
│                 │
│                 └─► NO → Can you use agentless collection?
│                           │
│                           ├─► YES → Use Heavy Forwarder with
│                           │         scripted inputs or WMI (Windows)
│                           │
│                           └─► NO → Evaluate network tap/SPAN
│                                     or API-based collection
│
├─► [Cloud service / SaaS application]
│   │
│   └─► Does a TA exist for this service?
│       │
│       ├─► YES → Use TA with modular inputs
│       │         Configure API credentials
│       │         Set appropriate polling interval
│       │
│       └─► NO → Does the service support webhooks?
│                 │
│                 ├─► YES → Configure HEC endpoint
│                 │         Set up webhook in cloud service
│                 │
│                 └─► NO → Does it have an API?
│                           │
│                           ├─► YES → Write scripted input or
│                           │         use Add-on Builder to create TA
│                           │
│                           └─► NO → Check for S3/Blob export options
│                                     or third-party integration
│
├─► [Network devices (firewalls, switches, routers)]
│   │
│   └─► What protocol does the device support?
│       │
│       ├─► Syslog → Configure device to send syslog
│       │            Deploy syslog receiver (HF or indexer)
│       │            Use TCP for reliability, UDP for volume
│       │            Port 514 (UDP) or 6514 (TLS)
│       │
│       ├─► SNMP → Use SNMP modular input
│       │          Configure SNMP community strings
│       │          Set polling interval based on metric type
│       │
│       └─► Streaming/API → Use appropriate TA
│                           (e.g., Palo Alto, Cisco)
│
├─► [Application sending events programmatically]
│   │
│   └─► Use HTTP Event Collector (HEC)
│       │
│       ├─► High volume (>10K EPS)? → Deploy load balancer
│       │                             Multiple HEC endpoints
│       │                             Enable indexer acknowledgment
│       │
│       └─► Standard volume → Single HEC endpoint
│                              Consider batching events
│                              Use JSON format preferred
│
└─► [Database tables or queries]
    │
    └─► Use DB Connect app
        Configure JDBC connection
        Set appropriate query schedule
        Consider checkpoint columns for incremental
                    </div>
                </div>

                <div class="config-section">
                    <h3>Decision Tree: Forwarder Architecture Selection</h3>
                    <div class="arch-diagram">
Decision Tree: Choosing Forwarder Architecture
═══════════════════════════════════════════════════════════════════════════════

START: What are your requirements?
│
├─► [Simple file monitoring, Windows Event Logs]
│   │
│   └─► Universal Forwarder (UF)
│       • 50-100MB memory footprint
│       • No local search capability
│       • Managed via deployment server
│       • Best for: endpoints, standard servers
│
├─► [Need local parsing or data transformation]
│   │
│   └─► Heavy Forwarder (HF)
│       • Full Splunk installation
│       • Can parse before forwarding
│       • Data routing/filtering at source
│       • Best for: aggregation points, DMZ, data reduction
│
├─► [Syslog aggregation point]
│   │
│   └─► What's the volume?
│       │
│       ├─► <50K EPS → Heavy Forwarder with syslog input
│       │              Enable TLS for secure transport
│       │
│       └─► >50K EPS → Consider dedicated syslog-ng/rsyslog
│                       Forward to Splunk via S2S or HEC
│                       Better performance for high-volume syslog
│
├─► [Routing data to multiple destinations]
│   │
│   └─► Heavy Forwarder with outputs.conf
│       • Configure multiple target groups
│       • Use routing rules by sourcetype/index
│       • Can send to Splunk + S3/Kafka simultaneously
│
├─► [Cloud/container environment]
│   │
│   └─► What platform?
│       │
│       ├─► Kubernetes → Splunk Connect for Kubernetes
│       │                DaemonSet for node logs
│       │                Sidecar for specific apps
│       │
│       ├─► AWS → Kinesis Firehose to HEC
│       │         Lambda for custom processing
│       │         AWS Add-on for CloudWatch/CloudTrail
│       │
│       └─► Azure → Azure Functions to HEC
│                   Event Hub integration
│                   Azure Monitor Add-on
│
└─► [Air-gapped or intermittent connectivity]
    │
    └─► Universal Forwarder with persistent queue
        • Configure maxQueueSize appropriately
        • Enable indexer acknowledgment
        • Consider store-and-forward architecture
                    </div>
                </div>

                <div class="config-section">
                    <h3>Decision Tree: Troubleshooting Data Ingestion Issues</h3>
                    <div class="arch-diagram">
Decision Tree: Data Not Appearing in Splunk
═══════════════════════════════════════════════════════════════════════════════

START: Data is not appearing in Splunk searches
│
├─► Step 1: Is the forwarder running?
│   │
│   ├─► Check: splunk status
│   │
│   ├─► NOT RUNNING → Check logs: /opt/splunk/var/log/splunk/splunkd.log
│   │                 Common issues: license, disk space, permissions
│   │
│   └─► RUNNING → Continue to Step 2
│
├─► Step 2: Is data being monitored?
│   │
│   ├─► Check: splunk btool inputs list --debug
│   │
│   ├─► Input NOT listed → Verify inputs.conf syntax and location
│   │                      Check app context (local vs default)
│   │                      Restart after changes
│   │
│   └─► Input listed → Continue to Step 3
│
├─► Step 3: Is data being read?
│   │
│   ├─► Check fishbucket: splunk cmd btprobe -d /opt/splunk/var/lib/splunk/fishbucket/splunk_private_db --file <filepath>
│   │
│   ├─► File shows seekptr → Data is being read
│   │    Check internal logs: index=_internal source=*metrics.log group=per_sourcetype_thruput
│   │
│   └─► No fishbucket entry → Permission issue on file?
│                             File path correct?
│                             Whitelist/blacklist filtering?
│
├─► Step 4: Is data being sent?
│   │
│   ├─► Check: index=_internal source=*metrics.log group=tcpout_connections
│   │
│   ├─► Connection errors → Firewall blocking 9997?
│   │                       Certificate issues (if SSL)?
│   │                       Check outputs.conf server list
│   │
│   └─► Data sending OK → Continue to Step 5
│
├─► Step 5: Is indexer receiving?
│   │
│   ├─► On indexer: index=_internal source=*splunkd.log component=TcpInputProc
│   │
│   ├─► Connection refused → Check inputs.conf [splunktcp:9997]
│   │                        Verify indexer is listening
│   │
│   └─► Receiving OK → Continue to Step 6
│
├─► Step 6: Is data being indexed?
│   │
│   ├─► Check: index=_internal source=*metrics.log group=per_index_thruput
│   │
│   ├─► No thruput → Index doesn't exist?
│   │                Index disabled?
│   │                Parsing/routing issue sending to null queue?
│   │
│   └─► Thruput OK → Continue to Step 7
│
└─► Step 7: Search and permissions
    │
    ├─► Data indexed but not searchable → 
    │   • Check user's index permissions
    │   • Verify search time range
    │   • Check for time zone parsing issues
    │   • Run: | tstats count where index=* by index (admin)
    │
    └─► Still not found →
        • Check if data is being dropped by transforms
        • Verify sourcetype assignment
        • Check license warnings in _internal
                    </div>
                </div>

                <!-- What Enterprises Usually Miss -->
                <h2><i class="fas fa-exclamation-circle"></i> What Enterprises Usually Miss</h2>
                
                <div class="enterprise-gap-grid">
                    <div class="gap-card gap-critical">
                        <h4>🔴 Critical Gaps</h4>
                        <ul>
                            <li><strong>No data validation pipeline:</strong> No verification that expected data is flowing (golden source checks)</li>
                            <li><strong>Missing command-line logging:</strong> Windows endpoints without Sysmon or enhanced logging - blind to execution</li>
                            <li><strong>No HEC token rotation:</strong> Same tokens used for years, no expiration policy, compromised tokens undetected</li>
                            <li><strong>Forwarder orphan problem:</strong> Thousands of forwarders deployed but no inventory - decommissioned servers still licensed</li>
                            <li><strong>No indexer acknowledgment:</strong> Data loss during indexer restarts or failures goes unnoticed</li>
                        </ul>
                    </div>
                    
                    <div class="gap-card gap-warning">
                        <h4>🟡 Common Oversights</h4>
                        <ul>
                            <li><strong>Deployment server sprawl:</strong> Multiple deployment servers with conflicting serverclasses, inconsistent configs</li>
                            <li><strong>TAs on wrong tier:</strong> Props/transforms on search heads instead of indexers/HFs - search-time parsing overhead</li>
                            <li><strong>No forwarder monitoring:</strong> No alerts for forwarders going silent - blind spots emerge undetected</li>
                            <li><strong>HEC without load balancing:</strong> Single HEC endpoint becomes bottleneck and single point of failure</li>
                            <li><strong>Syslog UDP without rate limiting:</strong> Bursty sources overwhelm UDP listeners, silent data loss</li>
                        </ul>
                    </div>
                    
                    <div class="gap-card gap-info">
                        <h4>🔵 Often Overlooked</h4>
                        <ul>
                            <li><strong>No sourcetype standardization:</strong> Same data ingested with different sourcetypes across environments</li>
                            <li><strong>Missing null queue usage:</strong> Ingesting verbose debug logs at full license cost instead of filtering</li>
                            <li><strong>No ingestion latency monitoring:</strong> Delayed data breaks real-time alerting without detection</li>
                            <li><strong>Fishbucket not managed:</strong> Fishbucket grows indefinitely, never cleaned for decommissioned files</li>
                            <li><strong>SSL certificate expiration:</strong> S2S and HEC certificates expire causing silent ingestion failures</li>
                        </ul>
                    </div>
                    
                    <div class="gap-card gap-best-practice">
                        <h4>🟢 Best Practices Often Skipped</h4>
                        <ul>
                            <li><strong>Ingest-time data enrichment:</strong> Not adding business context (asset owner, criticality) during ingestion</li>
                            <li><strong>Parallel ingestion pipelines:</strong> Single pipeline for all data instead of dedicated pipelines for critical sources</li>
                            <li><strong>Pre-production testing:</strong> New TAs/sourcetypes pushed to production without parsing validation</li>
                            <li><strong>Event sampling:</strong> Not using sampling for high-volume, low-value logs (web access logs)</li>
                            <li><strong>Data source documentation:</strong> No catalog of what data is ingested, why, and who owns it</li>
                        </ul>
                    </div>
                </div>

                <!-- Interview Q&A -->
                <h2><i class="fas fa-comments"></i> Interview Questions & Answers</h2>
                
                <div class="qa-section">
                    <div class="qa-item">
                        <button class="qa-question">Q: Walk me through onboarding a new data source end-to-end. What's your process?</button>
                        <div class="qa-answer">
                            <p><strong>Structured Approach:</strong></p>
                            <div class="arch-diagram">
Phase 1: Discovery & Planning (Week 1)
─────────────────────────────────────────────────────────
• Meet with data owner to understand:
  - What business/security value does this data provide?
  - Expected volume (GB/day) and event rate (EPS)
  - Retention requirements (compliance, operational)
  - Who needs access and what searches will they run?

• Technical assessment:
  - Sample data (minimum 1000 events across scenarios)
  - Log format analysis (structured/unstructured)
  - Check Splunkbase for existing TAs
  - Determine ingestion method (UF, HEC, syslog, API)

Phase 2: Development (Week 2)
─────────────────────────────────────────────────────────
• Create or customize TA:
  - inputs.conf: Collection configuration
  - props.conf: Time parsing, line breaking, field extraction
  - transforms.conf: Lookups, routing, field transforms
  - tags.conf/eventtypes.conf: CIM compliance

• Index planning:
  - Dedicated index or shared (based on retention, access)
  - Calculate storage: GB/day × retention × 0.5 (compression)
  - Configure indexes.conf

Phase 3: Testing (Week 3)
─────────────────────────────────────────────────────────
• Dev/Test environment validation:
  - Verify all events parse correctly (_time, host, source, sourcetype)
  - Confirm field extractions work across all event types
  - Test CIM compliance: | datamodel | tstats count
  - Performance test: ingestion rate matches expected
  - Search performance on typical queries

Phase 4: Production Deployment (Week 4)
─────────────────────────────────────────────────────────
• Staged rollout:
  - Deploy to 10% of sources first
  - Monitor for 24-48 hours
  - Check: internal logs for errors, license usage, parse failures
  
• Full deployment:
  - Push via deployment server (serverclass.conf)
  - Configure monitoring: expected data check alert
  - Document in data source catalog
  - Handoff to operations team
                            </div>
                            <p><strong>Why this matters:</strong> Shows methodical approach that prevents "data in but unusable" syndrome. Emphasizes business value, testing, and operationalization.</p>
                        </div>
                    </div>

                    <div class="qa-item">
                        <button class="qa-question">Q: Your organization is ingesting 500GB/day and needs to reduce license costs by 40%. How do you approach this?</button>
                        <div class="qa-answer">
                            <p><strong>Data Reduction Strategy:</strong></p>
                            <div class="arch-diagram">
Step 1: Analyze Current Usage
─────────────────────────────────────────────────────────
| tstats count where index=* by index, sourcetype
| sort -count
| eventstats sum(count) as total
| eval percentage=round(count/total*100,2)
| table index sourcetype count percentage

# Identify top 10 sourcetypes consuming license
index=_internal source=*license_usage.log type=Usage
| stats sum(b) as bytes by idx, st
| eval GB=round(bytes/1024/1024/1024,2)
| sort -GB

Step 2: Categorize Data Value
─────────────────────────────────────────────────────────
HIGH VALUE (Keep full fidelity):
• Authentication logs
• Security alerts
• Firewall deny logs
• Endpoint detection

MEDIUM VALUE (Filter/aggregate):
• Firewall allow logs → Keep only summary stats
• Web proxy → Filter routine traffic, keep suspicious
• Performance metrics → Sample or aggregate

LOW VALUE (Reduce significantly):
• Debug logs → Route to null queue in production
• Heartbeat/health checks → Drop or sample
• Verbose audit trails → Keep summaries only

Step 3: Implement Reduction Techniques
─────────────────────────────────────────────────────────
# Null queue for debug logs (transforms.conf)
[setnull]
REGEX = .
DEST_KEY = queue
FORMAT = nullQueue

# In props.conf - route debug to null
[sourcetype:app:verbose]
TRANSFORMS-null = setnull

# Aggregation at forwarder (Heavy Forwarder)
# Aggregate firewall allows every 5 minutes
| stats count by src_ip, dest_ip, dest_port, action
| where count > 100

# Event sampling for high-volume sources
[monitor:///var/log/verbose.log]
_INDEX_AND_FORWARD_ROUTING = true
TRANSFORMS-sample = sample_1_in_10

Step 4: Results Tracking
─────────────────────────────────────────────────────────
Before: 500 GB/day
After reduction:
• Debug logs null queued: -50 GB
• Firewall allows aggregated: -100 GB
• Verbose logs sampled: -50 GB
• Total: 300 GB/day (40% reduction achieved)
                            </div>
                            <p><strong>Trade-offs:</strong> Must balance cost savings against detection capability. Document what's being reduced and get security team sign-off. Keep raw data for high-value sources.</p>
                        </div>
                    </div>

                    <div class="qa-item">
                        <button class="qa-question">Q: Explain the difference between index-time and search-time field extractions. When would you use each?</button>
                        <div class="qa-answer">
                            <p><strong>Comparison:</strong></p>
                            <div class="arch-diagram">
INDEX-TIME EXTRACTIONS
═══════════════════════════════════════════════════════════
When it happens: During data ingestion, before data is written to index
Where configured: props.conf with TRANSFORMS pointing to transforms.conf
Storage impact: Extracted fields stored in index (increases storage)
Performance: Fast at search time - fields already exist
Flexibility: Cannot change without re-indexing data

Use cases:
• Fields needed for routing (DEST_KEY = _MetaData:Index)
• Fields that will be searched frequently
• Calculated fields that are expensive to compute
• Data reduction/masking before storage

Example - Index-time extraction:
[source::/var/log/app.log]
TRANSFORMS-extract = extract_user_id

[transforms.conf]
[extract_user_id]
REGEX = user_id=(\d+)
FORMAT = user_id::$1
WRITE_META = true

SEARCH-TIME EXTRACTIONS
═══════════════════════════════════════════════════════════
When it happens: During search execution
Where configured: props.conf with EXTRACT or REPORT
Storage impact: None - fields computed on the fly
Performance: Slower - regex runs on every search
Flexibility: Can modify anytime, immediately effective

Use cases:
• Most field extractions (default recommendation)
• Fields with evolving formats
• Fields not frequently searched
• When storage cost is a concern

Example - Search-time extraction:
[sourcetype:myapp]
EXTRACT-user = user_id=(?P<user_id>\d+)
# Or using transforms:
REPORT-user = extract_user_id

DECISION MATRIX
═══════════════════════════════════════════════════════════
Use INDEX-TIME when:          │ Use SEARCH-TIME when:
───────────────────────────────┼───────────────────────────
• Routing by field value      │ • Field format may change
• Field searched in 90%+      │ • Field rarely searched
  of queries                  │ • Storage is expensive
• Complex regex (expensive)   │ • Need flexibility
• Data masking required       │ • Standard extractions
• Metric/summary data        │ • Most typical use cases
                            </div>
                            <p><strong>Best practice:</strong> Default to search-time. Only use index-time when there's a specific need (routing, performance-critical fields, data masking). Index-time extractions increase storage and reduce flexibility.</p>
                        </div>
                    </div>

                    <div class="qa-item">
                        <button class="qa-question">Q: You're seeing data latency - events are arriving 30 minutes late. How do you troubleshoot?</button>
                        <div class="qa-answer">
                            <p><strong>Systematic Troubleshooting:</strong></p>
                            <div class="arch-diagram">
Step 1: Identify Where Latency Occurs
─────────────────────────────────────────────────────────
# Calculate latency at each stage
index=_internal source=*metrics.log group=per_sourcetype_thruput
| eval latency_seconds = _time - _indextime
| stats avg(latency_seconds) as avg_latency by series
| sort -avg_latency

# Check for specific sourcetype
index=<yourindex> sourcetype=<sourcetype>
| eval indextime=strftime(_indextime,"%Y-%m-%d %H:%M:%S")
| eval eventtime=strftime(_time,"%Y-%m-%d %H:%M:%S")
| eval lag_seconds=_indextime-_time
| table _time eventtime indextime lag_seconds

Step 2: Source-Side Investigation
─────────────────────────────────────────────────────────
Common causes at source:
• Log rotation delay: Files not flushed frequently
• NFS/network mount latency
• Application buffering before write

Validation:
# On source server
ls -la /var/log/app.log  # Check last modified time
tail -f /var/log/app.log  # Verify real-time writes

Step 3: Forwarder Investigation
─────────────────────────────────────────────────────────
# Check forwarder queue backlog
index=_internal host=<forwarder> source=*metrics.log group=queue
| timechart avg(current_size_kb) by name

# Check outputs.conf connection status
splunk btool outputs list --debug

# Verify forwarder internal logs
index=_internal host=<forwarder> source=*splunkd.log
| search "connection" OR "blocked" OR "queue"

Step 4: Network/Indexer Investigation
─────────────────────────────────────────────────────────
# Check receiving indexer
index=_internal source=*metrics.log host=<indexer> group=per_source_thruput
| timechart sum(kb) by series

# Check indexer queue status
index=_internal source=*metrics.log host=<indexer> group=queue
| where name="parsingQueue" OR name="indexQueue"
| timechart avg(current_size_kb) by name

Step 5: Time Parsing Investigation
─────────────────────────────────────────────────────────
# Is timestamp being parsed correctly?
index=<yourindex> sourcetype=<sourcetype>
| head 100
| eval diff=_indextime-_time
| where diff > 1800  # 30 min in seconds
| table _raw _time _indextime diff

# Common time parsing issues:
• Wrong timezone: TZ not set in props.conf
• Wrong timestamp format: TIME_FORMAT mismatch
• Future timestamps: Source clock skew
• Timestamp in wrong field: TIME_PREFIX needed

FIX EXAMPLE:
[sourcetype:myapp]
TIME_FORMAT = %Y-%m-%dT%H:%M:%S.%3N%z
TIME_PREFIX = timestamp=
TZ = America/New_York
MAX_TIMESTAMP_LOOKAHEAD = 30
                            </div>
                            <p><strong>Most common causes:</strong> 1) Source clock skew, 2) Incorrect TIME_FORMAT, 3) Missing TZ setting, 4) Forwarder queue backlog. Always trace the data path from source to indexer.</p>
                        </div>
                    </div>

                    <div class="qa-item">
                        <button class="qa-question">Q: How would you set up HEC for a high-volume application sending 50,000 events per second?</button>
                        <div class="qa-answer">
                            <p><strong>Enterprise HEC Architecture:</strong></p>
                            <div class="arch-diagram">
ARCHITECTURE
═══════════════════════════════════════════════════════════

Application Cluster          Load Balancer          Indexer Cluster
┌─────────────────┐         ┌───────────┐          ┌─────────────┐
│  App Server 1   │────────►│           │          │  Indexer 1  │
│  App Server 2   │────────►│    F5     │─────────►│  Indexer 2  │
│  App Server 3   │────────►│    or     │─────────►│  Indexer 3  │
│  App Server N   │────────►│   HAProxy │─────────►│  Indexer 4  │
└─────────────────┘         └───────────┘          │  Indexer 5  │
                                                    └─────────────┘

INDEXER CONFIGURATION (inputs.conf on each indexer)
═══════════════════════════════════════════════════════════

[http]
disabled = 0
port = 8088
enableSSL = 1
serverCert = /opt/splunk/etc/auth/server.pem
sslPassword = <encrypted>
maxThreads = 10
maxSockets = 100
dedicatedIoThreads = 2

[http://high_volume_token]
disabled = 0
token = <guid>
index = application
sourcetype = app:events
indexes = application,application_errors
useACK = 1                    # Enable indexer acknowledgment
connection_pool_size = 10

LOAD BALANCER CONFIGURATION (HAProxy example)
═══════════════════════════════════════════════════════════

frontend hec_frontend
    bind *:8088 ssl crt /etc/ssl/splunk-hec.pem
    default_backend hec_backend
    option httplog
    timeout client 30s

backend hec_backend
    balance roundrobin
    option httpchk GET /services/collector/health
    http-check expect status 200
    
    server indexer1 10.0.1.1:8088 check ssl verify none
    server indexer2 10.0.1.2:8088 check ssl verify none
    server indexer3 10.0.1.3:8088 check ssl verify none
    server indexer4 10.0.1.4:8088 check ssl verify none
    server indexer5 10.0.1.5:8088 check ssl verify none

APPLICATION BEST PRACTICES
═══════════════════════════════════════════════════════════

// Batch events (don't send 1 event per request)
{
  "event": {"message": "event 1", "severity": "info"},
  "time": 1640000001,
  "sourcetype": "app:events"
}
{
  "event": {"message": "event 2", "severity": "warn"},
  "time": 1640000002,
  "sourcetype": "app:events"
}

// Send 100-500 events per batch
// Target 1-5MB per request
// Implement retry with exponential backoff
// Handle 503 responses (indexer busy)

MONITORING
═══════════════════════════════════════════════════════════

# HEC health metrics
index=_internal sourcetype=splunkd_access method=POST uri_path="/services/collector*"
| timechart span=1m count by status

# Throughput monitoring
index=_internal source=*metrics.log group=per_sourcetype_thruput series=app:events
| timechart span=1m sum(kb) as KB_per_min
| eval expected_min=50000*0.5/1024  # 50K EPS * 0.5KB avg event
| eval health=if(KB_per_min > expected_min*0.9, "OK", "DEGRADED")
                            </div>
                            <p><strong>Key considerations:</strong> Enable useACK for guaranteed delivery, implement proper retry logic in app, size thread/socket pools for expected load, monitor HEC health actively.</p>
                        </div>
                    </div>

                    <div class="qa-item">
                        <button class="qa-question">Q: What's your approach to managing Universal Forwarders at scale (10,000+ endpoints)?</button>
                        <div class="qa-answer">
                            <p><strong>Enterprise Forwarder Management:</strong></p>
                            <div class="arch-diagram">
ARCHITECTURE
═══════════════════════════════════════════════════════════

                    ┌──────────────────────┐
                    │  Deployment Server   │
                    │   (Primary + DR)     │
                    └──────────────────────┘
                              │
            ┌─────────────────┼─────────────────┐
            ▼                 ▼                 ▼
    ┌──────────────┐  ┌──────────────┐  ┌──────────────┐
    │ ServerClass: │  │ ServerClass: │  │ ServerClass: │
    │   Windows    │  │    Linux     │  │   Network    │
    │   Servers    │  │   Servers    │  │   Devices    │
    └──────────────┘  └──────────────┘  └──────────────┘

DEPLOYMENT SERVER CONFIG (serverclass.conf)
═══════════════════════════════════════════════════════════

[global]
repositoryLocation = /opt/splunk/etc/deployment-apps
targetRepositoryLocation = /opt/splunk/etc/apps

# Windows Domain Controllers
[serverClass:windows_dc]
whitelist.0 = *-dc-*
whitelist.1 = *-domaincontroller-*
[serverClass:windows_dc:app:TA-windows-dc]
restartSplunkWeb = false
restartSplunkd = true
stateOnClient = enabled

# Windows Servers (general)
[serverClass:windows_servers]
machineTypesFilter = windows-x64
blacklist.0 = *-dc-*
[serverClass:windows_servers:app:Splunk_TA_windows]
[serverClass:windows_servers:app:outputs_windows]

# Linux Servers
[serverClass:linux_servers]
machineTypesFilter = linux-x86_64
[serverClass:linux_servers:app:Splunk_TA_nix]
[serverClass:linux_servers:app:outputs_linux]

FORWARDER INVENTORY & MONITORING
═══════════════════════════════════════════════════════════

# Forwarder inventory dashboard
index=_internal sourcetype=splunkd group=tcpin_connections
| stats latest(version) as version,
        latest(arch) as arch,
        latest(os) as os,
        max(_time) as last_seen
        by hostname
| eval days_silent = round((now()-last_seen)/86400,1)
| eval status = case(
    days_silent < 0.1, "Active",
    days_silent < 1, "Warning",
    days_silent < 7, "Silent",
    true(), "Orphan")
| stats count by status

# Forwarders not phoning home (alert)
index=_internal sourcetype=splunkd group=tcpin_connections
| stats max(_time) as last_seen by hostname
| where last_seen < relative_time(now(), "-24h")
| table hostname last_seen

# Version compliance
index=_internal sourcetype=splunkd group=tcpin_connections
| stats latest(version) by hostname
| eval compliant = if(version >= "9.0.0", "Yes", "No")
| stats count by compliant

SCALING CONSIDERATIONS
═══════════════════════════════════════════════════════════

For 10,000+ forwarders:

1. Multiple Deployment Servers
   • Geographic distribution (NA, EU, APAC)
   • Max 5,000 clients per deployment server
   • Use DNS round-robin or load balancer

2. Phone Home Interval
   [deployment-client]
   phoneHomeIntervalInSecs = 600  # 10 min for large deployments
   # Default 60 seconds creates unnecessary load

3. Serverclass Best Practices
   • Use specific whitelists, not broad matches
   • Avoid overlapping serverclasses
   • Test changes in dev serverclass first

4. Outputs Configuration
   • Use indexer discovery for dynamic cluster membership
   • Multiple output groups for resilience
   • Persistent queue for critical data

[tcpout]
defaultGroup = primary_indexers
indexerDiscovery = cluster_master

[indexer_discovery:cluster_master]
master_uri = https://cm.company.com:8089

[tcpout:primary_indexers]
indexerDiscovery = cluster_master
useACK = true
                            </div>
                            <p><strong>Key success factors:</strong> Automate deployment server management, implement forwarder inventory tracking, alert on silent forwarders, maintain version compliance, use indexer discovery for cluster awareness.</p>
                        </div>
                    </div>

                    <div class="qa-item">
                        <button class="qa-question">Q: How do you ensure data quality and validate that ingested data is correct?</button>
                        <div class="qa-answer">
                            <p><strong>Data Quality Framework:</strong></p>
                            <div class="arch-diagram">
DATA QUALITY DIMENSIONS
═══════════════════════════════════════════════════════════

1. COMPLETENESS - Is all expected data arriving?
2. TIMELINESS - Is data arriving within SLA?
3. ACCURACY - Are fields parsed correctly?
4. CONSISTENCY - Is format consistent across sources?
5. VALIDITY - Does data conform to expected schema?

VALIDATION APPROACH
═══════════════════════════════════════════════════════════

1. Golden Source Checks (Completeness)
────────────────────────────────────────
# Expected vs actual event count per source
| tstats count where index=* by index, sourcetype, host
| lookup expected_data_rates.csv sourcetype OUTPUT expected_hourly
| eval deviation = abs(count - expected_hourly) / expected_hourly * 100
| where deviation > 20
| table index sourcetype host count expected_hourly deviation

# Alert: Data source went silent
| tstats latest(_time) as last_event where index=* by sourcetype, host
| where last_event < relative_time(now(), "-1h")
| table sourcetype host last_event

2. Latency Monitoring (Timeliness)
────────────────────────────────────────
# Data latency by sourcetype
index=_internal source=*metrics.log group=per_sourcetype_thruput
| eval latency = _indextime - _time
| stats avg(latency) as avg_latency_sec,
        perc95(latency) as p95_latency_sec
        by series
| where avg_latency_sec > 300  # 5 min threshold
| table series avg_latency_sec p95_latency_sec

3. Parse Quality (Accuracy)
────────────────────────────────────────
# Events with timestamp parsing issues
index=* 
| where _time < relative_time(now(), "-1y") OR _time > now()
| stats count by sourcetype
| where count > 0
| rename count as "Bad Timestamps"

# Events with truncation
index=_internal source=*splunkd.log "truncating"
| stats count by host, source

# Line breaking issues
index=* 
| eval event_length = len(_raw)
| where event_length > 10000 OR event_length < 10
| stats count by sourcetype
| table sourcetype count

4. Field Extraction Validation (Accuracy)
────────────────────────────────────────
# Check critical fields are populated
index=<index> sourcetype=<sourcetype>
| stats count(eval(isnull(user))) as missing_user,
        count(eval(isnull(src_ip))) as missing_src,
        count(eval(isnull(action))) as missing_action,
        count as total
| eval user_coverage = round((total-missing_user)/total*100,1)
| eval src_coverage = round((total-missing_src)/total*100,1)
| eval action_coverage = round((total-missing_action)/total*100,1)

5. CIM Compliance (Consistency)
────────────────────────────────────────
# Validate data appears in data model
| tstats count from datamodel=Authentication by sourcetype
| lookup expected_authentication_sources.csv sourcetype OUTPUT expected
| where isnull(expected) OR count < expected * 0.8
| table sourcetype count expected

AUTOMATED QUALITY DASHBOARD
═══════════════════════════════════════════════════════════

Create saved searches that run hourly:
• data_quality_completeness - Check all sources reporting
• data_quality_latency - Alert on >5min average latency
• data_quality_parsing - Detect timestamp/truncation issues
• data_quality_cim - Verify CIM field coverage

Roll up into single Data Quality Score:
Score = (Completeness * 0.4) + (Timeliness * 0.3) + (Accuracy * 0.3)
Target: >95% quality score
                            </div>
                            <p><strong>Best practice:</strong> Implement data quality monitoring from day one. Build dashboards showing health by sourcetype. Alert on degradation before it impacts detection capabilities.</p>
                        </div>
                    </div>

                    <div class="qa-item">
                        <button class="qa-question">Q: Describe how you'd migrate data ingestion from an on-prem Splunk to Splunk Cloud.</button>
                        <div class="qa-answer">
                            <p><strong>Cloud Migration Strategy:</strong></p>
                            <div class="arch-diagram">
PHASE 1: ASSESSMENT (2-4 weeks)
═══════════════════════════════════════════════════════════

Inventory current state:
• Total daily volume (license usage reports)
• Sourcetype breakdown (what data, how much)
• Forwarder inventory (count, versions, locations)
• Custom TAs and apps
• Inputs methods (forwarders, HEC, syslog, API)

Splunk Cloud constraints:
• No direct syslog to cloud (must use forwarder)
• HEC available but may need SC4S for syslog
• Some TAs not cloud-compatible
• Data must traverse internet (or Private Link)

PHASE 2: ARCHITECTURE DESIGN
═══════════════════════════════════════════════════════════

HYBRID ARCHITECTURE (Recommended)

On-Premises                         Splunk Cloud
┌─────────────────────┐            ┌─────────────────────┐
│   Data Sources      │            │   Splunk Cloud      │
│   ┌───────────────┐ │            │   Environment       │
│   │ Universal     │ │            │                     │
│   │ Forwarders    │─┼──────────►│   Indexers (IDM)    │
│   └───────────────┘ │            │                     │
│   ┌───────────────┐ │    9997    │   Search Heads      │
│   │ Heavy        │─┼──────────►│                     │
│   │ Forwarders   │ │            │                     │
│   │ (syslog rx)  │ │            │                     │
│   └───────────────┘ │            │                     │
│   ┌───────────────┐ │    HEC     │                     │
│   │ Cloud Apps   │─┼──────────►│                     │
│   │ (via HEC)    │ │            │                     │
│   └───────────────┘ │            └─────────────────────┘
└─────────────────────┘

KEY DECISIONS:
• Heavy Forwarders on-prem for syslog aggregation
• Direct UF → Cloud for endpoints (ports 9997-9999)
• HEC for application/API-based ingestion
• Consider AWS/Azure Private Link for security

PHASE 3: MIGRATION EXECUTION
═══════════════════════════════════════════════════════════

Week 1-2: Infrastructure Setup
• Deploy Splunk Cloud stack
• Configure Private Link (if required)
• Set up IDM inputs (HEC, S2S)
• Test connectivity from on-prem

Week 3-4: TA Migration
• Export custom TAs from on-prem
• Validate cloud compatibility
• Submit TAs via Splunk Cloud admin
• Test parsing in cloud environment

Week 5-8: Phased Data Migration
Approach: Migrate by sourcetype, not all at once

# Phase 1: Low-risk, high-value
- Windows Event Logs (well-defined, TA exists)
- Network device syslog via Heavy Forwarder

# Phase 2: Medium complexity
- Application logs via HEC
- Cloud service data (AWS, Azure, O365)

# Phase 3: Complex/Custom
- Custom applications
- Legacy systems

# Outputs.conf change (on forwarders)
[tcpout]
defaultGroup = splunk_cloud

[tcpout:splunk_cloud]
server = inputs1.splunkcloud.com:9997,inputs2.splunkcloud.com:9997
sslCertPath = /opt/splunk/etc/auth/cloud_cert.pem

PHASE 4: VALIDATION & CUTOVER
═══════════════════════════════════════════════════════════

Parallel run period (2-4 weeks):
• Send data to both on-prem and cloud
• Compare: event counts, parsing, dashboards
• Validate detection rules work correctly

Cutover checklist:
□ All sourcetypes migrated and validated
□ Dashboards rebuilt/migrated
□ Saved searches/alerts migrated  
□ User access configured
□ Detection rules tested
□ Runbooks updated
□ On-prem decommission plan

Post-migration:
• Monitor cloud ingestion health
• Decommission on-prem indexers
• Update documentation
                            </div>
                            <p><strong>Key risks:</strong> Network latency/bandwidth for high-volume sources, TA compatibility issues, detection rule migration gaps. Always parallel run before full cutover.</p>
                        </div>
                    </div>
                </div>
                <h2><i class="fas fa-link"></i> Related Resources</h2>
                <ul>
                    <li><a href="forwarders.html">Forwarders Deep Dive</a></li>
                    <li><a href="custom-log-onboarding.html">Custom Log Onboarding</a></li>
                    <li><a href="parsing-flows.html">Parsing & Props/Transforms</a></li>
                    <li><a href="retention-tiers.html">Index & Retention Management</a></li>
                    <li><a href="data-models.html">Data Models & CIM</a></li>
                </ul>
            </main>
        </div>
    </div>
        </div>
    </div>
    
    <script>
        function toggleSidebar() {
            const sidebar = document.getElementById('sidebar');
            const mainWrapper = document.getElementById('mainWrapper');
            const overlay = document.getElementById('sidebarOverlay');
            
            if (window.innerWidth <= 768) {
                sidebar.classList.toggle('open');
                overlay.classList.toggle('active');
            } else {
                sidebar.classList.toggle('collapsed');
                mainWrapper.classList.toggle('expanded');
            }
        }
        
        // Q&A toggle
        document.querySelectorAll('.qa-question').forEach(btn => {
            btn.addEventListener('click', () => {
                const answer = btn.nextElementSibling;
                const isOpen = answer.style.display === 'block';
                document.querySelectorAll('.qa-answer').forEach(a => a.style.display = 'none');
                if (!isOpen) answer.style.display = 'block';
            });
        });
        
        // Close sidebar on resize
        window.addEventListener('resize', () => {
            const sidebar = document.getElementById('sidebar');
            const overlay = document.getElementById('sidebarOverlay');
            if (window.innerWidth > 768) {
                sidebar.classList.remove('open');
                overlay.classList.remove('active');
            }
        });
    </script>
</body>
</html>
