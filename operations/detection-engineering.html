<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Detection Engineering | SOC Operations</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    <link rel="stylesheet" href="../assets/css/main.css">
    <style>
        .config-section { background: #161b22; border: 1px solid #30363d; border-radius: 8px; padding: 20px; margin: 16px 0; }
        .code-block { background: #0d1117; border: 1px solid #30363d; border-radius: 8px; padding: 16px; margin: 12px 0; font-family: 'Fira Code', monospace; font-size: 0.85rem; overflow-x: auto; white-space: pre; }
        .lifecycle-step { display: flex; align-items: center; gap: 16px; padding: 16px; background: #21262d; border-radius: 8px; margin: 12px 0; }
        .step-num { width: 40px; height: 40px; background: #58a6ff; color: #0d1117; border-radius: 50%; display: flex; align-items: center; justify-content: center; font-weight: 700; flex-shrink: 0; }
        .mitre-tag { background: #1f6feb; padding: 2px 8px; border-radius: 4px; font-size: 0.75rem; color: white; margin-right: 4px; }
    </style>
</head>
<body>
    <div class="app-container">
                                                                                <aside class="sidebar">
            <div class="sidebar-header">
                <a href="../index.html" class="sidebar-brand">
                    <div class="brand-icon"><i class="fas fa-shield-halved"></i></div>
                    <div class="brand-text">
                        <span class="brand-title">Nik's SIEM</span>
                        <span class="brand-subtitle">Security Operations</span>
                    </div>
                </a>
            </div>
            <nav class="sidebar-nav">
                <div class="nav-section">
                    <div class="nav-section-title">SOC Operations</div>
                    <a href="index.html" class="nav-link"><i class="fas fa-home"></i> Overview</a>
                    <a href="soc-operations.html" class="nav-link"><i class="fas fa-users"></i> SOC Structure</a>
                    <a href="shift-handoff.html" class="nav-link"><i class="fas fa-exchange-alt"></i> Shift Handoff</a>
                    <a href="escalation-matrix.html" class="nav-link"><i class="fas fa-level-up-alt"></i> Escalation Matrix</a>
                </div>
                <div class="nav-section">
                    <div class="nav-section-title">Incident Response</div>
                    <a href="triage-methodology.html" class="nav-link"><i class="fas fa-clipboard-check"></i> Triage Methodology</a>
                    <a href="triage-handbook.html" class="nav-link"><i class="fas fa-book"></i> Triage Handbook</a>
                    <a href="incident-classification.html" class="nav-link"><i class="fas fa-tags"></i> Incident Classification</a>
                    <a href="containment-procedures.html" class="nav-link"><i class="fas fa-shield-alt"></i> Containment</a>
                    <a href="playbook-library.html" class="nav-link"><i class="fas fa-book-open"></i> Playbook Library</a>
                    <a href="ir-integration.html" class="nav-link"><i class="fas fa-link"></i> IR Integration</a>
                </div>
                <div class="nav-section">
                    <div class="nav-section-title">Detection Engineering</div>
                    <a href="detection-engineering.html" class="nav-link active"><i class="fas fa-cogs"></i> Detection Engineering</a>
                    <a href="use-case-lifecycle.html" class="nav-link"><i class="fas fa-recycle"></i> Use Case Lifecycle</a>
                    <a href="tuning-optimization.html" class="nav-link"><i class="fas fa-sliders-h"></i> Tuning & Optimization</a>
                    <a href="mitre-mapping.html" class="nav-link"><i class="fas fa-map"></i> MITRE Mapping</a>
                </div>
                <div class="nav-section">
                    <div class="nav-section-title">Metrics & Compliance</div>
                    <a href="metrics-kpis.html" class="nav-link"><i class="fas fa-chart-line"></i> Metrics & KPIs</a>
                    <a href="soc-metrics.html" class="nav-link"><i class="fas fa-tachometer-alt"></i> SOC Metrics</a>
                    <a href="compliance-mapping.html" class="nav-link"><i class="fas fa-check-double"></i> Compliance Mapping</a>
                    <a href="audit-preparation.html" class="nav-link"><i class="fas fa-clipboard-list"></i> Audit Preparation</a>
                </div>
                <div class="nav-section">
                    <div class="nav-section-title">Planning & Design</div>
                    <a href="siem-project-planning.html" class="nav-link"><i class="fas fa-project-diagram"></i> SIEM Project Planning</a>
                    <a href="data-source-priority.html" class="nav-link"><i class="fas fa-sort-amount-down"></i> Data Source Priority</a>
                    <a href="sizing-calculator.html" class="nav-link"><i class="fas fa-calculator"></i> Sizing Calculator</a>
                    <a href="rbac-design.html" class="nav-link"><i class="fas fa-user-lock"></i> RBAC Design</a>
                    <a href="migration-guide.html" class="nav-link"><i class="fas fa-truck-moving"></i> Migration Guide</a>
                </div>
            </nav>
        </aside>
        <div class="main-wrapper">
            <header class="top-bar">
                <div class="breadcrumb">
                    <a href="../index.html">Home</a><span class="separator">/</span>
                    <a href="index.html">Operations</a><span class="separator">/</span>
                    <span class="current">Detection Engineering</span>
                </div>
            </header>
            <main class="main-content">
                <h1><i class="fas fa-shield-alt"></i> Detection Engineering</h1>
                <p class="lead">Build effective security detections that identify real threats while minimizing false positives. This guide covers the full detection lifecycle from threat modeling to production deployment.</p>

                <!-- What is Detection Engineering -->
                <h2><i class="fas fa-info-circle"></i> What is Detection Engineering?</h2>
                <div class="config-section">
                    <p>Detection Engineering is the practice of designing, building, testing, and maintaining security detections. It bridges the gap between threat intelligence and operational security monitoring.</p>
                    
                    <h4>Core Responsibilities</h4>
                    <ul style="color: #8b949e;">
                        <li><strong>Threat Research:</strong> Understand attacker techniques and behaviors</li>
                        <li><strong>Detection Development:</strong> Write rules that identify malicious activity</li>
                        <li><strong>Testing & Validation:</strong> Ensure detections work as intended</li>
                        <li><strong>Tuning & Optimization:</strong> Reduce false positives, improve true positive rate</li>
                        <li><strong>Documentation:</strong> Document detection logic and response procedures</li>
                        <li><strong>Coverage Analysis:</strong> Identify gaps in detection coverage</li>
                    </ul>

                    <h4>Detection Engineering vs Traditional SOC Roles</h4>
                    <table class="styled-table">
                        <thead>
                            <tr><th>Aspect</th><th>SOC Analyst</th><th>Detection Engineer</th></tr>
                        </thead>
                        <tbody>
                            <tr><td>Focus</td><td>Responding to alerts</td><td>Creating and tuning alerts</td></tr>
                            <tr><td>Workflow</td><td>Reactive (alert-driven)</td><td>Proactive (threat-driven)</td></tr>
                            <tr><td>Output</td><td>Incident tickets, reports</td><td>Detection rules, playbooks</td></tr>
                            <tr><td>Skills</td><td>Investigation, triage</td><td>Coding, threat modeling</td></tr>
                        </tbody>
                    </table>
                </div>

                <!-- Detection Lifecycle -->
                <h2><i class="fas fa-sync-alt"></i> Detection Development Lifecycle</h2>
                <div class="config-section">
                    <div class="lifecycle-step">
                        <div class="step-num">1</div>
                        <div>
                            <h4 style="margin: 0;">Threat Research</h4>
                            <p style="color: #8b949e; margin: 4px 0 0 0;">Identify the threat to detect. Sources: threat intel, MITRE ATT&CK, incident reports, red team findings.</p>
                        </div>
                    </div>
                    <div class="lifecycle-step">
                        <div class="step-num">2</div>
                        <div>
                            <h4 style="margin: 0;">Data Requirements</h4>
                            <p style="color: #8b949e; margin: 4px 0 0 0;">Determine what data sources and fields are needed. Validate data is being collected.</p>
                        </div>
                    </div>
                    <div class="lifecycle-step">
                        <div class="step-num">3</div>
                        <div>
                            <h4 style="margin: 0;">Detection Logic</h4>
                            <p style="color: #8b949e; margin: 4px 0 0 0;">Write the detection rule. Define conditions, thresholds, and time windows.</p>
                        </div>
                    </div>
                    <div class="lifecycle-step">
                        <div class="step-num">4</div>
                        <div>
                            <h4 style="margin: 0;">Testing</h4>
                            <p style="color: #8b949e; margin: 4px 0 0 0;">Validate with test data. Simulate the attack. Check for false positives in historical data.</p>
                        </div>
                    </div>
                    <div class="lifecycle-step">
                        <div class="step-num">5</div>
                        <div>
                            <h4 style="margin: 0;">Documentation</h4>
                            <p style="color: #8b949e; margin: 4px 0 0 0;">Document detection logic, MITRE mapping, response procedures, and known limitations.</p>
                        </div>
                    </div>
                    <div class="lifecycle-step">
                        <div class="step-num">6</div>
                        <div>
                            <h4 style="margin: 0;">Deployment</h4>
                            <p style="color: #8b949e; margin: 4px 0 0 0;">Deploy to production. Start in test/low severity mode. Monitor for issues.</p>
                        </div>
                    </div>
                    <div class="lifecycle-step">
                        <div class="step-num">7</div>
                        <div>
                            <h4 style="margin: 0;">Continuous Improvement</h4>
                            <p style="color: #8b949e; margin: 4px 0 0 0;">Tune based on feedback. Update for new attack variants. Retire ineffective rules.</p>
                        </div>
                    </div>
                </div>

                <!-- Threat Modeling for Detection -->
                <h2><i class="fas fa-crosshairs"></i> Threat Modeling for Detection</h2>
                <div class="config-section">
                    <h4>Approach 1: MITRE ATT&CK-Driven</h4>
                    <p style="color: #8b949e;">Start with MITRE technique, identify observable behaviors:</p>
                    <div class="code-block">Example: T1059.001 - PowerShell

Observable Behaviors:
├── Process Creation
│   ├── powershell.exe or pwsh.exe execution
│   ├── Command line parameters (-enc, -NoP, -W Hidden)
│   └── Parent process (unusual parents = suspicious)
│
├── Script Block Logging
│   ├── Event ID 4104 (Script Block)
│   ├── Encoded commands
│   └── Known malicious patterns
│
├── Network Activity
│   ├── PowerShell making outbound connections
│   ├── Download cradles (Invoke-WebRequest, Net.WebClient)
│   └── Connections to rare domains
│
└── File System
    ├── PowerShell writing to suspicious locations
    └── Script files created in temp directories</div>

                    <h4>Approach 2: Attack Narrative-Driven</h4>
                    <p style="color: #8b949e;">Map the full attack chain, detect at multiple points:</p>
                    <div class="code-block">Attack: Phishing → Macro Execution → PowerShell Download → Persistence

Detection Points:
1. Initial Access: Email with macro attachment (email gateway)
2. Execution: Office spawning cmd/PowerShell (endpoint)
3. Command & Control: PowerShell outbound connection (network/endpoint)
4. Persistence: Scheduled task creation (endpoint)

Key Insight: Detect at MULTIPLE points for defense in depth</div>

                    <h4>Approach 3: Data Source-Driven</h4>
                    <p style="color: #8b949e;">Start with available data, identify detectable threats:</p>
                    <div class="code-block">Available Data: Windows Security Events

High-Value Events:
├── 4624/4625: Logon Success/Failure → Brute force, lateral movement
├── 4688: Process Creation → Execution detection (needs command line)
├── 4698: Scheduled Task Created → Persistence
├── 4720: User Account Created → Account manipulation
├── 4728/4732: User Added to Group → Privilege escalation
└── 1102: Audit Log Cleared → Defense evasion</div>
                </div>

                <!-- Detection Types -->
                <h2><i class="fas fa-layer-group"></i> Types of Detections</h2>
                <div class="config-section">
                    <table class="styled-table">
                        <thead>
                            <tr><th>Type</th><th>Description</th><th>Example</th><th>Pros/Cons</th></tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td><strong>Signature</strong></td>
                                <td>Match known IOCs</td>
                                <td>File hash, IP address, domain</td>
                                <td>✅ Low FP, ❌ Easy to evade</td>
                            </tr>
                            <tr>
                                <td><strong>Behavioral</strong></td>
                                <td>Detect patterns of activity</td>
                                <td>Process chain, sequence of events</td>
                                <td>✅ Harder to evade, ❌ More FP</td>
                            </tr>
                            <tr>
                                <td><strong>Threshold</strong></td>
                                <td>Trigger on volume</td>
                                <td>>10 failed logins in 5 minutes</td>
                                <td>✅ Simple, ❌ Threshold tuning</td>
                            </tr>
                            <tr>
                                <td><strong>Anomaly</strong></td>
                                <td>Deviation from baseline</td>
                                <td>Unusual login time, location</td>
                                <td>✅ Unknown threats, ❌ Baseline drift</td>
                            </tr>
                            <tr>
                                <td><strong>Correlation</strong></td>
                                <td>Multiple conditions together</td>
                                <td>Failed login + success + data access</td>
                                <td>✅ High confidence, ❌ Complex</td>
                            </tr>
                        </tbody>
                    </table>
                </div>

                <!-- Writing Effective Detection Rules -->
                <h2><i class="fas fa-pen"></i> Writing Effective Detection Rules</h2>
                <div class="config-section">
                    <h4>Rule Structure Best Practices</h4>
                    <div class="code-block">// GOOD: Specific, contextual detection
Process creation where:
  - Process is powershell.exe or pwsh.exe
  - Command line contains "-enc" or "-encodedcommand"
  - Parent process is NOT cmd.exe, explorer.exe, or known admin tools
  - User is not in "IT Admins" group

// BAD: Too broad, high false positives
Process creation where:
  - Process is powershell.exe
  
// GOOD: Threshold with context
Failed login where:
  - Count > 10 in 5 minutes
  - Same source IP
  - Against multiple user accounts (> 3)

// BAD: Simple threshold
Failed login where:
  - Count > 5</div>

                    <h4>Detection Rule Template</h4>
                    <div class="code-block">Rule Name: [Category] - [Technique] - [Behavior]
Example: "Execution - PowerShell - Encoded Command with Network Connection"

Detection Logic:
- Data Source: [Windows Event Log / Sysmon / EDR]
- Event Type: [Process Creation / Network Connection]
- Conditions:
  - Condition 1: [specific filter]
  - Condition 2: [specific filter]
  - AND/OR logic
- Threshold: [if applicable]
- Time Window: [if applicable]

MITRE Mapping:
- Tactic: [e.g., Execution]
- Technique: [e.g., T1059.001]
- Sub-technique: [if applicable]

Severity: [Critical / High / Medium / Low]

False Positive Sources:
- [Known legitimate use case 1]
- [Known legitimate use case 2]

Exclusions:
- [User/system exclusion 1]
- [User/system exclusion 2]

Response Actions:
- [Investigation step 1]
- [Investigation step 2]
- [Containment action if confirmed]</div>
                </div>

                <!-- Platform-Specific Examples -->
                <h2><i class="fas fa-code"></i> Platform-Specific Detection Examples</h2>
                <div class="config-section">
                    <h4>Splunk (SPL)</h4>
                    <div class="code-block">// Encoded PowerShell Detection
index=endpoint sourcetype=sysmon EventCode=1
| where process_name IN ("powershell.exe", "pwsh.exe")
| where match(CommandLine, "(?i)-e(nc|ncodedcommand)")
| where parent_image NOT IN ("explorer.exe", "cmd.exe", "sccm.exe")
| stats count by host, user, CommandLine, parent_image
| where count > 0</div>

                    <h4>Microsoft Sentinel (KQL)</h4>
                    <div class="code-block">// Encoded PowerShell Detection
DeviceProcessEvents
| where TimeGenerated > ago(1h)
| where FileName in~ ("powershell.exe", "pwsh.exe")
| where ProcessCommandLine matches regex @"(?i)-e(nc|ncodedcommand)"
| where InitiatingProcessFileName !in~ ("explorer.exe", "cmd.exe")
| project TimeGenerated, DeviceName, AccountName, ProcessCommandLine</div>

                    <h4>XSIAM (XQL)</h4>
                    <div class="code-block">// Encoded PowerShell Detection
dataset = xdr_data
| filter event_type = "process_execution"
| filter process_name in ("powershell.exe", "pwsh.exe")
| filter process_command_line ~= "(?i)-e(nc|ncodedcommand)"
| filter parent_process_name not in ("explorer.exe", "cmd.exe")
| fields _time, host_name, user_name, process_command_line</div>

                    <h4>Sigma Rule (Portable Format)</h4>
                    <div class="code-block">title: Encoded PowerShell Command Execution
id: f3b6b7f0-1234-5678-90ab-cdef01234567
status: production
description: Detects PowerShell with encoded command execution
references:
    - https://attack.mitre.org/techniques/T1059/001/
author: Detection Engineering Team
date: 2024/01/15
modified: 2024/06/01
tags:
    - attack.execution
    - attack.t1059.001
logsource:
    category: process_creation
    product: windows
detection:
    selection:
        Image|endswith:
            - '\powershell.exe'
            - '\pwsh.exe'
        CommandLine|contains:
            - '-enc'
            - '-encodedcommand'
    filter:
        ParentImage|endswith:
            - '\explorer.exe'
            - '\cmd.exe'
    condition: selection and not filter
falsepositives:
    - Administrative scripts
    - Software deployment tools
level: high</div>
                </div>

                <!-- Testing Detections -->
                <h2><i class="fas fa-flask"></i> Testing Detections</h2>
                <div class="config-section">
                    <h4>Testing Methods</h4>
                    <table class="styled-table">
                        <thead>
                            <tr><th>Method</th><th>Description</th><th>When to Use</th></tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td><strong>Historical Data</strong></td>
                                <td>Run detection against past logs</td>
                                <td>Check for false positives</td>
                            </tr>
                            <tr>
                                <td><strong>Atomic Red Team</strong></td>
                                <td>Execute specific technique tests</td>
                                <td>Validate true positive detection</td>
                            </tr>
                            <tr>
                                <td><strong>Purple Team</strong></td>
                                <td>Coordinated attack simulation</td>
                                <td>Full attack chain testing</td>
                            </tr>
                            <tr>
                                <td><strong>Log Replay</strong></td>
                                <td>Replay known-bad logs</td>
                                <td>Regression testing after changes</td>
                            </tr>
                        </tbody>
                    </table>

                    <h4>Atomic Red Team Testing</h4>
                    <div class="code-block"># Install Atomic Red Team
IEX (IWR 'https://raw.githubusercontent.com/redcanaryco/invoke-atomicredteam/master/install-atomicredteam.ps1' -UseBasicParsing);
Install-AtomicRedTeam -getAtomics

# Run specific technique test
Invoke-AtomicTest T1059.001 -TestNumbers 1

# List available tests for a technique
Invoke-AtomicTest T1059.001 -ShowDetailsBrief

# Run all tests for a technique
Invoke-AtomicTest T1059.001</div>

                    <h4>Testing Checklist</h4>
                    <ul style="color: #8b949e;">
                        <li>✅ Detection fires on simulated attack</li>
                        <li>✅ Alert contains necessary context for investigation</li>
                        <li>✅ Historical data shows acceptable false positive rate</li>
                        <li>✅ Detection works across different OS versions/configurations</li>
                        <li>✅ Query performance is acceptable (completes in reasonable time)</li>
                        <li>✅ Detection is resilient to minor attack variations</li>
                    </ul>
                </div>

                <!-- Tuning and Optimization -->
                <h2><i class="fas fa-sliders-h"></i> Tuning and Optimization</h2>
                <div class="config-section">
                    <h4>Common False Positive Sources</h4>
                    <table class="styled-table">
                        <thead>
                            <tr><th>Source</th><th>Solution</th></tr>
                        </thead>
                        <tbody>
                            <tr><td>Admin tools/scripts</td><td>Whitelist specific admin accounts or systems</td></tr>
                            <tr><td>Security scanners</td><td>Exclude scanner IPs/hostnames</td></tr>
                            <tr><td>Backup software</td><td>Exclude backup service accounts</td></tr>
                            <tr><td>Software deployment</td><td>Exclude deployment servers and time windows</td></tr>
                            <tr><td>Monitoring agents</td><td>Exclude monitoring tool processes</td></tr>
                        </tbody>
                    </table>

                    <h4>Tuning Strategies</h4>
                    <div class="code-block">// Strategy 1: Add context-based exclusions
WHERE user NOT IN (known_admin_accounts)
WHERE src_host NOT IN (known_scanner_hosts)
WHERE NOT (process = "backup.exe" AND parent = "scheduler.exe")

// Strategy 2: Increase specificity
// Instead of just PowerShell execution:
WHERE powershell.exe
  AND command_line CONTAINS "-enc"
  AND (downloads_file OR creates_scheduled_task)

// Strategy 3: Correlation
// Instead of single event:
WHERE failed_login_count > 10
  AND subsequent_successful_login
  AND accessed_sensitive_data

// Strategy 4: Baselining
// Compare to historical normal:
WHERE current_value > (baseline_avg + 3 * baseline_stdev)</div>

                    <h4>Metrics for Detection Health</h4>
                    <table class="styled-table">
                        <thead>
                            <tr><th>Metric</th><th>Target</th><th>Action if Miss</th></tr>
                        </thead>
                        <tbody>
                            <tr><td>True Positive Rate</td><td>> 80%</td><td>Increase detection specificity</td></tr>
                            <tr><td>False Positive Rate</td><td>< 20%</td><td>Add exclusions, increase thresholds</td></tr>
                            <tr><td>Alert Volume</td><td>Manageable</td><td>Adjust thresholds, add grouping</td></tr>
                            <tr><td>Detection Latency</td><td>< 15 min</td><td>Optimize query, increase frequency</td></tr>
                            <tr><td>Coverage (MITRE)</td><td>Priority techniques</td><td>Develop new detections</td></tr>
                        </tbody>
                    </table>
                </div>

                <!-- Detection Coverage Analysis -->
                <h2><i class="fas fa-th"></i> Detection Coverage Analysis</h2>
                <div class="config-section">
                    <h4>MITRE ATT&CK Coverage Mapping</h4>
                    <p style="color: #8b949e;">Map your detections to MITRE ATT&CK to identify gaps:</p>
                    <div class="code-block">Coverage Analysis Process:
1. Export all detection rules
2. Map each rule to MITRE technique(s)
3. Compare against threat model priorities
4. Identify gaps in coverage
5. Prioritize new detection development

Priority Techniques (typical enterprise):
- T1566: Phishing (Initial Access)
- T1059: Command and Scripting Interpreter (Execution)
- T1053: Scheduled Task/Job (Persistence)
- T1078: Valid Accounts (multiple tactics)
- T1021: Remote Services (Lateral Movement)
- T1003: OS Credential Dumping (Credential Access)
- T1486: Data Encrypted for Impact (Impact)</div>

                    <h4>Coverage by Data Source</h4>
                    <div class="code-block">Data Source Coverage Assessment:

Endpoint (Sysmon/EDR):
├── Process Creation: ████████░░ 80%
├── Network Connection: ██████░░░░ 60%
├── File Creation: █████░░░░░ 50%
├── Registry: ████░░░░░░ 40%
└── WMI: ██░░░░░░░░ 20%

Network (Firewall/Proxy):
├── DNS: ███████░░░ 70%
├── HTTP/S: ██████░░░░ 60%
├── Non-standard ports: ████░░░░░░ 40%
└── Encrypted traffic: ██░░░░░░░░ 20%

Identity (AD/Azure AD):
├── Authentication: ████████░░ 80%
├── Group Changes: ███████░░░ 70%
├── Account Changes: ██████░░░░ 60%
└── Permission Changes: ████░░░░░░ 40%</div>
                </div>

                <!-- Interview Questions -->
                <h2><i class="fas fa-comments"></i> Interview Questions</h2>
                <div class="qa-section">
                    <div class="qa-item">
                        <button class="qa-question"><i class="fas fa-lightbulb" style="color: #d29922;"></i> Q: Walk me through your process for developing a new detection rule.</button>
                        <div class="qa-answer">
                            <ol>
                                <li><strong>Understand the threat:</strong> Research the technique, understand attacker behavior, map to MITRE ATT&CK</li>
                                <li><strong>Identify data requirements:</strong> What logs/telemetry needed? Is that data being collected?</li>
                                <li><strong>Develop detection logic:</strong> Write the rule focusing on observable behaviors, not just IOCs</li>
                                <li><strong>Test thoroughly:</strong> Validate with Atomic Red Team, check historical data for false positives</li>
                                <li><strong>Document:</strong> MITRE mapping, response procedures, known limitations, exclusions</li>
                                <li><strong>Deploy incrementally:</strong> Start in test mode, monitor, then enable in production</li>
                                <li><strong>Iterate:</strong> Tune based on feedback, update for new variants</li>
                            </ol>
                        </div>
                    </div>
                    <div class="qa-item">
                        <button class="qa-question"><i class="fas fa-lightbulb" style="color: #d29922;"></i> Q: How do you reduce false positives in detection rules?</button>
                        <div class="qa-answer">
                            <ul>
                                <li><strong>Add context:</strong> Combine multiple conditions (process + command line + parent)</li>
                                <li><strong>Whitelist known-good:</strong> Exclude admin accounts, scanners, backup tools</li>
                                <li><strong>Increase specificity:</strong> Be more precise about what you're detecting</li>
                                <li><strong>Use correlation:</strong> Require multiple related events</li>
                                <li><strong>Baseline comparison:</strong> Alert on deviation from normal, not absolute values</li>
                                <li><strong>Time-based exclusions:</strong> Exclude during maintenance windows</li>
                                <li><strong>Regular review:</strong> Analyze FPs weekly, update exclusions</li>
                            </ul>
                        </div>
                    </div>
                    <div class="qa-item">
                        <button class="qa-question"><i class="fas fa-lightbulb" style="color: #d29922;"></i> Q: How do you prioritize which detections to build?</button>
                        <div class="qa-answer">
                            <ol>
                                <li><strong>Threat model:</strong> What threats are most likely for your organization?</li>
                                <li><strong>Coverage gaps:</strong> What MITRE techniques have no detection?</li>
                                <li><strong>Incident history:</strong> What attacks have you seen? Where did detection fail?</li>
                                <li><strong>Threat intel:</strong> What are threat actors targeting your industry using?</li>
                                <li><strong>Data availability:</strong> Do you have the data to detect it?</li>
                                <li><strong>Business impact:</strong> What would cause the most damage if undetected?</li>
                            </ol>
                            <p style="margin-top: 12px;"><strong>Framework:</strong> Risk = Likelihood × Impact × (1 - Detection Capability)</p>
                        </div>
                    </div>
                    <div class="qa-item">
                        <button class="qa-question"><i class="fas fa-lightbulb" style="color: #d29922;"></i> Q: What's the difference between behavioral and signature-based detection?</button>
                        <div class="qa-answer">
                            <p><strong>Signature-based:</strong></p>
                            <ul>
                                <li>Matches known IOCs (hashes, IPs, domains)</li>
                                <li>Low false positives when IOC is accurate</li>
                                <li>Easy to evade (change one character, new hash)</li>
                                <li>Good for known threats</li>
                            </ul>
                            <p><strong>Behavioral:</strong></p>
                            <ul>
                                <li>Detects patterns of activity</li>
                                <li>More resilient to attacker changes</li>
                                <li>Can detect novel attacks</li>
                                <li>Higher false positive potential</li>
                                <li>Example: "PowerShell executing encoded command from Office application"</li>
                            </ul>
                            <p style="margin-top: 12px;"><strong>Best practice:</strong> Use both - signatures for known threats, behavioral for unknown variants.</p>
                        </div>
                    </div>
                </div>

            </main>
        </div>
    </div>
    <script>
        document.querySelectorAll('.qa-question').forEach(btn => {
            btn.addEventListener('click', () => {
                const answer = btn.nextElementSibling;
                const isOpen = answer.style.display === 'block';
                document.querySelectorAll('.qa-answer').forEach(a => a.style.display = 'none');
                document.querySelectorAll('.qa-question').forEach(b => b.classList.remove('active'));
                if (!isOpen) { answer.style.display = 'block'; btn.classList.add('active'); }
            });
        });
    </script>
</body>
</html>

                <!-- Detection Rule Repository Management -->
                <h2><i class="fas fa-folder-open"></i> Detection Rule Repository</h2>
                <div class="config-section">
                    <h4>Repository Structure</h4>
                    <div class="code-block">detection-rules/
├── README.md                    # Repository overview
├── rules/
│   ├── execution/
│   │   ├── powershell_encoded_command.yaml
│   │   ├── wmi_process_creation.yaml
│   │   └── office_macro_execution.yaml
│   ├── persistence/
│   │   ├── scheduled_task_creation.yaml
│   │   ├── registry_run_key.yaml
│   │   └── new_service_installed.yaml
│   ├── lateral_movement/
│   │   ├── psexec_execution.yaml
│   │   ├── wmi_remote_process.yaml
│   │   └── rdp_from_unusual_source.yaml
│   └── credential_access/
│       ├── lsass_memory_access.yaml
│       ├── kerberoasting.yaml
│       └── dcsync_detection.yaml
├── exclusions/
│   ├── global_exclusions.yaml   # Apply to all rules
│   ├── admin_accounts.yaml      # Admin user whitelist
│   └── scanner_hosts.yaml       # Security scanner hosts
├── tests/
│   ├── atomic_tests/            # Atomic Red Team mappings
│   └── sample_logs/             # Test log files
└── docs/
    ├── rule_template.md         # How to write rules
    ├── testing_guide.md         # How to test rules
    └── deployment_guide.md      # How to deploy rules</div>

                    <h4>Version Control Best Practices</h4>
                    <ul style="color: #8b949e;">
                        <li><strong>Branching:</strong> Use feature branches for new detections</li>
                        <li><strong>Code Review:</strong> Require peer review before merge</li>
                        <li><strong>CI/CD:</strong> Automated testing on pull requests</li>
                        <li><strong>Tagging:</strong> Version tags for production deployments</li>
                        <li><strong>Changelog:</strong> Document changes for each release</li>
                    </ul>

                    <h4>Rule Metadata Schema</h4>
                    <div class="code-block">---
id: det-exec-001
name: PowerShell Encoded Command Execution
version: 2.1
created: 2024-01-15
modified: 2024-06-20
author: Detection Engineering Team
status: production  # draft, testing, production, deprecated

description: |
  Detects PowerShell execution with encoded command parameters,
  commonly used by attackers to obfuscate malicious commands.

mitre:
  tactic: Execution
  technique: T1059.001
  subtechnique: null

severity: high
confidence: medium

data_sources:
  - Windows Security Event Log (4688)
  - Sysmon (Event ID 1)
  - EDR Process Creation

false_positives:
  - Administrative scripts using encoded commands
  - SCCM/ConfigMgr deployments
  - Some legitimate software installers

references:
  - https://attack.mitre.org/techniques/T1059/001/
  - https://example.com/powershell-attacks

tags:
  - powershell
  - execution
  - obfuscation
---</div>
                </div>

                <!-- Detection-as-Code Pipeline -->
                <h2><i class="fas fa-code-branch"></i> Detection-as-Code Pipeline</h2>
                <div class="config-section">
                    <h4>CI/CD Pipeline for Detections</h4>
                    <div class="code-block"># .github/workflows/detection-pipeline.yml
name: Detection Rule Pipeline

on:
  pull_request:
    paths:
      - 'rules/**'
  push:
    branches:
      - main

jobs:
  validate:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      
      - name: Validate YAML syntax
        run: |
          pip install yamllint
          yamllint rules/
      
      - name: Validate rule schema
        run: python scripts/validate_schema.py rules/
      
      - name: Check for duplicate rule IDs
        run: python scripts/check_duplicates.py rules/

  test:
    needs: validate
    runs-on: ubuntu-latest
    steps:
      - name: Run detection tests
        run: python scripts/test_detections.py
      
      - name: Check Sigma rule compatibility
        run: |
          pip install sigma-cli
          sigma check rules/

  deploy:
    needs: test
    if: github.ref == 'refs/heads/main'
    runs-on: ubuntu-latest
    steps:
      - name: Deploy to SIEM
        run: python scripts/deploy_rules.py --target production
        env:
          SIEM_API_KEY: ${{ secrets.SIEM_API_KEY }}</div>

                    <h4>Automated Testing Framework</h4>
                    <div class="code-block"># test_detections.py
import yaml
import pytest

class TestDetectionRules:
    
    def test_rule_has_required_fields(self, rule_file):
        """Verify all required metadata fields are present"""
        with open(rule_file) as f:
            rule = yaml.safe_load(f)
        
        required_fields = ['id', 'name', 'description', 'mitre', 'severity']
        for field in required_fields:
            assert field in rule, f"Missing required field: {field}"
    
    def test_mitre_mapping_valid(self, rule_file):
        """Verify MITRE technique ID is valid format"""
        with open(rule_file) as f:
            rule = yaml.safe_load(f)
        
        technique = rule['mitre']['technique']
        assert technique.startswith('T'), f"Invalid technique: {technique}"
    
    def test_severity_valid(self, rule_file):
        """Verify severity is valid value"""
        with open(rule_file) as f:
            rule = yaml.safe_load(f)
        
        valid_severities = ['critical', 'high', 'medium', 'low', 'informational']
        assert rule['severity'] in valid_severities
    
    def test_detection_against_sample_logs(self, rule_file, sample_logs):
        """Run detection against known-bad sample logs"""
        # Load rule and convert to query
        # Execute against sample logs
        # Verify expected detections fire
        pass</div>
                </div>

                <!-- Advanced Detection Patterns -->
                <h2><i class="fas fa-project-diagram"></i> Advanced Detection Patterns</h2>
                <div class="config-section">
                    <h4>Pattern 1: Attack Chain Detection</h4>
                    <p style="color: #8b949e;">Detect multiple stages of an attack in sequence:</p>
                    <div class="code-block">// Detect: Phishing → Macro → PowerShell → C2
// Stage 1: Office spawns command interpreter
// Stage 2: PowerShell makes network connection
// Stage 3: Scheduled task created for persistence

// Splunk correlation search
index=endpoint sourcetype=sysmon
| eval stage = case(
    EventCode=1 AND parent_image LIKE "%WINWORD.EXE%" AND 
        (image LIKE "%powershell%" OR image LIKE "%cmd%"), "stage1_macro",
    EventCode=3 AND image LIKE "%powershell%" AND 
        NOT dest_ip IN (internal_ranges), "stage2_c2",
    EventCode=1 AND image LIKE "%schtasks%" AND 
        CommandLine LIKE "%/create%", "stage3_persistence",
    true(), null()
)
| where isnotnull(stage)
| stats values(stage) as stages, dc(stage) as stage_count by host, user
| where stage_count >= 2
| where "stage1_macro" IN (stages)</div>

                    <h4>Pattern 2: Peer Group Anomaly</h4>
                    <p style="color: #8b949e;">Detect behavior unusual for a user's peer group:</p>
                    <div class="code-block">// Compare user behavior to their department peers
// KQL Example

let UserDepartments = IdentityInfo | project UserPrincipalName, Department;
let DepartmentBaseline = 
    SigninLogs
    | where TimeGenerated > ago(30d)
    | join kind=inner UserDepartments on UserPrincipalName
    | summarize AvgLogins = avg(count()), StdDev = stdev(count()) 
        by Department;
SigninLogs
| where TimeGenerated > ago(1d)
| join kind=inner UserDepartments on UserPrincipalName
| summarize TodayLogins = count() by UserPrincipalName, Department
| join kind=inner DepartmentBaseline on Department
| where TodayLogins > AvgLogins + (3 * StdDev)
| project UserPrincipalName, Department, TodayLogins, AvgLogins, Deviation = TodayLogins - AvgLogins</div>

                    <h4>Pattern 3: First Occurrence Detection</h4>
                    <p style="color: #8b949e;">Alert on first-time observations:</p>
                    <div class="code-block">// First time a process has been seen in environment
// XQL Example

let historical = 
    dataset = xdr_data
    | filter _time > ago(30d) and _time < ago(1d)
    | filter event_type = "process_execution"
    | comp distinct_values = values(process_name);
dataset = xdr_data
| filter _time > ago(1d)
| filter event_type = "process_execution"
| filter process_name not in (historical.distinct_values)
| comp first_seen = min(_time), hosts = count_distinct(host_name) by process_name
| filter hosts < 3  // Only if seen on few hosts
| sort first_seen asc</div>

                    <h4>Pattern 4: Velocity Detection</h4>
                    <p style="color: #8b949e;">Detect rapid activity within short time window:</p>
                    <div class="code-block">// Detect rapid file access (potential ransomware)
index=endpoint sourcetype=sysmon EventCode=11
| bucket _time span=1m
| stats count as files_per_minute by _time, host, user
| where files_per_minute > 100
| streamstats window=5 avg(files_per_minute) as rolling_avg by host
| where files_per_minute > (rolling_avg * 10)  // 10x spike</div>
                </div>

                <!-- Data Quality for Detection -->
                <h2><i class="fas fa-database"></i> Data Quality Requirements</h2>
                <div class="config-section">
                    <h4>Essential Data Sources by Technique</h4>
                    <table class="styled-table">
                        <thead>
                            <tr><th>MITRE Tactic</th><th>Required Data Sources</th><th>Key Fields</th></tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>Execution</td>
                                <td>Process Creation (Sysmon 1, 4688)</td>
                                <td>Process name, command line, parent process, user</td>
                            </tr>
                            <tr>
                                <td>Persistence</td>
                                <td>Registry (Sysmon 12/13), Scheduled Tasks</td>
                                <td>Registry key, value, task name, action</td>
                            </tr>
                            <tr>
                                <td>Privilege Escalation</td>
                                <td>Windows Security (4672, 4673)</td>
                                <td>Privileges assigned, user, process</td>
                            </tr>
                            <tr>
                                <td>Defense Evasion</td>
                                <td>Sysmon, EDR telemetry</td>
                                <td>Process injection, tampering events</td>
                            </tr>
                            <tr>
                                <td>Credential Access</td>
                                <td>Windows Security (4624, 4625, 4768, 4769)</td>
                                <td>Logon type, failure reason, ticket type</td>
                            </tr>
                            <tr>
                                <td>Lateral Movement</td>
                                <td>Windows Security (4624 Type 3), Network</td>
                                <td>Source IP, destination, authentication type</td>
                            </tr>
                            <tr>
                                <td>Exfiltration</td>
                                <td>Network (firewall, proxy), DLP</td>
                                <td>Bytes transferred, destination, protocol</td>
                            </tr>
                        </tbody>
                    </table>

                    <h4>Data Quality Validation Queries</h4>
                    <div class="code-block">// Check if command line logging is enabled (Windows)
index=endpoint sourcetype=WinEventLog:Security EventCode=4688
| stats count(CommandLine) as with_cmdline, count by host
| eval pct_with_cmdline = round(with_cmdline * 100 / count, 1)
| where pct_with_cmdline < 90
| table host, pct_with_cmdline

// Check Sysmon coverage
index=endpoint sourcetype=sysmon
| stats dc(host) as sysmon_hosts
| appendcols [search index=endpoint | stats dc(host) as total_hosts]
| eval coverage = round(sysmon_hosts * 100 / total_hosts, 1)

// Check for log gaps
index=* sourcetype=WinEventLog:Security
| bucket _time span=1h
| stats count by _time, host
| where count < 100  // Suspiciously low event count</div>
                </div>

                <!-- Measuring Detection Effectiveness -->
                <h2><i class="fas fa-chart-line"></i> Measuring Detection Effectiveness</h2>
                <div class="config-section">
                    <h4>Key Performance Indicators</h4>
                    <table class="styled-table">
                        <thead>
                            <tr><th>KPI</th><th>Formula</th><th>Target</th><th>How to Improve</th></tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>True Positive Rate</td>
                                <td>TP / (TP + FN)</td>
                                <td>> 80%</td>
                                <td>Lower thresholds, broaden detection</td>
                            </tr>
                            <tr>
                                <td>False Positive Rate</td>
                                <td>FP / (FP + TN)</td>
                                <td>< 10%</td>
                                <td>Add exclusions, increase specificity</td>
                            </tr>
                            <tr>
                                <td>Precision</td>
                                <td>TP / (TP + FP)</td>
                                <td>> 70%</td>
                                <td>Tune thresholds, add context</td>
                            </tr>
                            <tr>
                                <td>Mean Time to Detect</td>
                                <td>Avg(Alert Time - Attack Time)</td>
                                <td>< 15 min</td>
                                <td>Increase rule frequency</td>
                            </tr>
                            <tr>
                                <td>MITRE Coverage</td>
                                <td>Techniques Covered / Priority Techniques</td>
                                <td>> 60%</td>
                                <td>Develop new detections</td>
                            </tr>
                        </tbody>
                    </table>

                    <h4>Detection Effectiveness Dashboard Query</h4>
                    <div class="code-block">// Monthly detection metrics
index=notable_events
| eval outcome = case(
    status="closed_true_positive", "TP",
    status="closed_false_positive", "FP",
    status="closed_benign", "FP",
    true(), "Pending"
)
| where outcome != "Pending"
| stats 
    count as total_alerts,
    countif(outcome="TP") as true_positives,
    countif(outcome="FP") as false_positives
    by rule_name
| eval 
    precision = round(true_positives * 100 / total_alerts, 1),
    fp_rate = round(false_positives * 100 / total_alerts, 1)
| sort - total_alerts
| table rule_name, total_alerts, true_positives, false_positives, precision, fp_rate</div>
                </div>

                <!-- Additional Interview Questions -->
                <h2><i class="fas fa-user-tie"></i> More Interview Questions</h2>
                <div class="qa-section">
                    <div class="qa-item">
                        <button class="qa-question"><i class="fas fa-lightbulb" style="color: #d29922;"></i> Q: How do you test a detection rule before deploying to production?</button>
                        <div class="qa-answer">
                            <ol>
                                <li><strong>Historical data analysis:</strong> Run the query against past logs to check false positive rate</li>
                                <li><strong>Atomic Red Team:</strong> Execute specific technique tests to verify true positive detection</li>
                                <li><strong>Sample log replay:</strong> Use known-bad log samples from previous incidents</li>
                                <li><strong>Staging environment:</strong> Deploy to test environment first if available</li>
                                <li><strong>Test mode deployment:</strong> Enable rule but mark alerts as test/informational</li>
                                <li><strong>Peer review:</strong> Have another engineer review the logic</li>
                                <li><strong>Performance testing:</strong> Verify query completes in acceptable time</li>
                            </ol>
                        </div>
                    </div>
                    <div class="qa-item">
                        <button class="qa-question"><i class="fas fa-lightbulb" style="color: #d29922;"></i> Q: What data sources would you prioritize for detection coverage?</button>
                        <div class="qa-answer">
                            <p><strong>Priority order based on detection value:</strong></p>
                            <ol>
                                <li><strong>Endpoint (Sysmon/EDR):</strong> Process creation with command lines - covers execution, most attack techniques</li>
                                <li><strong>Windows Security Events:</strong> Authentication events - covers credential access, lateral movement</li>
                                <li><strong>Network (Firewall/Proxy):</strong> Traffic metadata - covers C2, exfiltration</li>
                                <li><strong>Identity (AD/Azure AD):</strong> Privileged operations - covers persistence, privilege escalation</li>
                                <li><strong>DNS:</strong> Query logs - covers C2 beaconing, DGA domains</li>
                                <li><strong>Email:</strong> Message metadata - covers initial access (phishing)</li>
                            </ol>
                            <p style="margin-top: 12px;"><strong>Key principle:</strong> Endpoint data provides the highest detection value per byte ingested.</p>
                        </div>
                    </div>
                    <div class="qa-item">
                        <button class="qa-question"><i class="fas fa-lightbulb" style="color: #d29922;"></i> Q: How do you handle a detection rule that generates too many false positives?</button>
                        <div class="qa-answer">
                            <ol>
                                <li><strong>Analyze FP sources:</strong> Group false positives by common attributes (user, host, process)</li>
                                <li><strong>Add targeted exclusions:</strong> Whitelist specific known-good patterns</li>
                                <li><strong>Increase specificity:</strong> Add more conditions to narrow the detection</li>
                                <li><strong>Raise thresholds:</strong> Increase count thresholds if applicable</li>
                                <li><strong>Add correlation:</strong> Require additional indicators before alerting</li>
                                <li><strong>Time-based tuning:</strong> Exclude maintenance windows or business hours patterns</li>
                                <li><strong>Consider retirement:</strong> If FP rate can't be fixed, deprecate the rule</li>
                            </ol>
                            <p style="margin-top: 12px;"><strong>Important:</strong> Document all exclusions and regularly review them to prevent detection gaps.</p>
                        </div>
                    </div>
                </div>

            </main>
        </div>
    </div>
    <script>

            </main>
        </div>
    </div>
    <script>
        document.querySelectorAll('.qa-question').forEach(btn => {
            btn.addEventListener('click', () => {
                const answer = btn.nextElementSibling;
                const isOpen = answer.style.display === 'block';
                document.querySelectorAll('.qa-answer').forEach(a => a.style.display = 'none');
                document.querySelectorAll('.qa-question').forEach(b => b.classList.remove('active'));
                if (!isOpen) { answer.style.display = 'block'; btn.classList.add('active'); }
            });
        });
    </script>
</body>
</html>

                <!-- Detection as Code -->
                <h2><i class="fas fa-code-branch"></i> Detection as Code</h2>
                <div class="config-section">
                    <p>Manage detection rules like software: version control, testing, code review, CI/CD deployment.</p>
                    
                    <h4>Repository Structure</h4>
                    <div class="code-block">detection-rules/
├── README.md
├── rules/
│   ├── execution/
│   │   ├── T1059.001_powershell_encoded.yml
│   │   ├── T1059.003_windows_cmd.yml
│   │   └── T1059.005_vbscript.yml
│   ├── persistence/
│   │   ├── T1053.005_scheduled_task.yml
│   │   └── T1547.001_registry_run_keys.yml
│   ├── credential_access/
│   │   └── T1003.001_lsass_access.yml
│   └── lateral_movement/
│       └── T1021.002_smb_admin_shares.yml
├── tests/
│   ├── test_data/
│   └── test_rules.py
├── scripts/
│   ├── deploy_rules.py
│   ├── validate_rules.py
│   └── coverage_report.py
└── .github/
    └── workflows/
        └── ci.yml</div>

                    <h4>Rule File Format (YAML)</h4>
                    <div class="code-block">id: exec-powershell-encoded-001
name: PowerShell Encoded Command Execution
description: |
  Detects execution of PowerShell with encoded command parameter,
  commonly used to obfuscate malicious commands.
author: Detection Engineering Team
created: 2024-01-15
modified: 2024-06-01
version: 1.2

mitre:
  tactic: Execution
  technique: T1059.001
  subtechnique: null

severity: high
confidence: medium

data_sources:
  - Windows Security Event Log (4688)
  - Sysmon (Event ID 1)
  - EDR Process Events

detection:
  platform: splunk
  query: |
    index=endpoint sourcetype=sysmon EventCode=1
    | where process_name IN ("powershell.exe", "pwsh.exe")
    | where match(CommandLine, "(?i)-e(nc|ncodedcommand)")
    | where parent_image NOT IN ("explorer.exe", "sccm.exe")
  
  # Alternative queries for other platforms
  alternatives:
    sentinel: |
      DeviceProcessEvents
      | where FileName in~ ("powershell.exe", "pwsh.exe")
      | where ProcessCommandLine matches regex @"(?i)-e(nc|ncodedcommand)"
    xsiam: |
      dataset = xdr_data
      | filter process_name in ("powershell.exe", "pwsh.exe")
      | filter process_command_line ~= "(?i)-e(nc|ncodedcommand)"

false_positives:
  - Administrative automation scripts
  - SCCM/MECM deployments
  - Some legitimate software installers

exclusions:
  users:
    - svc_sccm
    - svc_automation
  hosts:
    - YOURCOMPANY-SCCM01
  paths:
    - C:\Windows\CCM\*

references:
  - https://attack.mitre.org/techniques/T1059/001/
  - https://docs.microsoft.com/powershell/

response:
  - Isolate host if confirmed malicious
  - Capture full command line for analysis
  - Check for persistence mechanisms
  - Review parent process chain

testing:
  atomic_red_team: T1059.001
  test_command: |
    powershell -enc VwByAGkAdABlAC0ASABvAHMAdAAgACIAdABlAHMAdAAi</div>

                    <h4>CI/CD Pipeline for Detection Rules</h4>
                    <div class="code-block"># .github/workflows/ci.yml
name: Detection Rule CI

on:
  push:
    paths:
      - 'rules/**'
  pull_request:
    paths:
      - 'rules/**'

jobs:
  validate:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      
      - name: Validate YAML syntax
        run: |
          pip install pyyaml
          python scripts/validate_rules.py
      
      - name: Check MITRE mapping
        run: python scripts/check_mitre_mapping.py
      
      - name: Run unit tests
        run: pytest tests/
      
      - name: Generate coverage report
        run: python scripts/coverage_report.py

  deploy:
    needs: validate
    if: github.ref == 'refs/heads/main'
    runs-on: ubuntu-latest
    steps:
      - name: Deploy to SIEM
        run: python scripts/deploy_rules.py --env production</div>
                </div>

                <!-- Common Detection Patterns -->
                <h2><i class="fas fa-puzzle-piece"></i> Common Detection Patterns</h2>
                <div class="config-section">
                    <h4>Pattern 1: Process Chain Analysis</h4>
                    <div class="code-block">// Detect Office application spawning command interpreter
Parent Process: WINWORD.EXE, EXCEL.EXE, OUTLOOK.EXE, POWERPNT.EXE
Child Process: cmd.exe, powershell.exe, wscript.exe, cscript.exe, mshta.exe

// Splunk
index=endpoint EventCode=1
| where parent_image IN ("*\\WINWORD.EXE", "*\\EXCEL.EXE", "*\\OUTLOOK.EXE")
| where image IN ("*\\cmd.exe", "*\\powershell.exe", "*\\wscript.exe")

// Why this works: Legitimate Office use rarely spawns command interpreters
// This pattern catches macro execution, OLE exploitation</div>

                    <h4>Pattern 2: Living Off the Land (LOLBins)</h4>
                    <div class="code-block">// Detect abuse of legitimate Windows tools
LOLBins with network capability:
- certutil.exe -urlcache -split -f http://...
- bitsadmin.exe /transfer ... http://...
- mshta.exe http://...
- regsvr32.exe /s /n /u /i:http://... scrobj.dll

// Detection Logic
Process = LOLBin
AND (CommandLine contains "http" OR CommandLine contains "ftp" OR CommandLine contains "\\\\")

// Additional context to reduce FPs:
AND NOT (parent is legitimate admin tool)
AND NOT (user is admin account during business hours)</div>

                    <h4>Pattern 3: Rare/First-Time Execution</h4>
                    <div class="code-block">// Alert on processes never seen before in environment
// Requires baseline of "normal" processes

// Build baseline (run weekly)
Last 30 days | Collect distinct (process_hash, process_name) | Store as baseline

// Detection
Current event
| WHERE process_hash NOT IN baseline
| AND execution_count_last_7_days < 5
| AND NOT (process is signed by trusted vendor)

// This catches new malware, tools attackers bring</div>

                    <h4>Pattern 4: Threshold with Velocity</h4>
                    <div class="code-block">// Not just "count > N" but "count > N in time window"

// Brute Force Detection
Failed logins > 10 in 5 minutes
FROM same source IP
TO same or multiple users

// Port Scanning Detection  
Connections to > 20 unique ports in 1 minute
FROM same source IP
TO same destination

// Data Exfiltration Detection
Bytes transferred > 1GB in 1 hour
TO external IP
FROM single internal host</div>

                    <h4>Pattern 5: Sequence Detection</h4>
                    <div class="code-block">// Detect events that occur in specific order

// Reconnaissance followed by Exploitation
Event A: Port scan detected (T1046)
FOLLOWED BY (within 1 hour)
Event B: Successful connection to scanned service

// Credential Stuffing
Event A: Multiple failed logins to different accounts
FOLLOWED BY
Event B: Successful login
FROM same source IP

// Privilege Escalation Chain
Event A: New user created
FOLLOWED BY (within 30 minutes)
Event B: User added to admin group</div>
                </div>

                <!-- Detection Engineering Metrics -->
                <h2><i class="fas fa-chart-line"></i> Measuring Detection Program Effectiveness</h2>
                <div class="config-section">
                    <h4>Key Performance Indicators</h4>
                    <table class="styled-table">
                        <thead>
                            <tr><th>Metric</th><th>Formula</th><th>Target</th></tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>Detection Coverage</td>
                                <td>MITRE techniques with detection / Priority techniques</td>
                                <td>> 80%</td>
                            </tr>
                            <tr>
                                <td>True Positive Rate</td>
                                <td>True positives / (True positives + False negatives)</td>
                                <td>> 90%</td>
                            </tr>
                            <tr>
                                <td>False Positive Rate</td>
                                <td>False positives / Total alerts</td>
                                <td>< 20%</td>
                            </tr>
                            <tr>
                                <td>Mean Time to Detect (MTTD)</td>
                                <td>Avg time from attack to alert</td>
                                <td>< 15 min</td>
                            </tr>
                            <tr>
                                <td>Detection Rule Efficiency</td>
                                <td>Alerts leading to action / Total alerts</td>
                                <td>> 50%</td>
                            </tr>
                            <tr>
                                <td>Rule Freshness</td>
                                <td>Rules updated in last 90 days / Total rules</td>
                                <td>> 25%</td>
                            </tr>
                        </tbody>
                    </table>

                    <h4>Detection Quality Scorecard</h4>
                    <div class="code-block">Detection Rule Evaluation Criteria (score 1-5):

1. Detection Efficacy
   - Does it detect the intended threat? 
   - How easily can it be evaded?
   
2. Signal Quality
   - True positive rate
   - False positive rate
   - Alert fatigue impact
   
3. Context Richness
   - Does alert contain investigation context?
   - Are relevant entities identified?
   - Is MITRE mapping accurate?
   
4. Response Readiness
   - Is response procedure documented?
   - Can it trigger automated response?
   
5. Maintenance Burden
   - How often does it need tuning?
   - Is the query performant?
   
Total Score: ___ / 25
Threshold for production: >= 18</div>
                </div>

                <!-- Tools and Resources -->
                <h2><i class="fas fa-tools"></i> Tools and Resources</h2>
                <div class="config-section">
                    <h4>Detection Development Tools</h4>
                    <table class="styled-table">
                        <thead>
                            <tr><th>Tool</th><th>Purpose</th><th>Link</th></tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td><strong>Sigma</strong></td>
                                <td>Portable detection rule format</td>
                                <td>github.com/SigmaHQ/sigma</td>
                            </tr>
                            <tr>
                                <td><strong>Atomic Red Team</strong></td>
                                <td>Test detection coverage</td>
                                <td>github.com/redcanaryco/atomic-red-team</td>
                            </tr>
                            <tr>
                                <td><strong>MITRE ATT&CK Navigator</strong></td>
                                <td>Visualize coverage</td>
                                <td>mitre-attack.github.io/attack-navigator</td>
                            </tr>
                            <tr>
                                <td><strong>Detection Lab</strong></td>
                                <td>Test environment</td>
                                <td>github.com/clong/DetectionLab</td>
                            </tr>
                            <tr>
                                <td><strong>Uncoder.io</strong></td>
                                <td>Translate between query languages</td>
                                <td>uncoder.io</td>
                            </tr>
                        </tbody>
                    </table>

                    <h4>Detection Content Sources</h4>
                    <ul style="color: #8b949e;">
                        <li><strong>Sigma Rules Repository:</strong> 3000+ community detection rules</li>
                        <li><strong>Elastic Detection Rules:</strong> Open source detection content</li>
                        <li><strong>Splunk Security Content:</strong> ESCU detection rules</li>
                        <li><strong>Microsoft Sentinel Content Hub:</strong> Detection templates</li>
                        <li><strong>MITRE CAR:</strong> Cyber Analytics Repository</li>
                    </ul>
                </div>

                <!-- Building a Detection Engineering Program -->
                <h2><i class="fas fa-building"></i> Building a Detection Engineering Program</h2>
                <div class="config-section">
                    <h4>Maturity Levels</h4>
                    <div class="code-block">Level 0 - Ad Hoc
├── Detections created reactively after incidents
├── No documentation or version control
├── No testing process
└── High false positive rates, limited coverage

Level 1 - Defined
├── Basic detection development process documented
├── Some MITRE ATT&CK mapping
├── Manual testing with Atomic Red Team
└── Rules stored in SIEM, no version control

Level 2 - Managed
├── Detection rules in version control (Git)
├── Formal review process for new rules
├── Regular coverage assessments
├── Metrics tracked (FP rate, coverage)
└── Sigma used for portability

Level 3 - Optimized
├── Full CI/CD pipeline for detection rules
├── Automated testing on every change
├── Continuous coverage monitoring
├── Detection engineering KPIs reviewed weekly
├── Threat intelligence automatically converted to detections
└── Purple team exercises validate detection effectiveness

Level 4 - Innovative
├── ML/AI-assisted detection development
├── Automated detection generation from threat intel
├── Real-time coverage gap identification
├── Predictive detection (anticipate new techniques)
└── Contribution back to community (Sigma, etc.)</div>

                    <h4>Team Structure</h4>
                    <div class="code-block">Small Team (1-2 people):
├── Single detection engineer
├── Focuses on highest-priority detections
├── Leverages community content heavily
└── Part-time threat research

Medium Team (3-5 people):
├── Detection Engineer(s): Write and tune rules
├── Threat Researcher: Identify new techniques to detect
├── Platform Engineer: Manage SIEM/EDR, optimize performance
└── Regular purple team exercises

Large Team (6+ people):
├── Detection Engineering Lead
├── Senior Detection Engineers (by platform or domain)
├── Threat Intelligence Analyst
├── Detection Platform Engineer
├── Purple Team / Detection Validation
└── Metrics and Reporting Analyst</div>
                </div>

            </main>
        </div>
    </div>
    <script>

            </main>
        </div>
    </div>
    <script>
        document.querySelectorAll('.qa-question').forEach(btn => {
            btn.addEventListener('click', () => {
                const answer = btn.nextElementSibling;
                const isOpen = answer.style.display === 'block';
                document.querySelectorAll('.qa-answer').forEach(a => a.style.display = 'none');
                document.querySelectorAll('.qa-question').forEach(b => b.classList.remove('active'));
                if (!isOpen) { answer.style.display = 'block'; btn.classList.add('active'); }
            });
        });
    </script>
</body>
</html>
