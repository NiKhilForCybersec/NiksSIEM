<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Threat Hunting | Sentinel</title>
    <link rel="stylesheet" href="../assets/css/main.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
</head>
<body>
    <div class="app-container">
        <!-- Sidebar -->
    <aside class="sidebar" id="sidebar">
        <div class="sidebar-header">
            <a href="../index.html" class="sidebar-brand">
                <i class="fas fa-shield-alt"></i>
                <span>Nik's SIEM</span>
            </a>
            <button class="sidebar-toggle" onclick="toggleSidebar()" title="Collapse sidebar">
                <i class="fas fa-chevron-left"></i>
            </button>
        </div>
        
        <div class="sidebar-content">
            <!-- Platforms Section -->
            <div class="sidebar-section">
                <div class="sidebar-section-title">Platforms</div>
                <ul class="sidebar-nav">
                    <li class="sidebar-nav-item"><a href="../xsiam/index.html" class="sidebar-nav-link"><span class="platform-dot"></span><span>XSIAM</span></a></li>
                    <li class="sidebar-nav-item"><a href="../splunk/index.html" class="sidebar-nav-link"><span class="platform-dot"></span><span>Splunk</span></a></li>
                    <li class="sidebar-nav-item"><a href="../sentinel/index.html" class="sidebar-nav-link active"><span class="platform-dot"></span><span>Sentinel</span></a></li>
                    <li class="sidebar-nav-item"><a href="../crowdstrike/index.html" class="sidebar-nav-link"><span class="platform-dot"></span><span>CrowdStrike</span></a></li>
                    <li class="sidebar-nav-item"><a href="../cortex/index.html" class="sidebar-nav-link"><span class="platform-dot"></span><span>Cortex XDR</span></a></li>
                    <li class="sidebar-nav-item"><a href="../mde/index.html" class="sidebar-nav-link"><span class="platform-dot"></span><span>Defender for Endpoint</span></a></li>
                    <li class="sidebar-nav-item"><a href="../operations/index.html" class="sidebar-nav-link"><span class="platform-dot"></span><span>SOC Operations</span></a></li>
                </ul>
            </div>

            <div class="sidebar-divider"></div>
            
            <!-- Getting Started -->
            <div class="sidebar-section">
                <div class="sidebar-section-title">Getting Started</div>
                <ul class="sidebar-nav">
                    <li class="sidebar-nav-item"><a href="index.html" class="sidebar-nav-link"><i class="fas fa-home"></i><span>Overview</span></a></li>
                    <li class="sidebar-nav-item"><a href="architecture.html" class="sidebar-nav-link"><i class="fas fa-sitemap"></i><span>Architecture</span></a></li>
                    <li class="sidebar-nav-item"><a href="content-hub.html" class="sidebar-nav-link"><i class="fas fa-store"></i><span>Content Hub</span></a></li>
                </ul>
            </div>
            
            <!-- Data Collection -->
            <div class="sidebar-section">
                <div class="sidebar-section-title">Data Collection</div>
                <ul class="sidebar-nav">
                    <li class="sidebar-nav-item"><a href="data-connectors.html" class="sidebar-nav-link"><i class="fas fa-plug"></i><span>Data Connectors</span></a></li>
                    <li class="sidebar-nav-item"><a href="data-ingestion.html" class="sidebar-nav-link"><i class="fas fa-database"></i><span>Data Ingestion</span></a></li>
                    <li class="sidebar-nav-item"><a href="dcr-dce-guide.html" class="sidebar-nav-link"><i class="fas fa-cogs"></i><span>DCR & DCE Guide</span></a></li>
                    <li class="sidebar-nav-item"><a href="custom-log-onboarding.html" class="sidebar-nav-link"><i class="fas fa-plus-circle"></i><span>Custom Log Onboarding</span></a></li>
                    <li class="sidebar-nav-item"><a href="event-hub.html" class="sidebar-nav-link"><i class="fas fa-stream"></i><span>Event Hub Integration</span></a></li>
                    <li class="sidebar-nav-item"><a href="parsing-flows.html" class="sidebar-nav-link"><i class="fas fa-filter"></i><span>Parsing & Flows</span></a></li>
                    <li class="sidebar-nav-item"><a href="retention-tiers.html" class="sidebar-nav-link"><i class="fas fa-archive"></i><span>Retention & Tiers</span></a></li>
                </ul>
            </div>
            
            <!-- Query & Analysis -->
            <div class="sidebar-section">
                <div class="sidebar-section-title">Query & Analysis</div>
                <ul class="sidebar-nav">
                    <li class="sidebar-nav-item"><a href="kql-fundamentals.html" class="sidebar-nav-link"><i class="fas fa-terminal"></i><span>KQL Fundamentals</span></a></li>
                    <li class="sidebar-nav-item"><a href="kql-intermediate.html" class="sidebar-nav-link"><i class="fas fa-code"></i><span>KQL Intermediate</span></a></li>
                    <li class="sidebar-nav-item"><a href="kql-advanced.html" class="sidebar-nav-link"><i class="fas fa-rocket"></i><span>KQL Advanced</span></a></li>
                    <li class="sidebar-nav-item"><a href="workbooks.html" class="sidebar-nav-link"><i class="fas fa-chart-bar"></i><span>Workbooks</span></a></li>
                    <li class="sidebar-nav-item"><a href="notebooks.html" class="sidebar-nav-link"><i class="fas fa-book-open"></i><span>Notebooks</span></a></li>
                </ul>
            </div>
            
            <!-- Detection & Response -->
            <div class="sidebar-section">
                <div class="sidebar-section-title">Detection & Response</div>
                <ul class="sidebar-nav">
                    <li class="sidebar-nav-item"><a href="analytics-rules.html" class="sidebar-nav-link"><i class="fas fa-shield-alt"></i><span>Analytics Rules</span></a></li>
                    <li class="sidebar-nav-item"><a href="watchlists.html" class="sidebar-nav-link"><i class="fas fa-list-check"></i><span>Watchlists</span></a></li>
                    <li class="sidebar-nav-item"><a href="ueba.html" class="sidebar-nav-link"><i class="fas fa-user-secret"></i><span>UEBA</span></a></li>
                    <li class="sidebar-nav-item"><a href="threat-intel.html" class="sidebar-nav-link"><i class="fas fa-skull-crossbones"></i><span>Threat Intelligence</span></a></li>
                    <li class="sidebar-nav-item"><a href="hunting.html" class="sidebar-nav-link"><i class="fas fa-crosshairs"></i><span>Hunting</span></a></li>
                    <li class="sidebar-nav-item"><a href="threat-hunting.html" class="sidebar-nav-link"><i class="fas fa-search"></i><span>Threat Hunting</span></a></li>
                </ul>
            </div>
            
            <!-- Security Use Cases -->
            <div class="sidebar-section">
                <div class="sidebar-section-title">Security Use Cases</div>
                <ul class="sidebar-nav">
                    <li class="sidebar-nav-item"><a href="security-usecases.html" class="sidebar-nav-link"><i class="fas fa-crosshairs"></i><span>Detection Use Cases</span></a></li>
                    <li class="sidebar-nav-item"><a href="enterprise-scenarios.html" class="sidebar-nav-link"><i class="fas fa-building"></i><span>Enterprise Scenarios</span></a></li>
                </ul>
            </div>
            
            <!-- Automation -->
            <div class="sidebar-section">
                <div class="sidebar-section-title">Automation</div>
                <ul class="sidebar-nav">
                    <li class="sidebar-nav-item"><a href="automation.html" class="sidebar-nav-link"><i class="fas fa-robot"></i><span>Automation & Playbooks</span></a></li>
                    <li class="sidebar-nav-item"><a href="soc-operations.html" class="sidebar-nav-link"><i class="fas fa-headset"></i><span>SOC Operations</span></a></li>
                </ul>
            </div>
            
            <!-- Administration -->
            <div class="sidebar-section">
                <div class="sidebar-section-title">Administration</div>
                <ul class="sidebar-nav">
                    <li class="sidebar-nav-item"><a href="rbac.html" class="sidebar-nav-link"><i class="fas fa-user-shield"></i><span>RBAC & Permissions</span></a></li>
                    <li class="sidebar-nav-item"><a href="config-reference.html" class="sidebar-nav-link"><i class="fas fa-cog"></i><span>Configuration Reference</span></a></li>
                    <li class="sidebar-nav-item"><a href="devops.html" class="sidebar-nav-link"><i class="fas fa-code-branch"></i><span>Sentinel as Code</span></a></li>
                    <li class="sidebar-nav-item"><a href="advanced-operations.html" class="sidebar-nav-link"><i class="fas fa-cogs"></i><span>Advanced Operations</span></a></li>
                    <li class="sidebar-nav-item"><a href="cost-optimization.html" class="sidebar-nav-link"><i class="fas fa-dollar-sign"></i><span>Cost Optimization</span></a></li>
                </ul>
            </div>
        </div>
    </aside>
    
    <div class="sidebar-overlay" id="sidebarOverlay" onclick="toggleSidebar()"></div>
        
        <div class="main-wrapper" id="mainWrapper">
            <button class="mobile-toggle" onclick="toggleSidebar()" title="Open menu">
                <i class="fas fa-bars"></i>
            </button>
            
            <main class="main-content">
                <div class="breadcrumb">
                    <a href="../index.html">Home</a>
                    <span class="separator">/</span>
                    <a href="index.html">Sentinel</a>
                    <span class="separator">/</span>
                    <span class="current">Hunting</span>
                </div>

                <h1><i class="fas fa-binoculars"></i> Threat Hunting in Sentinel</h1>
                <p class="lead">Master proactive threat hunting in Microsoft Sentinel using structured methodologies, hunting queries, bookmarks, and Livestream. Enterprise hunting programs detect threats missed by automated detection 60-70% of the time.</p>

                <!-- Hunting Overview -->
                <h2><i class="fas fa-compass"></i> Hunting Fundamentals</h2>
                <div class="config-section">
                    <p><span class="console-path">Sentinel → Threat Management → Hunting</span></p>

                    <h4>Hunting vs Detection: Key Differences</h4>
                    <div class="arch-diagram">Hunting vs Automated Detection:

┌─────────────────────────────────────────────────────────────────────────────┐
│                    DETECTION (Reactive)                                     │
├─────────────────────────────────────────────────────────────────────────────┤
│ • Automated analytics rules running continuously                           │
│ • Known attack patterns and signatures                                     │
│ • Alert-driven workflow                                                     │
│ • Good for: Known threats, compliance, scale                               │
│ • Limitation: Can't detect unknown/novel attacks                           │
└─────────────────────────────────────────────────────────────────────────────┘
                                    │
                            COMPLEMENTS
                                    │
┌─────────────────────────────────────────────────────────────────────────────┐
│                    HUNTING (Proactive)                                      │
├─────────────────────────────────────────────────────────────────────────────┤
│ • Human-driven hypothesis exploration                                       │
│ • Unknown attack patterns and anomalies                                    │
│ • Query-driven investigation                                                │
│ • Good for: APTs, insider threats, novel TTPs                              │
│ • Output: New detections, improved rules, threat intel                     │
└─────────────────────────────────────────────────────────────────────────────┘

ENTERPRISE VALUE:
├── 60-70% of sophisticated threats found via hunting, not alerts
├── Hunting findings feed back into detection engineering
├── Reduces attacker dwell time from months to days
└── Develops institutional threat knowledge</div>

                    <h4>Sentinel Hunting Capabilities</h4>
                    <div class="arch-diagram">Sentinel Hunting Feature Set:

1. HUNTING QUERIES
   ├── 100+ built-in queries organized by MITRE ATT&CK
   ├── Custom query creation and management
   ├── Query scheduling and automation
   └── Results export and sharing

2. BOOKMARKS
   ├── Save interesting findings during hunts
   ├── Preserve evidence with context
   ├── Promote to incidents for investigation
   └── Tag and categorize for tracking

3. LIVESTREAM
   ├── Real-time query monitoring
   ├── Active hunt sessions (24-hour window)
   ├── Notification on matches
   └── Interactive investigation

4. NOTEBOOKS (Jupyter)
   ├── Advanced analysis with Python
   ├── Machine learning integration
   ├── Custom visualizations
   └── Reproducible hunt playbooks

5. ENTITY BEHAVIOR (UEBA)
   ├── Anomaly-based hunting
   ├── User and entity timelines
   ├── Risk scoring integration
   └── Behavior baseline comparison</div>
                </div>

                <!-- Hunting Methodology -->
                <h2><i class="fas fa-map"></i> Structured Hunting Methodology</h2>
                <div class="config-section">
                    <h4>Enterprise Hunting Framework</h4>
                    <div class="arch-diagram">TaHiTI-Inspired Hunting Methodology:

┌─────────────────────────────────────────────────────────────────────────────┐
│ PHASE 1: HYPOTHESIS GENERATION                                              │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  INPUT SOURCES:                                                             │
│  ├── Threat intelligence (APT reports, industry alerts)                    │
│  ├── MITRE ATT&CK technique coverage gaps                                  │
│  ├── Recent incidents and near-misses                                      │
│  ├── Environmental changes (new tools, M&A)                                │
│  └── Industry-specific threats                                              │
│                                                                             │
│  OUTPUT: Prioritized hypothesis with success criteria                       │
│  Example: "APT29 may be using Teams for C2 based on recent intel"          │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
                                    │
                                    ▼
┌─────────────────────────────────────────────────────────────────────────────┐
│ PHASE 2: DATA ASSESSMENT                                                    │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  QUESTIONS:                                                                 │
│  ├── Do we have visibility into the technique?                             │
│  ├── What data sources are needed?                                         │
│  ├── Is data available in Sentinel? Quality?                               │
│  └── What time range should we query?                                       │
│                                                                             │
│  OUTPUT: Data availability matrix, hunt scope definition                    │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
                                    │
                                    ▼
┌─────────────────────────────────────────────────────────────────────────────┐
│ PHASE 3: QUERY DEVELOPMENT                                                  │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  APPROACH:                                                                  │
│  ├── Start broad, refine iteratively                                       │
│  ├── Use existing queries as templates                                     │
│  ├── Optimize for performance (time filters first)                         │
│  └── Document logic and assumptions                                         │
│                                                                             │
│  OUTPUT: Validated KQL queries with expected baselines                      │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
                                    │
                                    ▼
┌─────────────────────────────────────────────────────────────────────────────┐
│ PHASE 4: EXECUTION & ANALYSIS                                               │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  ACTIVITIES:                                                                │
│  ├── Run queries against defined time ranges                               │
│  ├── Analyze results for anomalies                                         │
│  ├── Investigate outliers manually                                          │
│  ├── Bookmark interesting findings                                          │
│  └── Pivot to related entities/events                                       │
│                                                                             │
│  OUTPUT: Findings documented with evidence                                  │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
                                    │
                                    ▼
┌─────────────────────────────────────────────────────────────────────────────┐
│ PHASE 5: RESPONSE & IMPROVEMENT                                             │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  IF THREAT FOUND:                                                           │
│  ├── Promote bookmark to incident                                          │
│  ├── Trigger incident response                                              │
│  └── Preserve evidence chain                                                │
│                                                                             │
│  ALWAYS:                                                                    │
│  ├── Convert successful queries to analytics rules                         │
│  ├── Document hunt findings and lessons learned                            │
│  ├── Update threat intelligence                                             │
│  └── Refine hypothesis for next iteration                                   │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘</div>

                    <h4>Hypothesis Generation Decision Tree</h4>
                    <div class="arch-diagram">Decision Tree: Choosing What to Hunt

START: What's driving this hunt?
│
├── THREAT INTEL DRIVEN
│   ├── New APT report released?
│   │   └── Map TTPs to your environment
│   │       → Hunt for specific IOCs and behaviors
│   │
│   ├── Industry alert (CISA, sector ISAC)?
│   │   └── Check applicability to your stack
│   │       → Prioritize if you have vulnerable systems
│   │
│   └── Vendor threat brief?
│       └── Validate relevance and timeliness
│           → Hunt if you use affected products
│
├── COVERAGE GAP DRIVEN
│   ├── MITRE ATT&CK assessment shows gaps?
│   │   └── Prioritize high-impact, low-coverage techniques
│   │       → Build hunts for T1055, T1003, T1059, etc.
│   │
│   └── New data source onboarded?
│       └── What attacks can we now see?
│           → Hunt historical data for missed attacks
│
├── ANOMALY DRIVEN
│   ├── UEBA flagged unusual behavior?
│   │   └── Investigate high-risk entities
│   │       → Hunt around user/host timeline
│   │
│   └── Statistical outlier in data?
│       └── Is deviation malicious or benign?
│           → Hunt to contextualize anomaly
│
└── INCIDENT DRIVEN
    ├── Recent incident revealed gaps?
    │   └── Could similar attack be elsewhere?
    │       → Hunt for lateral indicators
    │
    └── Near-miss or suspicious activity?
        └── Was it part of larger campaign?
            → Hunt for related activity</div>
                </div>

                <!-- Hunting Queries -->
                <h2><i class="fas fa-search"></i> Hunting Queries</h2>
                <div class="config-section">
                    <h4>Built-in Hunting Queries</h4>
                    <p>Sentinel includes 100+ pre-built queries organized by MITRE ATT&CK tactics. Access via <span class="console-path">Hunting → Queries</span>.</p>

                    <div class="arch-diagram">Built-in Query Categories:

INITIAL ACCESS (TA0001)
├── Phishing detection patterns
├── External remote services
├── Valid account abuse
└── Supply chain indicators

EXECUTION (TA0002)
├── PowerShell suspicious activity
├── Command-line obfuscation
├── Scripting interpreter abuse
└── Scheduled task creation

PERSISTENCE (TA0003)
├── Registry run key modifications
├── Scheduled task persistence
├── Account creation
└── Service installation

PRIVILEGE ESCALATION (TA0004)
├── Token manipulation
├── UAC bypass attempts
├── Credential dumping preparation
└── Service account abuse

DEFENSE EVASION (TA0005)
├── Log tampering
├── Indicator removal
├── Process injection
└── Rootkit indicators

CREDENTIAL ACCESS (TA0006)
├── LSASS access
├── Credential dumping tools
├── Kerberoasting
└── Password spraying

DISCOVERY (TA0007)
├── Network scanning
├── Account enumeration
├── System information gathering
└── Security tool discovery

LATERAL MOVEMENT (TA0008)
├── Remote service exploitation
├── Pass-the-hash/ticket
├── Remote desktop abuse
└── SMB lateral movement

COLLECTION (TA0009)
├── Data staging
├── Email collection
├── Screen capture
└── Clipboard monitoring

EXFILTRATION (TA0010)
├── Data compression
├── Cloud storage exfil
├── DNS tunneling
└── Encrypted channel exfil

COMMAND & CONTROL (TA0011)
├── Beaconing patterns
├── DNS C2
├── HTTP C2
└── Protocol tunneling</div>

                    <h4>Custom Hunting Query Examples</h4>
                    <div class="arch-diagram">// Hunt: Unusual Process from Unusual Location
// Hypothesis: Attackers execute malware from non-standard paths

let suspiciousPaths = dynamic([
    @"C:\Users\Public",
    @"C:\ProgramData",
    @"C:\Windows\Temp",
    @"C:\Users\*\AppData\Local\Temp",
    @"C:\Users\*\Downloads"
]);
DeviceProcessEvents
| where TimeGenerated > ago(7d)
| where FolderPath has_any (suspiciousPaths)
| where ProcessCommandLine !contains "setup" 
    and ProcessCommandLine !contains "install"
| summarize 
    ExecutionCount = count(),
    UniqueHosts = dcount(DeviceName),
    FirstSeen = min(TimeGenerated),
    LastSeen = max(TimeGenerated)
    by FileName, FolderPath, ProcessCommandLine
| where ExecutionCount < 5  // Rare executions
| where UniqueHosts < 3     // Not widespread
| order by ExecutionCount asc

// Hunt: Potential Kerberoasting Activity
// Hypothesis: Attacker requesting TGS for offline cracking

SecurityEvent
| where TimeGenerated > ago(24h)
| where EventID == 4769
| where ServiceName !endswith "$"  // Exclude machine accounts
| where TicketEncryptionType in ("0x17", "0x18")  // RC4 encryption
| summarize 
    TicketCount = count(),
    UniqueServices = dcount(ServiceName),
    Services = make_set(ServiceName)
    by TargetUserName, IpAddress
| where TicketCount > 10  // Threshold for suspicion
| where UniqueServices > 5  // Multiple service requests
| order by TicketCount desc

// Hunt: Living Off the Land (LOLBins)
// Hypothesis: Attacker using legitimate tools maliciously

let lolbins = dynamic([
    "certutil.exe", "mshta.exe", "regsvr32.exe", "rundll32.exe",
    "msiexec.exe", "installutil.exe", "cscript.exe", "wscript.exe",
    "powershell.exe", "cmd.exe", "bitsadmin.exe", "wmic.exe"
]);
DeviceProcessEvents
| where TimeGenerated > ago(7d)
| where FileName in~ (lolbins)
| where ProcessCommandLine has_any ("http", "ftp", "-enc", "-e ", 
    "downloadstring", "downloadfile", "-c ", "bypass", "hidden")
| project 
    TimeGenerated,
    DeviceName,
    FileName,
    ProcessCommandLine,
    InitiatingProcessFileName,
    AccountName
| order by TimeGenerated desc</div>

                    <h4>Query Performance Optimization</h4>
                    <div class="arch-diagram">Hunting Query Performance Best Practices:

1. TIME FILTER FIRST
   ✓ | where TimeGenerated > ago(7d)  // First line
   ✗ | where EventID == 4688 | where TimeGenerated > ago(7d)

2. USE INDEXED COLUMNS EARLY
   ✓ | where Computer == "DC01"       // Indexed
   ✗ | where tostring(AdditionalData) contains "DC01"

3. AVOID has() WHEN POSSIBLE
   ✓ | where ProcessCommandLine has_any (terms)  // Better
   ✗ | where ProcessCommandLine contains "term"   // Slower

4. PROJECT EARLY
   ✓ | project TimeGenerated, User, Process | where...
   ✗ | where... | project TimeGenerated, User, Process

5. LIMIT RESULT SETS
   ✓ | take 1000  // During development
   ✓ | top 100 by TimeGenerated desc  // Final results

6. USE SUMMARIZE FOR AGGREGATION
   ✓ | summarize count() by User, bin(TimeGenerated, 1h)
   ✗ | extend Hour = bin(TimeGenerated, 1h) | distinct...</div>
                </div>

                <!-- Bookmarks -->
                <h2><i class="fas fa-bookmark"></i> Bookmarks & Evidence Management</h2>
                <div class="config-section">
                    <h4>Bookmark Workflow</h4>
                    <div class="arch-diagram">Bookmark Lifecycle:

┌─────────────────────────────────────────────────────────────────────────────┐
│ STEP 1: CREATE BOOKMARK                                                     │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│ During hunting, when you find something interesting:                        │
│ 1. Select the row(s) in query results                                      │
│ 2. Click "Add bookmark"                                                     │
│ 3. Provide context:                                                         │
│    ├── Name: Descriptive title                                             │
│    ├── Notes: Why this is interesting                                      │
│    ├── Tags: Categories for filtering                                      │
│    └── Entity mapping: Link to accounts/hosts/IPs                          │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
                                    │
                                    ▼
┌─────────────────────────────────────────────────────────────────────────────┐
│ STEP 2: INVESTIGATE & ENRICH                                                │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│ From the bookmark:                                                          │
│ ├── Run additional queries pivoting on entities                            │
│ ├── Check entity timelines for context                                     │
│ ├── Cross-reference with threat intel                                      │
│ ├── Add investigation notes                                                │
│ └── Update tags as understanding evolves                                   │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
                                    │
                                    ▼
┌─────────────────────────────────────────────────────────────────────────────┐
│ STEP 3: PROMOTE OR CLOSE                                                    │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│ IF MALICIOUS:                                                               │
│ ├── "Promote to incident" creates case for IR                              │
│ ├── All bookmark evidence transfers                                        │
│ ├── Entity mappings enable timeline                                        │
│ └── Begin formal incident response                                          │
│                                                                             │
│ IF BENIGN:                                                                  │
│ ├── Document finding for future reference                                  │
│ ├── Consider tuning to reduce false positives                              │
│ └── Close bookmark with disposition                                        │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘</div>

                    <h4>Effective Bookmark Documentation</h4>
                    <div class="arch-diagram">Bookmark Best Practices:

NAMING CONVENTION:
[Date]-[Tactic]-[Brief Description]
Examples:
├── "2024-01-15-T1055-Suspicious Process Injection on WORKSTATION01"
├── "2024-01-15-T1059-Encoded PowerShell from Finance User"
└── "2024-01-15-T1003-LSASS Access from Non-Standard Process"

NOTES TEMPLATE:
┌─────────────────────────────────────────────────────────────────┐
│ OBSERVATION:                                                     │
│ [What was seen in the data]                                     │
│                                                                  │
│ HYPOTHESIS:                                                      │
│ [What this might indicate]                                      │
│                                                                  │
│ NEXT STEPS:                                                      │
│ [ ] Check entity timeline                                       │
│ [ ] Correlate with network logs                                 │
│ [ ] Review on affected host                                     │
│                                                                  │
│ CONFIDENCE: [Low/Medium/High]                                   │
│ PRIORITY: [P1/P2/P3]                                            │
└─────────────────────────────────────────────────────────────────┘

TAG TAXONOMY:
├── Tactic: initial-access, execution, persistence, etc.
├── Severity: critical, high, medium, low
├── Status: investigating, confirmed, false-positive
├── Source: threat-intel, anomaly, coverage-gap
└── Custom: campaign-name, hunt-project, etc.</div>
                </div>

                <!-- Livestream -->
                <h2><i class="fas fa-broadcast-tower"></i> Livestream for Active Hunting</h2>
                <div class="config-section">
                    <h4>Livestream Overview</h4>
                    <p>Livestream enables real-time monitoring of hunting queries for active threat hunting sessions.</p>

                    <div class="arch-diagram">Livestream Capabilities:

WHAT IT DOES:
├── Runs your hunting query continuously
├── Monitors for new matches in near real-time
├── Notifies you when results appear
├── Sessions last up to 24 hours
└── Supports multiple concurrent streams

USE CASES:
├── Active incident hunting: Watch for lateral movement
├── Threat intel validation: Monitor for new IOCs
├── Post-patch monitoring: Watch for exploitation attempts
├── Suspicious user monitoring: Track high-risk accounts
└── Attack simulation: Validate red team activity detection

LIMITATIONS:
├── 24-hour maximum session duration
├── Limited to hunting queries (not analytics rules)
├── Requires browser session for notifications
└── Not a replacement for automated detection</div>

                    <h4>Livestream Best Practices</h4>
                    <div class="arch-diagram">Effective Livestream Usage:

SCENARIO 1: ACTIVE INCIDENT
During IR, create Livestream for:
├── Attacker's known C2 communication
├── Compromised account activity
├── Lateral movement from infected host
└── Data exfiltration indicators

Query Example:
SigninLogs
| where TimeGenerated > ago(5m)
| where UserPrincipalName == "compromised.user@company.com"
| project TimeGenerated, Location, IPAddress, AppDisplayName

SCENARIO 2: THREAT INTEL MONITORING
New threat report indicates specific TTPs:
├── Create targeted query for technique
├── Run Livestream for 24 hours
├── Monitor for matches
└── Bookmark and investigate hits

Query Example:
DeviceProcessEvents
| where TimeGenerated > ago(5m)
| where ProcessCommandLine has "certutil" 
    and ProcessCommandLine has "-urlcache"
| project TimeGenerated, DeviceName, ProcessCommandLine

SCENARIO 3: POST-VULNERABILITY RESPONSE
Critical CVE announced, patch deployed:
├── Create exploitation detection query
├── Monitor for 48-72 hours post-announcement
├── Catch exploitation attempts during patch window
└── Validate patch effectiveness

Query Example:
CommonSecurityLog
| where TimeGenerated > ago(5m)
| where RequestURL has "/vulnerable/endpoint"
| where HttpStatusCode in (200, 500)
| project TimeGenerated, SourceIP, RequestURL</div>
                </div>

                <!-- What Enterprises Usually Miss -->
                <h2><i class="fas fa-exclamation-triangle"></i> What Enterprises Usually Miss</h2>
                <div class="config-section">
                    <div class="arch-diagram">Common Hunting Program Failures:

1. NO HUNTING PROGRAM AT ALL
   ✗ "We have detection rules, isn't that enough?"
   → FIX: Dedicate 20% of senior analyst time to hunting
   → Automated detection catches known threats; hunting finds unknown

2. UNSTRUCTURED HUNTING
   ✗ Random queries without hypothesis or documentation
   → FIX: Implement formal methodology (TaHiTI-based)
   → Every hunt should have documented hypothesis and outcome

3. NO FOLLOW-THROUGH
   ✗ Finding interesting things but not acting on them
   → FIX: Process to convert findings to detections or incidents
   → Every hunt should produce rules, intel, or incidents

4. INSUFFICIENT DATA VISIBILITY
   ✗ "We can't hunt for X because we don't have the logs"
   → FIX: Map MITRE coverage gaps, prioritize data onboarding
   → Hunting requirements should drive data collection

5. HUNTING IN SILOS
   ✗ Hunters don't share findings with detection engineering
   → FIX: Weekly sync between hunting and detection teams
   → Hunt findings should feed detection rule development

6. IGNORING THREAT INTELLIGENCE
   ✗ Hunting without context of current threat landscape
   → FIX: TI-informed hypothesis generation
   → Subscribe to relevant intel feeds, ISAC alerts

7. NO METRICS OR IMPROVEMENT
   ✗ No way to measure hunting effectiveness
   → FIX: Track hunts conducted, findings, detections created
   → Measure dwell time reduction, threats found

8. TOOL FIXATION
   ✗ "We need expensive tool X before we can hunt"
   → FIX: Sentinel built-in capabilities are sufficient to start
   → Invest in skills before tools

9. ALERT FATIGUE SPILLOVER
   ✗ Hunters pulled to triage alerts instead of hunting
   → FIX: Protect hunting time as dedicated activity
   → Separate hunting role from alert response

10. NO KNOWLEDGE MANAGEMENT
    ✗ Hunt findings lost when analysts leave
    → FIX: Document all hunts, findings, and playbooks
    → Build institutional hunting knowledge base</div>
                </div>

                <!-- Enterprise Hunting Program -->
                <h2><i class="fas fa-building"></i> Building an Enterprise Hunting Program</h2>
                <div class="config-section">
                    <h4>Hunting Program Structure</h4>
                    <div class="arch-diagram">Enterprise Hunting Program Components:

┌─────────────────────────────────────────────────────────────────────────────┐
│                         HUNTING GOVERNANCE                                  │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│ STAKEHOLDERS:                                                               │
│ ├── SOC Manager: Resource allocation, prioritization                       │
│ ├── Threat Intel: Hypothesis input, IOC feeds                              │
│ ├── Detection Engineering: Rule development from findings                  │
│ └── IR Team: Escalation and response coordination                          │
│                                                                             │
│ CADENCE:                                                                    │
│ ├── Daily: Livestream monitoring for active threats                        │
│ ├── Weekly: Structured hunts based on priority queue                       │
│ ├── Monthly: Hunt review and metrics                                       │
│ └── Quarterly: Program effectiveness assessment                            │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────────────┐
│                         HUNTING QUEUE                                       │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│ PRIORITY MATRIX:                                                            │
│ ┌────────────┬────────────┬────────────┬────────────┐                      │
│ │            │ High Impact│ Med Impact │ Low Impact │                      │
│ ├────────────┼────────────┼────────────┼────────────┤                      │
│ │ High Prob  │   P1       │    P2      │    P3      │                      │
│ │ Med Prob   │   P2       │    P3      │    P4      │                      │
│ │ Low Prob   │   P3       │    P4      │    P5      │                      │
│ └────────────┴────────────┴────────────┴────────────┘                      │
│                                                                             │
│ QUEUE SOURCES:                                                              │
│ ├── Threat intel reports (CISA, ISAC, vendor)                              │
│ ├── MITRE ATT&CK coverage gaps                                             │
│ ├── Recent incident learnings                                               │
│ ├── Industry-specific threats                                               │
│ └── Anomaly/UEBA triggers                                                   │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────────────┐
│                         HUNTING METRICS                                     │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│ ACTIVITY METRICS:                                                           │
│ ├── Hunts conducted per month                                              │
│ ├── Hours invested in hunting                                               │
│ ├── Data sources queried                                                    │
│ └── MITRE techniques covered                                                │
│                                                                             │
│ OUTCOME METRICS:                                                            │
│ ├── Threats discovered                                                      │
│ ├── Incidents created from hunts                                           │
│ ├── Detection rules created                                                 │
│ ├── False positive tuning from findings                                    │
│ └── Dwell time reduction                                                    │
│                                                                             │
│ EFFICIENCY METRICS:                                                         │
│ ├── Findings per hunt hour                                                 │
│ ├── Hunt-to-detection conversion rate                                      │
│ └── Repeat findings (should decrease)                                      │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘</div>

                    <h4>Hunting Maturity Model</h4>
                    <div class="arch-diagram">Hunting Maturity Assessment:

LEVEL 0: INITIAL
├── No formal hunting program
├── Ad-hoc queries during incidents only
├── No hypothesis documentation
└── FIX: Start with 4 hours/week dedicated hunting

LEVEL 1: MINIMAL
├── Some hunting using vendor queries
├── No custom hypothesis generation
├── Findings rarely converted to detections
└── FIX: Develop hypothesis pipeline from TI

LEVEL 2: PROCEDURAL
├── Regular hunting schedule
├── Documented methodology
├── Bookmarks used for evidence
├── Some detection rule creation
└── FIX: Integrate with detection engineering

LEVEL 3: INNOVATIVE
├── Custom hunt development
├── TI-driven hypothesis generation
├── Systematic finding follow-through
├── Metrics tracked and reported
└── FIX: Add advanced analytics, notebooks

LEVEL 4: LEADING
├── Machine learning augmented hunting
├── Automated hypothesis generation
├── Continuous improvement loop
├── Industry threat sharing
└── GOAL: Maintain and share knowledge

ASSESSMENT QUESTIONS:
1. How many hours/week dedicated to hunting?
2. Is there a documented methodology?
3. How are findings tracked and converted?
4. What metrics do you report?
5. How does TI inform hunting?</div>
                </div>

                <!-- Interview Questions -->
                <h2><i class="fas fa-user-tie"></i> Interview Questions & Answers</h2>
                <div class="config-section">
                    <h4>Q1: Walk me through your approach to starting a threat hunt based on a new APT report.</h4>
                    <div class="arch-diagram">APPROACH:
When I receive a new APT report, I follow a structured methodology to 
convert threat intel into actionable hunts.

PHASE 1: EXTRACT RELEVANT TTPS (30 min)
├── Read report, identify tactics and techniques
├── Map to MITRE ATT&CK framework
├── List specific IOCs (hashes, IPs, domains, file paths)
├── Note behavioral indicators (not just signatures)
└── Assess: Does this threat actor target our industry/region?

PHASE 2: DATA ASSESSMENT (15 min)
├── For each technique: Do we have visibility?
├── Which Sentinel tables contain relevant data?
├── What's our retention - can we look back far enough?
└── Gap identified? Flag for data source improvement

PHASE 3: QUERY DEVELOPMENT (1-2 hours)
├── Start with IOC searches (quick wins)
├── Develop behavioral queries for each TTP
├── Reference existing hunting queries as templates
├── Optimize for performance (7-30 day lookbacks)
└── Document query logic and assumptions

PHASE 4: EXECUTION (2-4 hours)
├── Run queries against historical data
├── Analyze results, filter noise
├── Investigate outliers
├── Bookmark interesting findings
└── Pivot on entities if suspicious

PHASE 5: OUTPUT (30 min)
├── Document hunt results
├── Create/update analytics rules if behavioral patterns found
├── Share findings with SOC team
├── Add to threat intel platform
└── Schedule follow-up hunt if needed

WHY THIS APPROACH:
- Structured methodology ensures nothing is missed
- Behavioral hunting catches variants, not just exact IOCs
- Documentation builds institutional knowledge
- Output loop improves detection over time

TRADE-OFFS:
- Time investment (4-8 hours per report)
- Not all reports are relevant - need to triage
- IOC-focused hunting has short shelf life

VALIDATION:
Track hit rate: What percentage of hunts find something?
Measure detection gap closure from hunt findings.</div>

                    <h4>Q2: How do you prioritize what to hunt when you have limited time?</h4>
                    <div class="arch-diagram">APPROACH:
Hunting prioritization should balance threat probability, impact, and 
detection gap severity.

MY PRIORITIZATION FRAMEWORK:

1. ACTIVE THREAT INTEL (Highest Priority)
   ├── Current campaigns targeting our industry
   ├── CISA alerts and advisories
   ├── Vendor threat briefs for tools we use
   └── Allocate 40% of hunting time

2. RECENT INCIDENTS (High Priority)
   ├── Did we miss related activity?
   ├── Could attack be elsewhere in environment?
   ├── What gaps did incident reveal?
   └── Allocate 25% of hunting time

3. COVERAGE GAPS (Medium Priority)
   ├── MITRE ATT&CK techniques without detection
   ├── Priority: Credential access, lateral movement
   ├── New data sources with historical gaps
   └── Allocate 20% of hunting time

4. ANOMALY FOLLOW-UP (Medium Priority)
   ├── UEBA high-risk entity investigation
   ├── Statistical outliers in key tables
   └── Allocate 10% of hunting time

5. EXPLORATORY (Lower Priority)
   ├── New technique research
   ├── Tool capability testing
   └── Allocate 5% of hunting time

DECISION MATRIX:
| Factor                    | Weight |
|---------------------------|--------|
| Threat targeting us       | 30%    |
| No existing detection     | 25%    |
| High impact if true       | 25%    |
| Data visibility exists    | 20%    |

Score each potential hunt, prioritize by total.

WHY:
Limited time means opportunity cost. Every hour spent on low-priority 
hunt is an hour not spent on high-priority threat.

TRADE-OFFS:
- Risk of missing novel attacks not in intel
- Over-focus on known threats
- Balance: Reserve time for exploratory hunting

VALIDATION:
Review prioritization quarterly. Are high-priority hunts finding more?
Adjust weights based on outcomes.</div>

                    <h4>Q3: You've found suspicious activity during a hunt. What's your process?</h4>
                    <div class="arch-diagram">APPROACH:
When I find something suspicious, I follow a triage process before 
escalating to determine confidence level.

IMMEDIATE ACTIONS (15-30 min):
1. PRESERVE EVIDENCE
   ├── Create bookmark with full context
   ├── Capture query and results
   ├── Note timestamp and affected entities
   └── Screenshot if time-sensitive

2. QUICK VALIDATION
   ├── Is this a known false positive?
   ├── Check against previous hunt findings
   ├── Validate data quality (parsing issues?)
   └── Confirm entity exists and is active

INVESTIGATION EXPANSION (30-60 min):
3. CONTEXT GATHERING
   ├── Entity timeline: What else did user/host do?
   ├── Temporal analysis: What happened before/after?
   ├── Peer comparison: Do similar entities behave this way?
   └── Threat intel: Does this match known TTPs?

4. PIVOT QUERIES
   ├── Network: Where did host connect?
   ├── Process: What spawned what?
   ├── Authentication: Where else did account log in?
   └── Files: What was created/modified?

DECISION POINT:
Based on investigation:

CONFIDENT MALICIOUS (>80% confidence):
├── Promote bookmark to incident immediately
├── Notify SOC lead
├── Begin containment recommendations
└── Preserve evidence for forensics

LIKELY MALICIOUS (50-80% confidence):
├── Document findings thoroughly
├── Request L3 analyst review
├── May promote to low-severity incident
└── Continue monitoring

UNCERTAIN (30-50% confidence):
├── Add Livestream monitoring
├── Expand investigation scope
├── Seek additional context
└── Document for future reference

LIKELY BENIGN (<30% confidence):
├── Document why benign
├── Consider rule tuning
├── Close bookmark with notes
└── May still inform detection improvement

WHY:
Structured triage prevents both over-escalation (alert fatigue) and 
under-escalation (missed threats). Documentation ensures decisions
are defensible.

VALIDATION:
Track escalation accuracy: Were promoted incidents real?
Learn from both false positives and false negatives.</div>

                    <h4>Q4: How do you measure the success of a hunting program?</h4>
                    <div class="arch-diagram">APPROACH:
Hunting success should be measured across activity, outcomes, and 
program improvement dimensions.

METRICS I WOULD TRACK:

ACTIVITY METRICS (Leading Indicators):
├── Hunts completed per month
├── Hours invested in hunting
├── Hypotheses generated
├── Queries developed
├── Data sources queried
└── MITRE techniques covered (%)

OUTCOME METRICS (Lagging Indicators):
├── Threats discovered (true positives)
├── Incidents created from hunts
├── Mean time to discover (via hunting)
├── Attacker dwell time reduction
├── Detection rules created from findings
└── False positive tuning from hunts

QUALITY METRICS:
├── Finding-to-incident ratio
├── Hunt-to-detection conversion rate
├── Coverage gap closure rate
├── Repeat finding rate (should decrease)
└── Time to hunt new threat intel

DASHBOARD I WOULD BUILD:
┌────────────────────────────────────────────────────────────────┐
│ HUNTING PROGRAM DASHBOARD - Q1 2024                           │
├────────────────────────────────────────────────────────────────┤
│ Hunts Completed: 24    │ Threats Found: 3      │ Rules: 12   │
├────────────────────────────────────────────────────────────────┤
│ Hours Invested: 96     │ Incidents: 2          │ Coverage: 67%│
├────────────────────────────────────────────────────────────────┤
│ MITRE Coverage Trend   │ Finding by Tactic     │ Time to Hunt │
│ [Heatmap improving]    │ [Bar chart]           │ Avg: 4 days  │
└────────────────────────────────────────────────────────────────┘

BENCHMARKS (what good looks like):
├── 20-40% of senior analyst time on hunting
├── 1+ threat discovered per quarter
├── 50%+ hunts produce detection improvement
├── <7 days from intel to hunt execution
└── 70%+ MITRE technique coverage

WHY:
What gets measured gets improved. Metrics justify hunting investment 
and guide program evolution.

TRADE-OFFS:
- Metric gaming risk (quantity over quality)
- Some value is intangible (skills development)
- Don't over-engineer reporting

VALIDATION:
Compare to pre-hunting baseline. Are we finding threats earlier?
Survey SOC team: Is hunting improving detection?</div>

                    <h4>Q5: Describe a challenging hunt you've conducted and what you learned.</h4>
                    <div class="arch-diagram">APPROACH:
I'll walk through a hunt for potential supply chain compromise that 
illustrates methodology and lessons learned.

THE SCENARIO:
After the SolarWinds incident, leadership asked: "Could something 
similar be happening to us?"

HYPOTHESIS:
Software from trusted vendors may be beaconing to unexpected 
destinations, indicating compromise.

CHALLENGE 1: BASELINE UNKNOWN
├── Problem: What "normal" looks like for each vendor tool?
├── Solution: 30-day baseline of all software network activity
├── Learning: You can't detect anomalies without understanding normal
└── Query developed baseline per-application

CHALLENGE 2: MASSIVE DATA VOLUME
├── Problem: Millions of network connections per day
├── Solution: Statistical aggregation - connections per hour, unique 
│   destinations per application
├── Learning: Hunt queries need to reduce, not just filter
└── Created summarization approach for future hunts

CHALLENGE 3: FALSE POSITIVE NOISE
├── Problem: CDNs, cloud services look suspicious but are legitimate
├── Solution: Built allowlist of known-good destinations
├── Learning: Tuning takes time, save allowlists for reuse
└── Created shareable hunting list asset

WHAT I FOUND:
├── A developer tool making connections to unusual region
├── Investigation: Developer had installed unofficial plugin
├── Outcome: Not malicious, but policy violation
├── Action: Updated acceptable use policy, created detection

DETECTION CREATED:
"Approved Software to Unapproved Destination"
- Monitor vendor software for connections outside baseline
- Alert on new destinations after 30-day learning

LESSONS LEARNED:
1. Baseline before you hunt - can't find anomalies otherwise
2. Statistical approaches beat signature matching for unknowns
3. Even "benign" findings have security value
4. Every hunt should produce something (rule, tuning, documentation)
5. Complex hunts take multiple iterations

WHY THIS EXAMPLE:
Shows structured methodology, dealing with ambiguity, and producing 
value even when no threat is found.

VALIDATION:
Detection has caught 3 policy violations since deployment.
Approach reused for multiple supply chain hunts.</div>

                    <h4>Q6: How would you integrate Sentinel hunting with other security tools?</h4>
                    <div class="arch-diagram">APPROACH:
Effective hunting leverages multiple data sources and tools. Sentinel 
should be the correlation hub, not an island.

INTEGRATION ARCHITECTURE:

┌─────────────────────────────────────────────────────────────────┐
│                    SENTINEL (Hunting Hub)                       │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐            │
│  │ M365 Defender│  │ EDR Data   │  │ Network     │            │
│  │ (Integrated) │  │ (DeviceX)  │  │ (Firewall)  │            │
│  └─────────────┘  └─────────────┘  └─────────────┘            │
│                                                                 │
│  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐            │
│  │ Cloud Logs  │  │ Identity    │  │ Application │            │
│  │ (Azure/AWS) │  │ (Entra ID)  │  │ (Custom)    │            │
│  └─────────────┘  └─────────────┘  └─────────────┘            │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
         │                   │                    │
         ▼                   ▼                    ▼
┌─────────────────┐ ┌─────────────────┐ ┌─────────────────┐
│ THREAT INTEL    │ │ SOAR            │ │ EDR CONSOLE     │
│ (TIP/MISP)      │ │ (Logic Apps)    │ │ (Deep forensics)│
├─────────────────┤ ├─────────────────┤ ├─────────────────┤
│ IOCs in         │ │ Automated       │ │ Live response   │
│ ThreatIndicators│ │ enrichment      │ │ for validation  │
│ table           │ │ playbooks       │ │                 │
└─────────────────┘ └─────────────────┘ └─────────────────┘

SPECIFIC INTEGRATIONS:

1. THREAT INTELLIGENCE PLATFORM
   ├── Push IOCs to Sentinel TI table
   ├── Query TI during hunts for context
   ├── Feed hunt findings back to TIP
   └── Example: MISP → Sentinel TI Connector

2. EDR (Defender for Endpoint)
   ├── Hunt in Sentinel for breadth (all data)
   ├── Pivot to EDR for depth (host forensics)
   ├── Use Live Response for validation
   └── Bi-directional alert sync

3. SOAR (Logic Apps)
   ├── Auto-enrich entities during hunting
   ├── Automate IOC lookups
   ├── Create enrichment playbooks for bookmarks
   └── Notify team on hunt findings

4. NOTEBOOKS (Azure ML)
   ├── Advanced statistical analysis
   ├── ML-based anomaly detection
   ├── Visual investigation timelines
   └── Reusable hunt playbooks

HUNT WORKFLOW WITH INTEGRATION:
1. TI → Informs hypothesis
2. Sentinel → Broad hunting query
3. Finding → Auto-enrich via SOAR
4. Suspicious → Pivot to EDR for forensics
5. Confirmed → Promote to incident
6. Learnings → Feed back to TI and detection

WHY:
No single tool has complete visibility. Integration enables seeing 
the full picture while using each tool's strengths.

TRADE-OFFS:
- Integration complexity
- Data duplication costs
- Skill requirements across tools

VALIDATION:
Measure time to enrich findings.
Track pivots between tools during hunts.
Survey hunters: Is integration helpful?</div>
                </div>

            </main>
            
            <footer class="main-footer">
                <p>&copy; 2024 Nik's SIEM Compendium. Built for security professionals.</p>
            </footer>
        </div>
    </div>
    
    <script>
        function toggleSidebar() {
            const sidebar = document.getElementById('sidebar');
            const mainWrapper = document.getElementById('mainWrapper');
            const overlay = document.getElementById('sidebarOverlay');
            
            if (window.innerWidth <= 768) {
                sidebar.classList.toggle('open');
                overlay.classList.toggle('active');
            } else {
                sidebar.classList.toggle('collapsed');
                mainWrapper.classList.toggle('expanded');
            }
        }
    </script>
</body>
</html>
